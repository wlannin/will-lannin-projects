{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 136194553992213785217382377961235308297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
       "0        5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4        9    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "59995    8    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59997    5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59998    6    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59999    8    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       778  779  780  781  782  783  784  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "59995    0    0    0    0    0    0    0  \n",
       "59996    0    0    0    0    0    0    0  \n",
       "59997    0    0    0    0    0    0    0  \n",
       "59998    0    0    0    0    0    0    0  \n",
       "59999    0    0    0    0    0    0    0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/wilhelmlannin/Documents/Python Stuff/MNIST_stuff/MNIST_CSV/mnist_train.csv\",header=None)\n",
    "train_df\n",
    "\n",
    "# each row is a training case\n",
    "# 0th column tells us the label for the case in that row\n",
    "# rest of the columns tell us the pixel vals (reading left to right from top left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Nnet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see my rough code notebook for justifications/derivations for the code used in the class below\n",
    "\n",
    "# and see the attached pdfs for mathematical derivations of the gradients of each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nnet:\n",
    "    def __init__(self,train_df,nu=0.01,random_seed=seed):\n",
    "        #train_df: dataframe of mnist training cases \n",
    "        #nu: learning rate, deafult 0.01\n",
    "        #random_seed: number that sets the randomness of ranom number generator for replicability\n",
    "\n",
    "        self.train_df = train_df\n",
    "        #set seed for random number generation\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # map pixel values between 0 and 1\n",
    "        self.train_arr_01 = np.array(train_df.iloc[:,1:]) / 255\n",
    "\n",
    "        # true labels for each image\n",
    "        self.t = np.array(self.train_df[0])\n",
    "        #number of training images\n",
    "        self.K = len(self.t)\n",
    "\n",
    "        #number of neurons in each layer\n",
    "        self.L = {0:784, 1:128, 2:64, 3:10}\n",
    "        #bias vals initialised at 0\n",
    "        self.b = {1:0, 2:0, 3:0}\n",
    "        # weight matrices (randomly initialised using xavier)\n",
    "        self.W = {1:self.rng.normal(loc=0,scale=(1/self.L[0])**0.5,size=[self.L[0],self.L[1]]),\n",
    "            2:self.rng.normal(loc=0,scale=(1/self.L[1])**0.5,size=[self.L[1],self.L[2]]),\n",
    "            3:self.rng.normal(loc=0,scale=(1/self.L[2])**0.5,size=[self.L[2],self.L[3]])}\n",
    "        \n",
    "        #exponential e^ function\n",
    "        self.allexp = np.vectorize(math.exp)\n",
    "        #ReLU activation function\n",
    "        self.f = np.vectorize(lambda x: max([0,x]),otypes=[float])\n",
    "\n",
    "        #learning rate parameter nu for gradient steps\n",
    "        self.nu = nu\n",
    "\n",
    "        #training related data\n",
    "        self.wsums = {l:{} for l in self.L.keys()}\n",
    "        self.phat_all_k = []  # filled later\n",
    "        self.g_b = self.b.copy()\n",
    "        self.g_W = self.W.copy()\n",
    "        self.gradient_steps_taken = 0\n",
    "\n",
    "        \n",
    "\n",
    "    def view_train_image_k(self,k):\n",
    "        #silly way that I define the pixels at the start and end of each row\n",
    "        bound=1\n",
    "        bounds_list = []\n",
    "        while bound < 784:\n",
    "            bound_l = bound\n",
    "            bound += 28\n",
    "            bound_u = bound\n",
    "            bounds_list += [(bound_l,bound_u)]\n",
    "        #now use these bounds to show the train image\n",
    "        print(\"label:\",self.train_df.iloc[k,0])\n",
    "        img = np.array([list(self.train_df.iloc[k,x[0]:x[1]]) for x in bounds_list])\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    def generate_phat_k(self,k):\n",
    "        vec = self.train_arr_01[k]\n",
    "        p3 = self.f(self.b[3] + ( self.f(self.b[2] + ( self.f(self.b[1] + ( vec @ self.W[1] )) @ self.W[2])) @ self.W[3]))\n",
    "        expvec = self.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        return phat\n",
    "    \n",
    "    def predict_k(self,k):\n",
    "        vec = self.train_arr_01[k]\n",
    "        p3 = self.f(self.b[3] + ( self.f(self.b[2] + ( self.f(self.b[1] + ( vec @ self.W[1] )) @ self.W[2])) @ self.W[3]))\n",
    "        expvec = self.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "                    \n",
    "    def accuracy(self):\n",
    "        vec_predict = np.vectorize(self.predict_k)\n",
    "        pred_labels1 = vec_predict(np.array(range(len(self.t))))\n",
    "        # TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "        true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(self.t)]\n",
    "        total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "        return total_accuracy\n",
    "        \n",
    "    def all_performance(self):\n",
    "        vec_predict = np.vectorize(self.predict_k)\n",
    "        pred_labels1 = vec_predict(np.array(range(len(self.t))))\n",
    "\n",
    "        # TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "        true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(self.t)]\n",
    "        total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "        print(total_accuracy)\n",
    "        print(\"total accuracy =\", round(100*total_accuracy,2),\"%\")\n",
    "\n",
    "        # digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "        digit_accuracy = {}\n",
    "        for digit in range(10):\n",
    "            train_cases_containing_digit = [k for k,true_val in enumerate(self.t) if true_val==digit]\n",
    "            # now calculate how many of these cases we predicted correctly\n",
    "            checklist = [pred_labels1[k]==digit for k in train_cases_containing_digit]\n",
    "            digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "        print(\"digit accuracy:\")\n",
    "        print(digit_accuracy)\n",
    "\n",
    "        # digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "        digit_recall = {}\n",
    "        for digit in range(10):\n",
    "            cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "            # now calculate how many of these cases actually contained this digit correctly\n",
    "            checklist = [self.t[k]==digit for k in cases_we_predicted_digit]\n",
    "            digit_recall[digit] = sum(checklist)/len(checklist) if len(checklist) != 0 else None\n",
    "        print(\"digit_recall:\")\n",
    "        print(digit_recall)\n",
    "\n",
    "    def calc_wsums_and_phat_all_k(self):\n",
    "        # we need every single weight sum for every single k, so create a data structure that allows us to store these conveniently\n",
    "\n",
    "        self.wsums[1] = {k:self.b[1] + (self.train_arr_01[k] @ self.W[1]) for k in range(len(self.t))}\n",
    "        # now we need to define the second layer weight sums using wsums[1]\n",
    "        self.wsums[2] = {k:self.b[2] + self.f(self.wsums[1][k]) @ self.W[2] for k in range(len(self.t))}\n",
    "        # and define the 3rd layer weight sums using the second weight sums we just calculated\n",
    "        self.wsums[3] = {k:self.b[3] + self.f(self.wsums[2][k]) @ self.W[3] for k in range(len(self.t))}\n",
    "        #this way, we get all the weight sums we need and only have to do each matrix multiplication once! work samrter not harder?\n",
    "\n",
    "        # now we need to arrive at phat for each training case k from the layer 3 weightsums in wsums[3]\n",
    "        expvec_all_k = [self.allexp(self.f(self.wsums[3][k])) for k in range(len(self.t))]\n",
    "        self.phat_all_k = [expvec_all_k[k]/sum(expvec_all_k[k]) for k in range(len(self.t))]\n",
    "\n",
    "\n",
    "    def calc_bias_gradients(self):\n",
    "        # biases\n",
    "        self.g_b[3] = -sum([(self.wsums[3][k][self.t[k]] > 0) - sum([self.phat_all_k[k][z] for z in range(self.L[3]) if self.wsums[3][k][z] > 0]) for k in range(len(self.t))]) / self.K\n",
    "\n",
    "\n",
    "        self.g_b[2] = -sum([(sum([self.W[3][i][self.t[k]] for i in range(self.L[2]) if self.wsums[2][k][i] > 0]) if self.wsums[3][k][self.t[k]] > 0 else 0) \\\n",
    "        - sum([self.phat_all_k[k][z]*sum([self.W[3][i][z] for i in range(self.L[2]) if self.wsums[2][k][i] > 0]) for z in range(self.L[3]) if self.wsums[3][k][z] > 0]) for k in range(len(self.t))]) / self.K\n",
    "\n",
    "        self.g_b[1] = -sum(\n",
    "            [\n",
    "                (\n",
    "                    np.sum(\n",
    "                    np.multiply(self.W[3][np.nonzero(self.wsums[2][k][:] > 0),self.t[k]] , np.sum(self.W[2][np.nonzero(self.wsums[1][k][:] > 0)][:,np.nonzero(self.wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                        ) if self.wsums[3][k][self.t[k]] > 0 else 0) \\\n",
    "        - sum(\n",
    "            [\n",
    "                self.phat_all_k[k][z]*(np.sum(\n",
    "                    np.multiply(self.W[3][np.nonzero(self.wsums[2][k][:] > 0),z] , np.sum(self.W[2][np.nonzero(self.wsums[1][k][:] > 0)][:,np.nonzero(self.wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                        ) if self.wsums[3][k][z] > 0 else 0) for z in range(self.L[3]) if self.wsums[3][k][z] > 0]\n",
    "                        ) for k in range(len(self.t))]) / self.K\n",
    "    \n",
    "    def calc_weight_gradients(self):\n",
    "\n",
    "        self.g_W[3] = np.transpose(-np.sum(np.array([np.multiply(np.array([np.repeat([( (b==self.t[k]) - self.phat_all_k[k][b] )],repeats=self.L[2]) for b in range(self.L[3])]),np.transpose(np.array(self.wsums[3][k] > 0,ndmin=2)) @ np.array(self.f(self.wsums[2][k]),ndmin=2))\n",
    "                                    for k in range(len(self.t))]),axis=0) / self.K)\n",
    "        \n",
    "        # so, here is the final code I'll use for layer 2 weights\n",
    "        i3W3_all_k_all_z_fast = np.array(\n",
    "            [\n",
    "                [np.array(self.W[3][:,z]) if (self.wsums[3][k][z] > 0) else np.zeros(self.L[2]) for z in range(self.L[3])\n",
    "                ] for k in range(len(self.t))\n",
    "                ]\n",
    "                )\n",
    "\n",
    "        self.g_W[2] = -sum([np.multiply(\n",
    "            #start np.multiply with the transpose of new_dotted\n",
    "            np.transpose(np.transpose(np.array(self.wsums[2][k] > 0,ndmin=2)) @ np.array(self.f(self.wsums[1][k]),ndmin=2)),\n",
    "                    #bracketsterm_repeated is defined as the right-part of this np.multiply\n",
    "                    np.repeat([i3W3_all_k_all_z_fast[k][self.t[k]] - np.sum(np.array([self.phat_all_k[k][z] * i3W3_all_k_all_z_fast[k][z] for z in range(self.L[3])]),axis=0)],repeats=self.L[1],axis=0))\n",
    "                    for k in range(len(self.t))]) / self.K\n",
    "        \n",
    "        # so in totality here is g_W[1]\n",
    "        i3thensum_all_k_all_z_3 = [\n",
    "                [\n",
    "                    np.sum(np.multiply(self.W[2][:,np.nonzero(self.wsums[2][k] > 0)[0]],\n",
    "                                #this is \"repeated\" we defined earlier\n",
    "                                np.repeat([self.W[3][np.nonzero(self.wsums[2][k] > 0)[0],z]],repeats=self.L[1],axis=0)),axis=1,initial=0)\n",
    "                                    if (self.wsums[3][k][z] > 0) else np.zeros(self.L[1]) for z in range(self.L[3])\n",
    "                ] for k in range(len(self.t))\n",
    "            ]\n",
    "\n",
    "        self.g_W[1] = -sum([np.multiply(\n",
    "            #start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\n",
    "            np.transpose(np.transpose(np.array(self.wsums[1][k] > 0,ndmin=2)) @ np.array(self.train_arr_01[k],ndmin=2)),\n",
    "                    #then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\n",
    "                    np.repeat([i3thensum_all_k_all_z_3[k][self.t[k]] - np.sum(np.array([self.phat_all_k[k][z] * i3thensum_all_k_all_z_3[k][z] for z in range(self.L[3])]),axis=0)],repeats=self.L[0],axis=0))\n",
    "                    for k in range(len(self.t))]) / self.K\n",
    "        \n",
    "    def take_1_gradient_step(self):\n",
    "        self.calc_wsums_and_phat_all_k()\n",
    "        self.calc_bias_gradients()\n",
    "        self.calc_weight_gradients()\n",
    "\n",
    "        # now we have all the gradients calculated for this step, take the step\n",
    "        for l in [1,2,3]:\n",
    "            #gradient step for bias and weioghts corresponding to layer l\n",
    "            self.b[l] -= self.nu*self.g_b[l]\n",
    "            self.W[l] -= self.nu*self.g_W[l]\n",
    "\n",
    "        #increase number of steps taken\n",
    "        self.gradient_steps_taken += 1\n",
    "\n",
    "    def train_network_v1(self,eps=0.000001,max_wait_steps=3,max_steps=500):\n",
    "        #my first attempt at a function to train the network via many calls to self.take_1_gradient_step\n",
    "\n",
    "        # keep taking steps until either max_steps is hit or we go {max_wait_steps} many iterations without accuracy increasing by eps\n",
    "        \n",
    "        # accuracy pre step\n",
    "        pre_accuracy = self.accuracy()\n",
    "\n",
    "        #initialise steps_waited\n",
    "        steps_waited=0\n",
    "\n",
    "        while self.gradient_steps_taken < max_steps:\n",
    "\n",
    "            # step\n",
    "            self.take_1_gradient_step()\n",
    "\n",
    "            #accuracy after the step\n",
    "            post_accuracy = self.accuracy()\n",
    "            print(f\"accuracy after {self.gradient_steps_taken} steps:\",post_accuracy)\n",
    "\n",
    "            # if the change in accuracy is bigger than eps, continue\n",
    "            if abs(pre_accuracy - post_accuracy) > eps:\n",
    "                pre_accuracy = post_accuracy\n",
    "                continue\n",
    "            else:\n",
    "                pre_accuracy = post_accuracy\n",
    "                #add to steps_waited\n",
    "                steps_waited += 1\n",
    "                #if we have surpassed max steps waited we stop\n",
    "                if steps_waited > max_wait_steps:\n",
    "                    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Quick Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is sunday night and my code isn't that efficient so first experiment is trying a quick train with low iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quicknet = Nnet(train_df,nu=0.02,random_seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 1 steps: 0.07408333333333333\n",
      "accuracy after 2 steps: 0.07733333333333334\n",
      "accuracy after 3 steps: 0.08215\n",
      "accuracy after 4 steps: 0.08638333333333334\n",
      "accuracy after 5 steps: 0.09041666666666667\n",
      "accuracy after 6 steps: 0.0948\n",
      "accuracy after 7 steps: 0.09956666666666666\n",
      "accuracy after 8 steps: 0.10476666666666666\n",
      "accuracy after 9 steps: 0.1099\n",
      "accuracy after 10 steps: 0.11568333333333333\n",
      "accuracy after 11 steps: 0.12166666666666667\n",
      "accuracy after 12 steps: 0.12745\n",
      "accuracy after 13 steps: 0.13338333333333333\n",
      "accuracy after 14 steps: 0.13925\n",
      "accuracy after 15 steps: 0.1455\n",
      "accuracy after 16 steps: 0.15226666666666666\n",
      "accuracy after 17 steps: 0.1596\n",
      "accuracy after 18 steps: 0.16708333333333333\n",
      "accuracy after 19 steps: 0.1743\n",
      "accuracy after 20 steps: 0.18136666666666668\n",
      "accuracy after 21 steps: 0.18903333333333333\n",
      "accuracy after 22 steps: 0.19643333333333332\n",
      "accuracy after 23 steps: 0.20431666666666667\n",
      "accuracy after 24 steps: 0.21191666666666667\n",
      "accuracy after 25 steps: 0.21965\n",
      "accuracy after 26 steps: 0.2274\n",
      "accuracy after 27 steps: 0.23536666666666667\n",
      "accuracy after 28 steps: 0.24373333333333333\n",
      "accuracy after 29 steps: 0.25178333333333336\n",
      "accuracy after 30 steps: 0.2604\n",
      "accuracy after 31 steps: 0.2693333333333333\n",
      "accuracy after 32 steps: 0.27818333333333334\n",
      "accuracy after 33 steps: 0.287\n",
      "accuracy after 34 steps: 0.29625\n",
      "accuracy after 35 steps: 0.30548333333333333\n",
      "accuracy after 36 steps: 0.31528333333333336\n",
      "accuracy after 37 steps: 0.32575\n",
      "accuracy after 38 steps: 0.33713333333333334\n",
      "accuracy after 39 steps: 0.3467166666666667\n",
      "accuracy after 40 steps: 0.3567\n",
      "accuracy after 41 steps: 0.36645\n",
      "accuracy after 42 steps: 0.37633333333333335\n",
      "accuracy after 43 steps: 0.38571666666666665\n",
      "accuracy after 44 steps: 0.39536666666666664\n",
      "accuracy after 45 steps: 0.4043\n",
      "accuracy after 46 steps: 0.41201666666666664\n",
      "accuracy after 47 steps: 0.4194\n",
      "accuracy after 48 steps: 0.42715\n",
      "accuracy after 49 steps: 0.4344166666666667\n",
      "accuracy after 50 steps: 0.44145\n",
      "accuracy after 51 steps: 0.44855\n",
      "accuracy after 52 steps: 0.45535\n",
      "accuracy after 53 steps: 0.46121666666666666\n",
      "accuracy after 54 steps: 0.4670666666666667\n",
      "accuracy after 55 steps: 0.4733833333333333\n",
      "accuracy after 56 steps: 0.47881666666666667\n",
      "accuracy after 57 steps: 0.48385\n",
      "accuracy after 58 steps: 0.4892166666666667\n",
      "accuracy after 59 steps: 0.4947166666666667\n",
      "accuracy after 60 steps: 0.4997333333333333\n",
      "accuracy after 61 steps: 0.5043166666666666\n",
      "accuracy after 62 steps: 0.5081\n",
      "accuracy after 63 steps: 0.51285\n",
      "accuracy after 64 steps: 0.5168833333333334\n",
      "accuracy after 65 steps: 0.5212\n",
      "accuracy after 66 steps: 0.5255833333333333\n",
      "accuracy after 67 steps: 0.5294666666666666\n",
      "accuracy after 68 steps: 0.53295\n",
      "accuracy after 69 steps: 0.5369833333333334\n",
      "accuracy after 70 steps: 0.5404833333333333\n"
     ]
    }
   ],
   "source": [
    "#let it do 70 steps, use 0.02 learning rate\n",
    "Quicknet.train_network_v1(eps=0.000001,max_wait_steps=10,max_steps=70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 71 steps: 0.5442166666666667\n",
      "accuracy after 72 steps: 0.54775\n",
      "accuracy after 73 steps: 0.5510333333333334\n",
      "accuracy after 74 steps: 0.5544833333333333\n",
      "accuracy after 75 steps: 0.5582833333333334\n",
      "accuracy after 76 steps: 0.5613\n",
      "accuracy after 77 steps: 0.5644\n",
      "accuracy after 78 steps: 0.5676\n",
      "accuracy after 79 steps: 0.57085\n",
      "accuracy after 80 steps: 0.5741166666666667\n"
     ]
    }
   ],
   "source": [
    "#add another 10 steps\n",
    "Quicknet.train_network_v1(eps=0.000001,max_wait_steps=10,max_steps=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 81 steps: 0.5773166666666667\n",
      "accuracy after 82 steps: 0.5809\n",
      "accuracy after 83 steps: 0.58355\n"
     ]
    }
   ],
   "source": [
    "#add another 10 steps\n",
    "Quicknet.train_network_v1(eps=0.000001,max_wait_steps=10,max_steps=83)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58355\n",
      "total accuracy = 58.36 %\n",
      "digit accuracy:\n",
      "{0: 0.1485733580955597, 1: 0.13823791159893206, 2: 0.12772742531050688, 3: 0.16783558962648834, 4: 0.07839780896953098, 5: 0.012543811104962183, 6: 0.03751267320040554, 7: 0.21292897047086992, 8: 0.07502990941719365, 9: 0.0}\n",
      "digit_recall:\n",
      "{0: 0.10023612750885479, 1: 0.11305215872223609, 2: 0.09843571156047311, 3: 0.10330313843901547, 4: 0.10782980376918594, 5: 0.08707482993197278, 6: 0.09758713136729223, 7: 0.10659773510585918, 8: 0.09267376330619913, 9: None}\n"
     ]
    }
   ],
   "source": [
    "Quicknet.all_performance()\n",
    "# please note I made a mistake in my code for digit-level accuracy and recall, corrected later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
       "0       7    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1       2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2       1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4       4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9995    2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9997    4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9998    5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9999    6    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "      778  779  780  781  782  783  784  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9995    0    0    0    0    0    0    0  \n",
       "9996    0    0    0    0    0    0    0  \n",
       "9997    0    0    0    0    0    0    0  \n",
       "9998    0    0    0    0    0    0    0  \n",
       "9999    0    0    0    0    0    0    0  \n",
       "\n",
       "[10000 rows x 785 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/Users/wilhelmlannin/Documents/Python Stuff/MNIST_stuff/MNIST_CSV/mnist_test.csv\",header=None)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr_01 = np.array(test_df.iloc[:,1:]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_kth_test_case(k):\n",
    "    # view image\n",
    "    #silly way that I define the pixels at the start and end of each row\n",
    "    bound=1\n",
    "    bounds_list = []\n",
    "    while bound < 784:\n",
    "        bound_l = bound\n",
    "        bound += 28\n",
    "        bound_u = bound\n",
    "        bounds_list += [(bound_l,bound_u)]\n",
    "    #now use these bounds to show the train image\n",
    "    print(\"label:\",test_df.iloc[k,0])\n",
    "    img = np.array([list(test_df.iloc[k,x[0]:x[1]]) for x in bounds_list])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "view_kth_test_case(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict kth row of the test data\n",
    "\n",
    "def predict_kth_test_case(k,Nnet_instance):\n",
    "        vec = test_arr_01[k]\n",
    "        p3 = Nnet_instance.f(Nnet_instance.b[3] + ( Nnet_instance.f(Nnet_instance.b[2] + ( Nnet_instance.f(Nnet_instance.b[1] + ( vec @ Nnet_instance.W[1] )) @ Nnet_instance.W[2])) @ Nnet_instance.W[3]))\n",
    "        expvec = Nnet_instance.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default untrained net for comparison\n",
    "Defaultnet = Nnet(train_df,nu=0.02,random_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quicknet's prediction:\n",
      "1\n",
      "\n",
      "defaultnet's prediction:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "view_kth_test_case(k)\n",
    "print(\"quicknet's prediction:\")\n",
    "print(predict_kth_test_case(k,Quicknet))\n",
    "print()\n",
    "print(\"defaultnet's prediction:\")\n",
    "print(predict_kth_test_case(k,Defaultnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#make a copy of quicknet from 20250302\n",
    "Quicknet_20250302 = copy.deepcopy(Quicknet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 84 steps: 0.58695\n",
      "accuracy after 85 steps: 0.59055\n",
      "accuracy after 86 steps: 0.5935333333333334\n",
      "accuracy after 87 steps: 0.59625\n",
      "accuracy after 88 steps: 0.5997333333333333\n",
      "accuracy after 89 steps: 0.6034666666666667\n",
      "accuracy after 90 steps: 0.6061833333333333\n",
      "accuracy after 91 steps: 0.6093333333333333\n",
      "accuracy after 92 steps: 0.6121\n",
      "accuracy after 93 steps: 0.61485\n",
      "accuracy after 94 steps: 0.6176333333333334\n",
      "accuracy after 95 steps: 0.6200833333333333\n",
      "accuracy after 96 steps: 0.62295\n",
      "accuracy after 97 steps: 0.62585\n",
      "accuracy after 98 steps: 0.6287166666666667\n",
      "accuracy after 99 steps: 0.6313\n",
      "accuracy after 100 steps: 0.6336666666666667\n",
      "accuracy after 101 steps: 0.6363666666666666\n",
      "accuracy after 102 steps: 0.6389\n",
      "accuracy after 103 steps: 0.641\n",
      "accuracy after 104 steps: 0.6432833333333333\n",
      "accuracy after 105 steps: 0.6457\n",
      "accuracy after 106 steps: 0.6480833333333333\n",
      "accuracy after 107 steps: 0.6504\n",
      "accuracy after 108 steps: 0.6527333333333334\n",
      "accuracy after 109 steps: 0.6546333333333333\n",
      "accuracy after 110 steps: 0.6566833333333333\n",
      "accuracy after 111 steps: 0.6585333333333333\n",
      "accuracy after 112 steps: 0.6607166666666666\n",
      "accuracy after 113 steps: 0.6623\n",
      "accuracy after 114 steps: 0.6643333333333333\n",
      "accuracy after 115 steps: 0.6664666666666667\n",
      "accuracy after 116 steps: 0.6686666666666666\n",
      "accuracy after 117 steps: 0.6704833333333333\n",
      "accuracy after 118 steps: 0.6720833333333334\n",
      "accuracy after 119 steps: 0.67365\n",
      "accuracy after 120 steps: 0.6756166666666666\n",
      "accuracy after 121 steps: 0.6775833333333333\n",
      "accuracy after 122 steps: 0.6792\n",
      "accuracy after 123 steps: 0.6815\n",
      "accuracy after 124 steps: 0.6829666666666667\n",
      "accuracy after 125 steps: 0.6843166666666667\n",
      "accuracy after 126 steps: 0.686\n",
      "accuracy after 127 steps: 0.6877\n",
      "accuracy after 128 steps: 0.6892333333333334\n",
      "accuracy after 129 steps: 0.6910333333333334\n",
      "accuracy after 130 steps: 0.6927166666666666\n",
      "accuracy after 131 steps: 0.6943166666666667\n",
      "accuracy after 132 steps: 0.69605\n",
      "accuracy after 133 steps: 0.6976333333333333\n",
      "accuracy after 134 steps: 0.69915\n",
      "accuracy after 135 steps: 0.70055\n",
      "accuracy after 136 steps: 0.7022166666666667\n",
      "accuracy after 137 steps: 0.7037\n",
      "accuracy after 138 steps: 0.7051333333333333\n",
      "accuracy after 139 steps: 0.7069333333333333\n",
      "accuracy after 140 steps: 0.7082\n",
      "accuracy after 141 steps: 0.7095333333333333\n",
      "accuracy after 142 steps: 0.7106\n",
      "accuracy after 143 steps: 0.7119333333333333\n",
      "accuracy after 144 steps: 0.71285\n",
      "accuracy after 145 steps: 0.7144\n",
      "accuracy after 146 steps: 0.7156166666666667\n",
      "accuracy after 147 steps: 0.7165833333333333\n",
      "accuracy after 148 steps: 0.71805\n",
      "accuracy after 149 steps: 0.7193833333333334\n",
      "accuracy after 150 steps: 0.72055\n",
      "accuracy after 151 steps: 0.7218666666666667\n",
      "accuracy after 152 steps: 0.7231333333333333\n",
      "accuracy after 153 steps: 0.7241333333333333\n",
      "accuracy after 154 steps: 0.7253666666666667\n",
      "accuracy after 155 steps: 0.7264833333333334\n",
      "accuracy after 156 steps: 0.7275166666666667\n",
      "accuracy after 157 steps: 0.72835\n",
      "accuracy after 158 steps: 0.7294166666666667\n",
      "accuracy after 159 steps: 0.7304833333333334\n",
      "accuracy after 160 steps: 0.7318666666666667\n",
      "accuracy after 161 steps: 0.73305\n",
      "accuracy after 162 steps: 0.7339833333333333\n",
      "accuracy after 163 steps: 0.7346666666666667\n",
      "accuracy after 164 steps: 0.7356833333333334\n",
      "accuracy after 165 steps: 0.73665\n",
      "accuracy after 166 steps: 0.7376333333333334\n",
      "accuracy after 167 steps: 0.73855\n",
      "accuracy after 168 steps: 0.7391\n",
      "accuracy after 169 steps: 0.73975\n",
      "accuracy after 170 steps: 0.7405166666666667\n",
      "accuracy after 171 steps: 0.7412333333333333\n",
      "accuracy after 172 steps: 0.7424666666666667\n",
      "accuracy after 173 steps: 0.7432833333333333\n",
      "accuracy after 174 steps: 0.7442833333333333\n",
      "accuracy after 175 steps: 0.745\n",
      "accuracy after 176 steps: 0.7458666666666667\n",
      "accuracy after 177 steps: 0.7466333333333334\n",
      "accuracy after 178 steps: 0.74735\n",
      "accuracy after 179 steps: 0.7477833333333334\n",
      "accuracy after 180 steps: 0.7486166666666667\n",
      "accuracy after 181 steps: 0.7494\n",
      "accuracy after 182 steps: 0.7501666666666666\n",
      "accuracy after 183 steps: 0.7508166666666667\n",
      "accuracy after 184 steps: 0.7513333333333333\n",
      "accuracy after 185 steps: 0.7521166666666667\n",
      "accuracy after 186 steps: 0.7528166666666667\n",
      "accuracy after 187 steps: 0.7534333333333333\n",
      "accuracy after 188 steps: 0.7541166666666667\n",
      "accuracy after 189 steps: 0.7547166666666667\n",
      "accuracy after 190 steps: 0.7553\n",
      "accuracy after 191 steps: 0.7558333333333334\n",
      "accuracy after 192 steps: 0.75655\n",
      "accuracy after 193 steps: 0.7572166666666666\n",
      "accuracy after 194 steps: 0.7576333333333334\n",
      "accuracy after 195 steps: 0.7580833333333333\n",
      "accuracy after 196 steps: 0.7587333333333334\n",
      "accuracy after 197 steps: 0.7593333333333333\n",
      "accuracy after 198 steps: 0.7596666666666667\n",
      "accuracy after 199 steps: 0.7600833333333333\n",
      "accuracy after 200 steps: 0.7604833333333333\n",
      "accuracy after 201 steps: 0.7608666666666667\n",
      "accuracy after 202 steps: 0.7611166666666667\n",
      "accuracy after 203 steps: 0.7615\n",
      "accuracy after 204 steps: 0.7620333333333333\n",
      "accuracy after 205 steps: 0.7625166666666666\n",
      "accuracy after 206 steps: 0.76295\n",
      "accuracy after 207 steps: 0.7635166666666666\n",
      "accuracy after 208 steps: 0.7638333333333334\n",
      "accuracy after 209 steps: 0.7643833333333333\n",
      "accuracy after 210 steps: 0.7648166666666667\n",
      "accuracy after 211 steps: 0.76525\n",
      "accuracy after 212 steps: 0.7656666666666667\n",
      "accuracy after 213 steps: 0.7661333333333333\n",
      "accuracy after 214 steps: 0.76645\n",
      "accuracy after 215 steps: 0.767\n",
      "accuracy after 216 steps: 0.7673833333333333\n",
      "accuracy after 217 steps: 0.76785\n",
      "accuracy after 218 steps: 0.76825\n",
      "accuracy after 219 steps: 0.7687\n",
      "accuracy after 220 steps: 0.7692166666666667\n",
      "accuracy after 221 steps: 0.7695833333333333\n",
      "accuracy after 222 steps: 0.7699666666666667\n",
      "accuracy after 223 steps: 0.7704666666666666\n",
      "accuracy after 224 steps: 0.7709666666666667\n",
      "accuracy after 225 steps: 0.7712833333333333\n",
      "accuracy after 226 steps: 0.77165\n",
      "accuracy after 227 steps: 0.7718833333333334\n",
      "accuracy after 228 steps: 0.7723\n",
      "accuracy after 229 steps: 0.7727\n",
      "accuracy after 230 steps: 0.7729166666666667\n",
      "accuracy after 231 steps: 0.7733666666666666\n",
      "accuracy after 232 steps: 0.7737333333333334\n",
      "accuracy after 233 steps: 0.7741666666666667\n",
      "accuracy after 234 steps: 0.77445\n",
      "accuracy after 235 steps: 0.7748166666666667\n",
      "accuracy after 236 steps: 0.77515\n",
      "accuracy after 237 steps: 0.7753833333333333\n",
      "accuracy after 238 steps: 0.7757333333333334\n",
      "accuracy after 239 steps: 0.7759333333333334\n",
      "accuracy after 240 steps: 0.7762333333333333\n",
      "accuracy after 241 steps: 0.7764833333333333\n",
      "accuracy after 242 steps: 0.77675\n",
      "accuracy after 243 steps: 0.7771333333333333\n",
      "accuracy after 244 steps: 0.7774333333333333\n",
      "accuracy after 245 steps: 0.7777333333333334\n",
      "accuracy after 246 steps: 0.7779833333333334\n",
      "accuracy after 247 steps: 0.7783166666666667\n",
      "accuracy after 248 steps: 0.7784333333333333\n",
      "accuracy after 249 steps: 0.7786666666666666\n",
      "accuracy after 250 steps: 0.7789166666666667\n",
      "accuracy after 251 steps: 0.7792666666666667\n",
      "accuracy after 252 steps: 0.7794333333333333\n",
      "accuracy after 253 steps: 0.77965\n",
      "accuracy after 254 steps: 0.7797166666666666\n",
      "accuracy after 255 steps: 0.77995\n",
      "accuracy after 256 steps: 0.78015\n",
      "accuracy after 257 steps: 0.7804166666666666\n",
      "accuracy after 258 steps: 0.7806833333333333\n",
      "accuracy after 259 steps: 0.7809666666666667\n",
      "accuracy after 260 steps: 0.7813\n",
      "accuracy after 261 steps: 0.7814\n",
      "accuracy after 262 steps: 0.7816333333333333\n",
      "accuracy after 263 steps: 0.78195\n",
      "accuracy after 264 steps: 0.7822333333333333\n",
      "accuracy after 265 steps: 0.7823666666666667\n",
      "accuracy after 266 steps: 0.7825333333333333\n",
      "accuracy after 267 steps: 0.78275\n",
      "accuracy after 268 steps: 0.7828833333333334\n",
      "accuracy after 269 steps: 0.7831\n",
      "accuracy after 270 steps: 0.78335\n",
      "accuracy after 271 steps: 0.7835\n",
      "accuracy after 272 steps: 0.78375\n",
      "accuracy after 273 steps: 0.78395\n",
      "accuracy after 274 steps: 0.7841666666666667\n",
      "accuracy after 275 steps: 0.7843\n",
      "accuracy after 276 steps: 0.7845666666666666\n",
      "accuracy after 277 steps: 0.7847166666666666\n",
      "accuracy after 278 steps: 0.7849\n",
      "accuracy after 279 steps: 0.7851666666666667\n",
      "accuracy after 280 steps: 0.7852666666666667\n",
      "accuracy after 281 steps: 0.7853666666666667\n",
      "accuracy after 282 steps: 0.7855333333333333\n",
      "accuracy after 283 steps: 0.7857666666666666\n",
      "accuracy after 284 steps: 0.7859333333333334\n",
      "accuracy after 285 steps: 0.7861833333333333\n",
      "accuracy after 286 steps: 0.7864\n",
      "accuracy after 287 steps: 0.7864666666666666\n",
      "accuracy after 288 steps: 0.78675\n",
      "accuracy after 289 steps: 0.7869\n",
      "accuracy after 290 steps: 0.7871333333333334\n",
      "accuracy after 291 steps: 0.7872666666666667\n",
      "accuracy after 292 steps: 0.7874666666666666\n",
      "accuracy after 293 steps: 0.7877166666666666\n",
      "accuracy after 294 steps: 0.7879333333333334\n",
      "accuracy after 295 steps: 0.7880833333333334\n",
      "accuracy after 296 steps: 0.7882333333333333\n",
      "accuracy after 297 steps: 0.7884666666666666\n",
      "accuracy after 298 steps: 0.78865\n",
      "accuracy after 299 steps: 0.7887166666666666\n",
      "accuracy after 300 steps: 0.78885\n",
      "accuracy after 301 steps: 0.7889833333333334\n",
      "accuracy after 302 steps: 0.7890666666666667\n",
      "accuracy after 303 steps: 0.7892166666666667\n",
      "accuracy after 304 steps: 0.7894333333333333\n",
      "accuracy after 305 steps: 0.78955\n",
      "accuracy after 306 steps: 0.7897166666666666\n",
      "accuracy after 307 steps: 0.7898166666666666\n",
      "accuracy after 308 steps: 0.7900333333333334\n",
      "accuracy after 309 steps: 0.7902166666666667\n",
      "accuracy after 310 steps: 0.7903333333333333\n",
      "accuracy after 311 steps: 0.7905\n",
      "accuracy after 312 steps: 0.7906666666666666\n",
      "accuracy after 313 steps: 0.7907833333333333\n",
      "accuracy after 314 steps: 0.7909\n",
      "accuracy after 315 steps: 0.79105\n",
      "accuracy after 316 steps: 0.7912333333333333\n",
      "accuracy after 317 steps: 0.7913333333333333\n",
      "accuracy after 318 steps: 0.7915666666666666\n",
      "accuracy after 319 steps: 0.7917\n",
      "accuracy after 320 steps: 0.7917166666666666\n",
      "accuracy after 321 steps: 0.7919333333333334\n",
      "accuracy after 322 steps: 0.7922\n",
      "accuracy after 323 steps: 0.7924333333333333\n",
      "accuracy after 324 steps: 0.7925333333333333\n",
      "accuracy after 325 steps: 0.79275\n",
      "accuracy after 326 steps: 0.7930666666666667\n",
      "accuracy after 327 steps: 0.79325\n",
      "accuracy after 328 steps: 0.7933833333333333\n",
      "accuracy after 329 steps: 0.7934666666666667\n",
      "accuracy after 330 steps: 0.7936333333333333\n",
      "accuracy after 331 steps: 0.7938\n",
      "accuracy after 332 steps: 0.7938333333333333\n",
      "accuracy after 333 steps: 0.79395\n",
      "accuracy after 334 steps: 0.7940666666666667\n",
      "accuracy after 335 steps: 0.7941666666666667\n",
      "accuracy after 336 steps: 0.7943\n",
      "accuracy after 337 steps: 0.7944666666666667\n",
      "accuracy after 338 steps: 0.7946833333333333\n",
      "accuracy after 339 steps: 0.7948\n",
      "accuracy after 340 steps: 0.7949166666666667\n",
      "accuracy after 341 steps: 0.7950166666666667\n",
      "accuracy after 342 steps: 0.7951166666666667\n",
      "accuracy after 343 steps: 0.7952\n",
      "accuracy after 344 steps: 0.7953333333333333\n",
      "accuracy after 345 steps: 0.7955333333333333\n",
      "accuracy after 346 steps: 0.7956333333333333\n",
      "accuracy after 347 steps: 0.7957166666666666\n",
      "accuracy after 348 steps: 0.79585\n",
      "accuracy after 349 steps: 0.7958833333333334\n",
      "accuracy after 350 steps: 0.796\n",
      "accuracy after 351 steps: 0.7961166666666667\n",
      "accuracy after 352 steps: 0.7962333333333333\n",
      "accuracy after 353 steps: 0.7963\n",
      "accuracy after 354 steps: 0.7964\n",
      "accuracy after 355 steps: 0.79655\n",
      "accuracy after 356 steps: 0.79665\n",
      "accuracy after 357 steps: 0.7967833333333333\n",
      "accuracy after 358 steps: 0.79695\n",
      "accuracy after 359 steps: 0.7970666666666667\n",
      "accuracy after 360 steps: 0.79715\n"
     ]
    }
   ],
   "source": [
    "#now let quicknet continue training overnight\n",
    "Quicknet.train_network_v1(eps=0.00000001,max_wait_steps=5,max_steps=360)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 361 steps: 0.79725\n",
      "accuracy after 362 steps: 0.7973833333333333\n",
      "accuracy after 363 steps: 0.7974666666666667\n",
      "accuracy after 364 steps: 0.7975333333333333\n",
      "accuracy after 365 steps: 0.7976166666666666\n",
      "accuracy after 366 steps: 0.79775\n",
      "accuracy after 367 steps: 0.7979\n",
      "accuracy after 368 steps: 0.7980166666666667\n",
      "accuracy after 369 steps: 0.7981333333333334\n",
      "accuracy after 370 steps: 0.7982333333333334\n",
      "accuracy after 371 steps: 0.7983666666666667\n",
      "accuracy after 372 steps: 0.79855\n",
      "accuracy after 373 steps: 0.7987\n",
      "accuracy after 374 steps: 0.7988333333333333\n",
      "accuracy after 375 steps: 0.7989\n",
      "accuracy after 376 steps: 0.7989333333333334\n",
      "accuracy after 377 steps: 0.7991\n",
      "accuracy after 378 steps: 0.79915\n",
      "accuracy after 379 steps: 0.7993333333333333\n",
      "accuracy after 380 steps: 0.7994\n",
      "accuracy after 381 steps: 0.7994166666666667\n",
      "accuracy after 382 steps: 0.7995666666666666\n",
      "accuracy after 383 steps: 0.7996666666666666\n",
      "accuracy after 384 steps: 0.79965\n",
      "accuracy after 385 steps: 0.7997333333333333\n",
      "accuracy after 386 steps: 0.7998166666666666\n",
      "accuracy after 387 steps: 0.7999\n",
      "accuracy after 388 steps: 0.8000333333333334\n",
      "accuracy after 389 steps: 0.8002\n",
      "accuracy after 390 steps: 0.8002666666666667\n",
      "accuracy after 391 steps: 0.8005\n",
      "accuracy after 392 steps: 0.80055\n",
      "accuracy after 393 steps: 0.80065\n",
      "accuracy after 394 steps: 0.8007833333333333\n",
      "accuracy after 395 steps: 0.8009833333333334\n",
      "accuracy after 396 steps: 0.80115\n",
      "accuracy after 397 steps: 0.8011666666666667\n",
      "accuracy after 398 steps: 0.80125\n",
      "accuracy after 399 steps: 0.8013\n",
      "accuracy after 400 steps: 0.8013666666666667\n",
      "accuracy after 401 steps: 0.80135\n",
      "accuracy after 402 steps: 0.8014333333333333\n",
      "accuracy after 403 steps: 0.8015333333333333\n",
      "accuracy after 404 steps: 0.8016833333333333\n",
      "accuracy after 405 steps: 0.8016833333333333\n",
      "accuracy after 406 steps: 0.8018\n",
      "accuracy after 407 steps: 0.8018833333333333\n",
      "accuracy after 408 steps: 0.80195\n",
      "accuracy after 409 steps: 0.80205\n",
      "accuracy after 410 steps: 0.8020833333333334\n",
      "accuracy after 411 steps: 0.8021333333333334\n",
      "accuracy after 412 steps: 0.80225\n",
      "accuracy after 413 steps: 0.8023333333333333\n",
      "accuracy after 414 steps: 0.8024\n",
      "accuracy after 415 steps: 0.8024666666666667\n",
      "accuracy after 416 steps: 0.80255\n",
      "accuracy after 417 steps: 0.8026333333333333\n",
      "accuracy after 418 steps: 0.8027\n",
      "accuracy after 419 steps: 0.80275\n",
      "accuracy after 420 steps: 0.80285\n",
      "accuracy after 421 steps: 0.8029333333333334\n",
      "accuracy after 422 steps: 0.8030833333333334\n",
      "accuracy after 423 steps: 0.8031666666666667\n",
      "accuracy after 424 steps: 0.8031666666666667\n",
      "accuracy after 425 steps: 0.80325\n",
      "accuracy after 426 steps: 0.8033333333333333\n",
      "accuracy after 427 steps: 0.8033833333333333\n",
      "accuracy after 428 steps: 0.8034666666666667\n",
      "accuracy after 429 steps: 0.8035333333333333\n",
      "accuracy after 430 steps: 0.80355\n",
      "accuracy after 431 steps: 0.8036166666666666\n",
      "accuracy after 432 steps: 0.8036666666666666\n",
      "accuracy after 433 steps: 0.8037\n",
      "accuracy after 434 steps: 0.8037166666666666\n",
      "accuracy after 435 steps: 0.8037333333333333\n",
      "accuracy after 436 steps: 0.8038\n",
      "accuracy after 437 steps: 0.8039333333333334\n",
      "accuracy after 438 steps: 0.80395\n",
      "accuracy after 439 steps: 0.8040666666666667\n",
      "accuracy after 440 steps: 0.8042166666666667\n",
      "accuracy after 441 steps: 0.8043333333333333\n",
      "accuracy after 442 steps: 0.80445\n",
      "accuracy after 443 steps: 0.8045166666666667\n",
      "accuracy after 444 steps: 0.8045333333333333\n",
      "accuracy after 445 steps: 0.8046\n",
      "accuracy after 446 steps: 0.80465\n",
      "accuracy after 447 steps: 0.80475\n",
      "accuracy after 448 steps: 0.8047666666666666\n",
      "accuracy after 449 steps: 0.8049\n",
      "accuracy after 450 steps: 0.8050166666666667\n",
      "accuracy after 451 steps: 0.8051166666666667\n",
      "accuracy after 452 steps: 0.8052\n",
      "accuracy after 453 steps: 0.8052333333333334\n",
      "accuracy after 454 steps: 0.8052833333333334\n",
      "accuracy after 455 steps: 0.8053166666666667\n",
      "accuracy after 456 steps: 0.80535\n",
      "accuracy after 457 steps: 0.8054\n",
      "accuracy after 458 steps: 0.8055166666666667\n",
      "accuracy after 459 steps: 0.8056166666666666\n",
      "accuracy after 460 steps: 0.8057833333333333\n"
     ]
    }
   ],
   "source": [
    "#train some more\n",
    "Quicknet.train_network_v1(eps=0.00000001,max_wait_steps=5,max_steps=460)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8057833333333333\n",
      "total accuracy = 80.58 %\n",
      "digit accuracy:\n",
      "{0: 0.10552085092014182, 1: 0.1234055176505488, 2: 0.09516616314199396, 3: 0.10373511662045343, 4: 0.14464224580623075, 5: 0.08780667773473529, 6: 0.10645488340655627, 7: 0.1343974461292897, 8: 0.09861562126132285, 9: 0.0}\n",
      "digit_recall:\n",
      "{0: 0.0986970684039088, 1: 0.11195652173913044, 2: 0.09660351595835467, 3: 0.1015220207253886, 4: 0.09917452830188679, 5: 0.085675353432124, 6: 0.10073907455012854, 7: 0.10667856232475147, 8: 0.09199205823957644, 9: None}\n"
     ]
    }
   ],
   "source": [
    "Quicknet.all_performance()\n",
    "# please note I made a mistake in my code for digit-level accuracy and recall, corrected later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 461 steps: 0.8058666666666666\n",
      "accuracy after 462 steps: 0.8059\n",
      "accuracy after 463 steps: 0.8059833333333334\n",
      "accuracy after 464 steps: 0.80605\n",
      "accuracy after 465 steps: 0.8060666666666667\n",
      "accuracy after 466 steps: 0.8060666666666667\n",
      "accuracy after 467 steps: 0.8061166666666667\n",
      "accuracy after 468 steps: 0.8061666666666667\n",
      "accuracy after 469 steps: 0.8062\n",
      "accuracy after 470 steps: 0.8062666666666667\n",
      "accuracy after 471 steps: 0.8063333333333333\n",
      "accuracy after 472 steps: 0.8063333333333333\n",
      "accuracy after 473 steps: 0.8063333333333333\n",
      "accuracy after 474 steps: 0.8063833333333333\n",
      "accuracy after 475 steps: 0.80645\n",
      "accuracy after 476 steps: 0.8064833333333333\n",
      "accuracy after 477 steps: 0.8065833333333333\n",
      "accuracy after 478 steps: 0.8067\n",
      "accuracy after 479 steps: 0.8067166666666666\n",
      "accuracy after 480 steps: 0.8067666666666666\n",
      "accuracy after 481 steps: 0.8068\n",
      "accuracy after 482 steps: 0.8068833333333333\n",
      "accuracy after 483 steps: 0.8068833333333333\n",
      "accuracy after 484 steps: 0.8069\n",
      "accuracy after 485 steps: 0.80695\n",
      "accuracy after 486 steps: 0.8069833333333334\n",
      "accuracy after 487 steps: 0.80705\n",
      "accuracy after 488 steps: 0.8071\n",
      "accuracy after 489 steps: 0.8071333333333334\n",
      "accuracy after 490 steps: 0.8072166666666667\n",
      "accuracy after 491 steps: 0.8073166666666667\n",
      "accuracy after 492 steps: 0.8073333333333333\n",
      "accuracy after 493 steps: 0.8074666666666667\n",
      "accuracy after 494 steps: 0.8075\n",
      "accuracy after 495 steps: 0.80755\n",
      "accuracy after 496 steps: 0.8075666666666667\n",
      "accuracy after 497 steps: 0.8076166666666666\n",
      "accuracy after 498 steps: 0.8076166666666666\n",
      "accuracy after 499 steps: 0.8077333333333333\n",
      "accuracy after 500 steps: 0.8078\n",
      "accuracy after 501 steps: 0.80785\n",
      "accuracy after 502 steps: 0.80795\n",
      "accuracy after 503 steps: 0.8080166666666667\n",
      "accuracy after 504 steps: 0.8080333333333334\n",
      "accuracy after 505 steps: 0.8081166666666667\n",
      "accuracy after 506 steps: 0.8081666666666667\n",
      "accuracy after 507 steps: 0.8081666666666667\n"
     ]
    }
   ],
   "source": [
    "#train some more\n",
    "Quicknet.train_network_v1(eps=0.00000001,max_wait_steps=5,max_steps=666)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 508 steps: 0.8082666666666667\n",
      "accuracy after 509 steps: 0.8083833333333333\n",
      "accuracy after 510 steps: 0.8084666666666667\n",
      "accuracy after 511 steps: 0.8085166666666667\n",
      "accuracy after 512 steps: 0.80855\n",
      "accuracy after 513 steps: 0.8085833333333333\n",
      "accuracy after 514 steps: 0.8086833333333333\n",
      "accuracy after 515 steps: 0.8088333333333333\n",
      "accuracy after 516 steps: 0.8089\n",
      "accuracy after 517 steps: 0.8089666666666666\n"
     ]
    }
   ],
   "source": [
    "#train some more\n",
    "Quicknet.train_network_v1(eps=0.00000001,max_wait_steps=5,max_steps=517)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 518 steps: 0.8090333333333334\n",
      "accuracy after 519 steps: 0.8091166666666667\n",
      "accuracy after 520 steps: 0.8091666666666667\n",
      "accuracy after 521 steps: 0.8091833333333334\n",
      "accuracy after 522 steps: 0.8092333333333334\n"
     ]
    }
   ],
   "source": [
    "#train some more\n",
    "Quicknet.train_network_v1(eps=0.00000001,max_wait_steps=5,max_steps=522)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on own images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# darya and I drew some of our own 28x28 pixel images to test the net on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the image\n",
    "\n",
    "image = Image.open(\"drawnimg1.png\").convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "# Resize to 28x28\n",
    "# image = image.resize((28, 28))\n",
    "\n",
    "# Convert to NumPy array\n",
    "array = np.array(image)\n",
    "\n",
    "# Flatten the array into 784 elements\n",
    "flattened_array = array.flatten()\n",
    "\n",
    "# Print shape to verify\n",
    "print(flattened_array.shape)  # Should print (784,)\n",
    "\n",
    "# Display the first few pixel values\n",
    "print(flattened_array[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_array(img_path):\n",
    "    image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    array = np.array(image)\n",
    "\n",
    "    # Flatten the array into 784 elements\n",
    "    return array.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_image(pixel_array):\n",
    "    # view image\n",
    "    #silly way that I define the pixels at the start and end of each row\n",
    "    bound=0\n",
    "    bounds_list = []\n",
    "    while bound < 783:\n",
    "        bound_l = bound\n",
    "        bound += 28\n",
    "        bound_u = bound\n",
    "        bounds_list += [(bound_l,bound_u)]\n",
    "    img = np.array([list(pixel_array[x[0]:x[1]]) for x in bounds_list])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "view_image(flattened_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict_image(pixel_array,Nnet_instance):\n",
    "        vec = pixel_array\n",
    "        p3 = Nnet_instance.f(Nnet_instance.b[3] + ( Nnet_instance.f(Nnet_instance.b[2] + ( Nnet_instance.f(Nnet_instance.b[1] + ( vec @ Nnet_instance.W[1] )) @ Nnet_instance.W[2])) @ Nnet_instance.W[3]))\n",
    "        expvec = Nnet_instance.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quicknet's prediction:\n",
      "6\n",
      "\n",
      "defaultnet's prediction:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "pixel_array = img_to_array(\"drawnimg6.png\")\n",
    "view_image(pixel_array)\n",
    "print(\"quicknet's prediction:\")\n",
    "print(predict_image(pixel_array,Quicknet))\n",
    "print()\n",
    "print(\"defaultnet's prediction:\")\n",
    "print(predict_image(pixel_array,Defaultnet))\n",
    "\n",
    "#correct!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quicknet's prediction:\n",
      "2\n",
      "\n",
      "defaultnet's prediction:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "pixel_array = img_to_array(\"drawnimg22.png\")\n",
    "view_image(pixel_array)\n",
    "print(\"quicknet's prediction:\")\n",
    "print(predict_image(pixel_array,Quicknet))\n",
    "print()\n",
    "print(\"defaultnet's prediction:\")\n",
    "print(predict_image(pixel_array,Defaultnet))\n",
    "\n",
    "#correct!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAY8ElEQVR4nO3df0xV9/3H8ddF5Wpb7mWIcLkVLWqrS60sc8rQltlIBLYYfy2xXf/QxWh02ExZ28Vl1XZbwuaSruli7f7SNavamUxN/cNEsWA20UarMWYrEcYGRsDWhHsRCxr4fP/w27teBRW8l/fl8nwkn0TuOXDfPTvhucO9HDzOOScAAIZYivUAAICRiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATo60HuFNvb6+uXLmitLQ0eTwe63EAAAPknFNHR4eCwaBSUvq/zkm4AF25ckW5ubnWYwAAHlJzc7MmTpzY7/aE+xFcWlqa9QgAgBi43/fzuAVox44deuKJJzR27FgVFBTok08+eaDP48duAJAc7vf9PC4B+vDDD1VRUaFt27bp008/VX5+vkpKSnT16tV4PB0AYDhycTB37lxXXl4e+binp8cFg0FXWVl5388NhUJOEovFYrGG+QqFQvf8fh/zK6CbN2/q7NmzKi4ujjyWkpKi4uJi1dbW3rV/d3e3wuFw1AIAJL+YB+iLL75QT0+PsrOzox7Pzs5Wa2vrXftXVlbK7/dHFu+AA4CRwfxdcFu2bFEoFIqs5uZm65EAAEMg5r8HlJmZqVGjRqmtrS3q8ba2NgUCgbv293q98nq9sR4DAJDgYn4FlJqaqtmzZ6uqqiryWG9vr6qqqlRYWBjrpwMADFNxuRNCRUWFVq1ape985zuaO3eu3n77bXV2durHP/5xPJ4OADAMxSVAK1eu1Oeff66tW7eqtbVV3/rWt3TkyJG73pgAABi5PM45Zz3E14XDYfn9fusxAAAPKRQKyefz9bvd/F1wAICRiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEyMth4A9k6ePGk9AuJk3rx51iMA/eIKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw4XHOOeshvi4cDsvv91uPASScobxpLDcxRSyEQiH5fL5+t3MFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGG09AIAHM5gbhA7lDUyBgeIKCABgggABAEzEPEBvvPGGPB5P1JoxY0asnwYAMMzF5TWgp59+WseOHfvfk4zmpSYAQLS4lGH06NEKBALx+NIAgCQRl9eALl26pGAwqClTpuill15SU1NTv/t2d3crHA5HLQBA8ot5gAoKCrR7924dOXJEO3fuVGNjo5577jl1dHT0uX9lZaX8fn9k5ebmxnokAEAC8jjnXDyfoL29XZMnT9Zbb72lNWvW3LW9u7tb3d3dkY/D4TARAmJksL8HNJjfOQLuFAqF5PP5+t0e93cHpKen66mnnlJ9fX2f271er7xeb7zHAAAkmLj/HtD169fV0NCgnJyceD8VAGAYiXmAXnnlFdXU1Og///mPTp48qWXLlmnUqFF68cUXY/1UAIBhLOY/grt8+bJefPFFXbt2TRMmTNCzzz6rU6dOacKECbF+KgDAMBbzAO3bty/WXxIAkIS4FxwAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMdp6AAAP5uTJkwP+nHnz5sVhEiA2uAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJgYcoBMnTmjx4sUKBoPyeDw6ePBg1HbnnLZu3aqcnByNGzdOxcXFunTpUqzmBQAkiQEHqLOzU/n5+dqxY0ef27dv36533nlH7733nk6fPq1HH31UJSUl6urqeuhhAQDJY8B/EbWsrExlZWV9bnPO6e2339Yvf/lLLVmyRJL0/vvvKzs7WwcPHtQLL7zwcNMCAJJGTF8DamxsVGtrq4qLiyOP+f1+FRQUqLa2ts/P6e7uVjgcjloAgOQX0wC1trZKkrKzs6Mez87Ojmy7U2Vlpfx+f2Tl5ubGciQAQIIyfxfcli1bFAqFIqu5udl6JADAEIhpgAKBgCSpra0t6vG2trbItjt5vV75fL6oBQBIfjENUF5engKBgKqqqiKPhcNhnT59WoWFhbF8KgDAMDfgd8Fdv35d9fX1kY8bGxt1/vx5ZWRkaNKkSdq0aZN+85vf6Mknn1ReXp5ef/11BYNBLV26NJZzAwCGuQEH6MyZM3r++ecjH1dUVEiSVq1apd27d+u1115TZ2en1q1bp/b2dj377LM6cuSIxo4dG7upAQDDnsc556yH+LpwOCy/3289BhBXJ0+eHPDnzJs3Lw6TAPETCoXu+bq++bvgAAAjEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwM+M8xAIjGna2BweEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IkfAGc7PPocSNRYHB4QoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUgT2GBuwllYWBiHSWKntrZ2SJ6HG4QCiY8rIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhMc556yH+LpwOCy/3289BuJkMDdYTXTc+BToWygUks/n63c7V0AAABMECABgYsABOnHihBYvXqxgMCiPx6ODBw9GbV+9erU8Hk/UKi0tjdW8AIAkMeAAdXZ2Kj8/Xzt27Oh3n9LSUrW0tETW3r17H2pIAEDyGfBfRC0rK1NZWdk99/F6vQoEAoMeCgCQ/OLyGlB1dbWysrI0ffp0bdiwQdeuXet33+7uboXD4agFAEh+MQ9QaWmp3n//fVVVVel3v/udampqVFZWpp6enj73r6yslN/vj6zc3NxYjwQASEAP9XtAHo9HBw4c0NKlS/vd59///remTp2qY8eOaeHChXdt7+7uVnd3d+TjcDhMhJIYvwcEjBzmvwc0ZcoUZWZmqr6+vs/tXq9XPp8vagEAkl/cA3T58mVdu3ZNOTk58X4qAMAwMuB3wV2/fj3qaqaxsVHnz59XRkaGMjIy9Oabb2rFihUKBAJqaGjQa6+9pmnTpqmkpCSmgwMAhrcBB+jMmTN6/vnnIx9XVFRIklatWqWdO3fqwoUL+vOf/6z29nYFg0EtWrRIv/71r+X1emM3NQBg2ONmpMBDSuQ3VvAGCVgyfxMCAAB9IUAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIkB/zkGANES+Y7TiXynbimxjx3ijysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCExznnrIf4unA4LL/fbz0GgAEaqhufcgPT4SMUCsnn8/W7nSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEaOsBACSHwdwkdKhuYIrExBUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMDGgAFVWVmrOnDlKS0tTVlaWli5dqrq6uqh9urq6VF5ervHjx+uxxx7TihUr1NbWFtOhAQDD34ACVFNTo/Lycp06dUpHjx7VrVu3tGjRInV2dkb22bx5sz766CPt379fNTU1unLlipYvXx7zwQEAw5vHOecG+8mff/65srKyVFNTo6KiIoVCIU2YMEF79uzRD3/4Q0nSZ599pm9+85uqra3Vd7/73ft+zXA4LL/fP9iRAAwjg/mLqIP5y6uwEQqF5PP5+t3+UK8BhUIhSVJGRoYk6ezZs7p165aKi4sj+8yYMUOTJk1SbW1tn1+ju7tb4XA4agEAkt+gA9Tb26tNmzZp/vz5mjlzpiSptbVVqampSk9Pj9o3Oztbra2tfX6dyspK+f3+yMrNzR3sSACAYWTQASovL9fFixe1b9++hxpgy5YtCoVCkdXc3PxQXw8AMDyMHswnbdy4UYcPH9aJEyc0ceLEyOOBQEA3b95Ue3t71FVQW1ubAoFAn1/L6/XK6/UOZgwAwDA2oCsg55w2btyoAwcO6Pjx48rLy4vaPnv2bI0ZM0ZVVVWRx+rq6tTU1KTCwsLYTAwASAoDugIqLy/Xnj17dOjQIaWlpUVe1/H7/Ro3bpz8fr/WrFmjiooKZWRkyOfz6eWXX1ZhYeEDvQMOADByDChAO3fulCQtWLAg6vFdu3Zp9erVkqQ//OEPSklJ0YoVK9Td3a2SkhK9++67MRkWAJA8Hur3gOKB3wMCRg5+Dyi5xfX3gAAAGCwCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGNRfRAWAOw3mztYY2bgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDPSJJPoN4ScN2+e9Qh4AEN1HnE+jGxcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZaZJJ9Js7JvrNUnFbop9HSA5cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKYYUN7kE8BWugAAAJggQAMDEgAJUWVmpOXPmKC0tTVlZWVq6dKnq6uqi9lmwYIE8Hk/UWr9+fUyHBgAMfwMKUE1NjcrLy3Xq1CkdPXpUt27d0qJFi9TZ2Rm139q1a9XS0hJZ27dvj+nQAIDhb0BvQjhy5EjUx7t371ZWVpbOnj2roqKiyOOPPPKIAoFAbCYEACSlh3oNKBQKSZIyMjKiHv/ggw+UmZmpmTNnasuWLbpx40a/X6O7u1vhcDhqAQBGADdIPT097gc/+IGbP39+1ON/+tOf3JEjR9yFCxfcX/7yF/f444+7ZcuW9ft1tm3b5iSxWCwWK8lWKBS6Z0cGHaD169e7yZMnu+bm5nvuV1VV5SS5+vr6Prd3dXW5UCgUWc3NzeYHjcVisVgPv+4XoEH9IurGjRt1+PBhnThxQhMnTrznvgUFBZKk+vp6TZ069a7tXq9XXq93MGMAAIaxAQXIOaeXX35ZBw4cUHV1tfLy8u77OefPn5ck5eTkDGpAAEByGlCAysvLtWfPHh06dEhpaWlqbW2VJPn9fo0bN04NDQ3as2ePvv/972v8+PG6cOGCNm/erKKiIs2aNSsu/wEAgGFqIK/7qJ+f8+3atcs551xTU5MrKipyGRkZzuv1umnTprlXX331vj8H/LpQKGT+c0sWi8ViPfy63/d+z/+HJWGEw2H5/X7rMQAADykUCsnn8/W7nXvBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJFyAnHPWIwAAYuB+388TLkAdHR3WIwAAYuB+3889LsEuOXp7e3XlyhWlpaXJ4/FEbQuHw8rNzVVzc7N8Pp/RhPY4DrdxHG7jONzGcbgtEY6Dc04dHR0KBoNKSen/Omf0EM70QFJSUjRx4sR77uPz+Ub0CfYVjsNtHIfbOA63cRxusz4Ofr//vvsk3I/gAAAjAwECAJgYVgHyer3atm2bvF6v9SimOA63cRxu4zjcxnG4bTgdh4R7EwIAYGQYVldAAIDkQYAAACYIEADABAECAJgYNgHasWOHnnjiCY0dO1YFBQX65JNPrEcacm+88YY8Hk/UmjFjhvVYcXfixAktXrxYwWBQHo9HBw8ejNrunNPWrVuVk5OjcePGqbi4WJcuXbIZNo7udxxWr1591/lRWlpqM2ycVFZWas6cOUpLS1NWVpaWLl2qurq6qH26urpUXl6u8ePH67HHHtOKFSvU1tZmNHF8PMhxWLBgwV3nw/r1640m7tuwCNCHH36oiooKbdu2TZ9++qny8/NVUlKiq1evWo825J5++mm1tLRE1t///nfrkeKus7NT+fn52rFjR5/bt2/frnfeeUfvvfeeTp8+rUcffVQlJSXq6uoa4knj637HQZJKS0ujzo+9e/cO4YTxV1NTo/Lycp06dUpHjx7VrVu3tGjRInV2dkb22bx5sz766CPt379fNTU1unLlipYvX244dew9yHGQpLVr10adD9u3bzeauB9uGJg7d64rLy+PfNzT0+OCwaCrrKw0nGrobdu2zeXn51uPYUqSO3DgQOTj3t5eFwgE3O9///vIY+3t7c7r9bq9e/caTDg07jwOzjm3atUqt2TJEpN5rFy9etVJcjU1Nc652//bjxkzxu3fvz+yz7/+9S8nydXW1lqNGXd3HgfnnPve977nfvrTn9oN9QAS/gro5s2bOnv2rIqLiyOPpaSkqLi4WLW1tYaT2bh06ZKCwaCmTJmil156SU1NTdYjmWpsbFRra2vU+eH3+1VQUDAiz4/q6mplZWVp+vTp2rBhg65du2Y9UlyFQiFJUkZGhiTp7NmzunXrVtT5MGPGDE2aNCmpz4c7j8NXPvjgA2VmZmrmzJnasmWLbty4YTFevxLuZqR3+uKLL9TT06Ps7Oyox7Ozs/XZZ58ZTWWjoKBAu3fv1vTp09XS0qI333xTzz33nC5evKi0tDTr8Uy0trZKUp/nx1fbRorS0lItX75ceXl5amho0C9+8QuVlZWptrZWo0aNsh4v5np7e7Vp0ybNnz9fM2fOlHT7fEhNTVV6enrUvsl8PvR1HCTpRz/6kSZPnqxgMKgLFy7o5z//uerq6vS3v/3NcNpoCR8g/E9ZWVnk37NmzVJBQYEmT56sv/71r1qzZo3hZEgEL7zwQuTfzzzzjGbNmqWpU6equrpaCxcuNJwsPsrLy3Xx4sUR8TrovfR3HNatWxf59zPPPKOcnBwtXLhQDQ0Nmjp16lCP2aeE/xFcZmamRo0adde7WNra2hQIBIymSgzp6el66qmnVF9fbz2Kma/OAc6Pu02ZMkWZmZlJeX5s3LhRhw8f1scffxz151sCgYBu3ryp9vb2qP2T9Xzo7zj0paCgQJIS6nxI+AClpqZq9uzZqqqqijzW29urqqoqFRYWGk5m7/r162poaFBOTo71KGby8vIUCASizo9wOKzTp0+P+PPj8uXLunbtWlKdH845bdy4UQcOHNDx48eVl5cXtX327NkaM2ZM1PlQV1enpqampDof7ncc+nL+/HlJSqzzwfpdEA9i3759zuv1ut27d7t//vOfbt26dS49Pd21trZajzakfvazn7nq6mrX2Njo/vGPf7ji4mKXmZnprl69aj1aXHV0dLhz5865c+fOOUnurbfecufOnXP//e9/nXPO/fa3v3Xp6enu0KFD7sKFC27JkiUuLy/Pffnll8aTx9a9jkNHR4d75ZVXXG1trWtsbHTHjh1z3/72t92TTz7purq6rEePmQ0bNji/3++qq6tdS0tLZN24cSOyz/r1692kSZPc8ePH3ZkzZ1xhYaErLCw0nDr27ncc6uvr3a9+9St35swZ19jY6A4dOuSmTJniioqKjCePNiwC5Jxzf/zjH92kSZNcamqqmzt3rjt16pT1SENu5cqVLicnx6WmprrHH3/crVy50tXX11uPFXcff/yxk3TXWrVqlXPu9luxX3/9dZedne28Xq9buHChq6ursx06Du51HG7cuOEWLVrkJkyY4MaMGeMmT57s1q5dm3T/J62v/35JbteuXZF9vvzyS/eTn/zEfeMb33CPPPKIW7ZsmWtpabEbOg7udxyamppcUVGRy8jIcF6v102bNs29+uqrLhQK2Q5+B/4cAwDARMK/BgQASE4ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIn/Ay2AMLzlZYacAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quicknet's prediction:\n",
      "6\n",
      "\n",
      "defaultnet's prediction:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pixel_array = img_to_array(\"drawnimg1.png\")\n",
    "view_image(pixel_array)\n",
    "print(\"quicknet's prediction:\")\n",
    "print(predict_image(pixel_array,Quicknet))\n",
    "print()\n",
    "print(\"defaultnet's prediction:\")\n",
    "print(predict_image(pixel_array,Defaultnet))\n",
    "\n",
    "# it got this one wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 523 steps: 0.8093333333333333\n",
      "accuracy after 524 steps: 0.8094\n",
      "accuracy after 525 steps: 0.8094833333333333\n",
      "accuracy after 526 steps: 0.8095\n",
      "accuracy after 527 steps: 0.8095833333333333\n",
      "accuracy after 528 steps: 0.8096333333333333\n",
      "accuracy after 529 steps: 0.8096833333333333\n",
      "accuracy after 530 steps: 0.8097166666666666\n",
      "accuracy after 531 steps: 0.8097833333333333\n",
      "accuracy after 532 steps: 0.8098666666666666\n",
      "accuracy after 533 steps: 0.80995\n",
      "accuracy after 534 steps: 0.8100166666666667\n",
      "accuracy after 535 steps: 0.8100833333333334\n",
      "accuracy after 536 steps: 0.81015\n",
      "accuracy after 537 steps: 0.8102\n",
      "accuracy after 538 steps: 0.81025\n",
      "accuracy after 539 steps: 0.8102833333333334\n",
      "accuracy after 540 steps: 0.8103333333333333\n",
      "accuracy after 541 steps: 0.81035\n",
      "accuracy after 542 steps: 0.8104333333333333\n",
      "accuracy after 543 steps: 0.8104333333333333\n",
      "accuracy after 544 steps: 0.81045\n",
      "accuracy after 545 steps: 0.8104833333333333\n",
      "accuracy after 546 steps: 0.8105\n",
      "accuracy after 547 steps: 0.8105333333333333\n",
      "accuracy after 548 steps: 0.8106\n",
      "accuracy after 549 steps: 0.8106\n",
      "accuracy after 550 steps: 0.8106\n",
      "accuracy after 551 steps: 0.8106333333333333\n",
      "accuracy after 552 steps: 0.8106833333333333\n",
      "accuracy after 553 steps: 0.81075\n",
      "accuracy after 554 steps: 0.8108166666666666\n",
      "accuracy after 555 steps: 0.8108333333333333\n",
      "accuracy after 556 steps: 0.8109166666666666\n",
      "accuracy after 557 steps: 0.81095\n",
      "accuracy after 558 steps: 0.8109833333333333\n",
      "accuracy after 559 steps: 0.8109833333333333\n",
      "accuracy after 560 steps: 0.8110333333333334\n",
      "accuracy after 561 steps: 0.81105\n",
      "accuracy after 562 steps: 0.8111\n",
      "accuracy after 563 steps: 0.81115\n",
      "accuracy after 564 steps: 0.8112166666666667\n",
      "accuracy after 565 steps: 0.8112833333333334\n",
      "accuracy after 566 steps: 0.8112833333333334\n",
      "accuracy after 567 steps: 0.8113833333333333\n",
      "accuracy after 568 steps: 0.8114\n",
      "accuracy after 569 steps: 0.8114166666666667\n",
      "accuracy after 570 steps: 0.8114166666666667\n"
     ]
    }
   ],
   "source": [
    "#train some more\n",
    "Quicknet.train_network_v1(eps=0.00000001,max_wait_steps=5,max_steps=999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Nnet at 0x12ffc9850>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Quicknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all test data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent of the vector t of true labels for the test set:\n",
    "test_t = list(test_df.iloc[:,0])\n",
    "len(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quicknet specific version of predict kth test case function\n",
    "def qnet_predict_kth_test_case(k):\n",
    "        vec = test_arr_01[k]\n",
    "        p3 = Quicknet.f(Quicknet.b[3] + ( Quicknet.f(Quicknet.b[2] + ( Quicknet.f(Quicknet.b[1] + ( vec @ Quicknet.W[1] )) @ Quicknet.W[2])) @ Quicknet.W[3]))\n",
    "        expvec = Quicknet.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rework the method used to check performance in my nnet class\n",
    "def test_performance():\n",
    "    vec_predict = np.vectorize(qnet_predict_kth_test_case)\n",
    "    pred_labels1 = vec_predict(np.array(range(len(test_t))))\n",
    "\n",
    "    # TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "    true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(test_t)]\n",
    "    total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "    print(total_accuracy)\n",
    "    print(\"total test-set accuracy =\", round(100*total_accuracy,2),\"%\")\n",
    "\n",
    "    # digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "    digit_accuracy = {}\n",
    "    for digit in range(10):\n",
    "        train_cases_containing_digit = [k for k,true_val in enumerate(test_t) if true_val==digit]\n",
    "        # now calculate how many of these cases we predicted correctly\n",
    "        checklist = [pred_labels1[k]==digit for k in train_cases_containing_digit]\n",
    "        digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "    print(\"test-set digit accuracy:\")\n",
    "    print(digit_accuracy)\n",
    "\n",
    "    # digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "    digit_recall = {}\n",
    "    for digit in range(10):\n",
    "        cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "        # now calculate how many of these cases actually contained this digit correctly\n",
    "        checklist = [test_t[k]==digit for k in cases_we_predicted_digit]\n",
    "        digit_recall[digit] = sum(checklist)/len(checklist) if len(checklist) != 0 else None\n",
    "    print(\"test-set digit recall:\")\n",
    "    print(digit_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8184\n",
      "total test-set accuracy = 81.84 %\n",
      "test-set digit accuracy:\n",
      "{0: 0.9693877551020408, 1: 0.9674008810572687, 2: 0.8624031007751938, 3: 0.8910891089108911, 4: 0.9501018329938901, 5: 0.8385650224215246, 6: 0.930062630480167, 7: 0.9163424124513618, 8: 0.8542094455852156, 9: 0.0}\n",
      "test-set digit recall:\n",
      "{0: 0.9152215799614644, 1: 0.9336734693877551, 2: 0.8811881188118812, 3: 0.8620689655172413, 4: 0.6381668946648427, 5: 0.7906976744186046, 6: 0.8874501992031872, 7: 0.734789391575663, 8: 0.8015414258188824, 9: None}\n"
     ]
    }
   ],
   "source": [
    "test_performance()\n",
    "# overall I am happy to see how our neural net performs on the unseen test-set!\n",
    "\n",
    "# interestingingly my network never predicted 9, I wonder if I made a mistake somewhere that caused this (maybe...\n",
    "# the way I coded it to always predict the lowest digit in cases of ties means 9 is picked the least often?)\n",
    "\n",
    "# overall the network seems to do the best job at identifying 1s and 0s\n",
    "\n",
    "# 4 & 7 have high accuracy, but low recall, suggesting the network predicts these digits too often!\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
