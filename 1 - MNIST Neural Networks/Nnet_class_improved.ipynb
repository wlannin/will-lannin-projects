{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "having read around how modern implementations of neural networks are done, I am going to try to add a few improvements to my class, namely:\n",
    "- each step should only use a minibatch of 128 samples of the train data, not the whole dataset\n",
    "- optional \"dropout\" to prevent overfitting, where randomly during training we have neurons zero-out with 20% probability\n",
    "- do a train/validation 80/20 split, and do early stopping based on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 136194553992213785217382377961235308297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
       "0        5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4        9    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "59995    8    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59997    5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59998    6    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59999    8    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       778  779  780  781  782  783  784  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "59995    0    0    0    0    0    0    0  \n",
       "59996    0    0    0    0    0    0    0  \n",
       "59997    0    0    0    0    0    0    0  \n",
       "59998    0    0    0    0    0    0    0  \n",
       "59999    0    0    0    0    0    0    0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/wilhelmlannin/Documents/Python Stuff/MNIST_stuff/MNIST_CSV/mnist_train.csv\",header=None)\n",
    "train_df\n",
    "\n",
    "# each row is a training case\n",
    "# 0th column tells us the label for the case in that row\n",
    "# rest of the columns tell us the pixel vals (reading left to right from top left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Improved Nnet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see my rough code notebook for justifications/derivations for the code used in the class below\n",
    "\n",
    "# and see the attached pdfs for mathematical derivations of the gradients of each parameter\n",
    "\n",
    "# and see the Nnet_class.ipynb file for the state of this class before I made these improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32658274, 0.2497092 , 0.15885385, 0.19883791, 0.38386115],\n",
       "       [0.14791713, 0.35724903, 0.92428994, 0.84056598, 0.40132719],\n",
       "       [0.02834348, 0.36737094, 0.63633423, 0.9161938 , 0.32484364]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed)\n",
    "rng.random((3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = (rng.random(784) > 0.2).astype(np.float32)\n",
    "print(test_mask.shape)\n",
    "test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr_01 = np.array(train_df.iloc[:,1:]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176,)\n",
      "(148,)\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "pre_masking = train_arr_01[k]\n",
    "test_masked = np.multiply(pre_masking,test_mask)\n",
    "test_masked\n",
    "print(np.nonzero(pre_masking)[0].shape)\n",
    "print(np.nonzero(test_masked)[0].shape)\n",
    "# this masking has worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4, 3, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(range(5))\n",
    "rng.shuffle(ls)\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(range(6))\n",
    "ls[:math.ceil(len(ls)*0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nnet_improved:\n",
    "    def __init__(self,train_df,initialization='He',nu=0.01,dropouts=[0.1,0.3,0.3,0],batch_size=128,random_seed=seed):\n",
    "        #train_df: dataframe of mnist training cases \n",
    "        #initialization: method for weight initialisation. options are 'Xavier','Glorot','He'\n",
    "        #nu: learning rate, deafult 0.01\n",
    "        #dropouts: during training, zero-out a neuron in layer l with probability equal to dropouts[l]\n",
    "        #batch_size: each gradient descent step will be derived using this many samples\n",
    "        #random_seed: number that sets the randomness of ranom number generator for replicability\n",
    "\n",
    "        self.train_df = train_df\n",
    "        #set seed for random number generation\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # split into train and validation\n",
    "        # self.training_df, self.valid_df = train_test_split(train_df, test_size=0.2, random_state=random_seed, shuffle=True)\n",
    "\n",
    "        # select top 20% as validation data and use rest as train data\n",
    "        all_k = list(range(len(train_df)))\n",
    "        self.rng.shuffle(all_k)\n",
    "        valid_k = all_k[:math.ceil(len(train_df) * 0.2)]\n",
    "        training_k = list(set(all_k) - set(valid_k))\n",
    "        self.valid_df = train_df[train_df.index.isin(valid_k)].reset_index().drop(['index'],axis=1)\n",
    "        self.training_df = train_df[train_df.index.isin(training_k)].reset_index().drop(['index'],axis=1)\n",
    "\n",
    "        # map pixel values between 0 and 1\n",
    "        self.train_arr_01 = np.array(self.training_df.iloc[:,1:]) / 255\n",
    "        # and for validation\n",
    "        self.valid_arr_01 = np.array(self.valid_df.iloc[:,1:]) / 255\n",
    "\n",
    "        # true labels for each image\n",
    "        self.t = np.array(self.training_df[0])\n",
    "        #number of training images\n",
    "        self.K = len(self.t)\n",
    "\n",
    "        #number of neurons in each layer\n",
    "        self.L = {0:784, 1:128, 2:64, 3:10}\n",
    "        #bias vals initialised at 0\n",
    "        self.b = {1:0, 2:0, 3:0}\n",
    "        # weight matrices (randomly initialised using given initialization)\n",
    "        if initialization in ['Xavier','Glorot']:\n",
    "            var_scale = 1\n",
    "        elif initialization=='He':\n",
    "            var_scale = 2\n",
    "        else:\n",
    "            var_scale = 1\n",
    "        \n",
    "        self.W = {1:self.rng.normal(loc=0,scale=(var_scale/self.L[0])**0.5,size=[self.L[0],self.L[1]]),\n",
    "            2:self.rng.normal(loc=0,scale=(var_scale/self.L[1])**0.5,size=[self.L[1],self.L[2]]),\n",
    "            3:self.rng.normal(loc=0,scale=(var_scale/self.L[2])**0.5,size=[self.L[2],self.L[3]])}\n",
    "        \n",
    "        #exponential e^ function\n",
    "        self.allexp = np.vectorize(math.exp)\n",
    "        #ReLU activation function\n",
    "        self.f = np.vectorize(lambda x: max([0,x]),otypes=[float])\n",
    "\n",
    "        #learning rate parameter nu for gradient steps\n",
    "        self.nu = nu\n",
    "        #other parameters\n",
    "        self.dropouts = dropouts\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #training related data\n",
    "        self.wsums = {l:{} for l in self.L.keys()}\n",
    "        self.phat_all_k = []  # filled later\n",
    "        self.g_b = self.b.copy()\n",
    "        self.g_W = self.W.copy()\n",
    "        self.gradient_steps_taken = 0\n",
    "        self.epochs = 0\n",
    "\n",
    "        \n",
    "\n",
    "    def view_train_image_k(self,k):\n",
    "        #silly way that I define the pixels at the start and end of each row\n",
    "        bound=1\n",
    "        bounds_list = []\n",
    "        while bound < 784:\n",
    "            bound_l = bound\n",
    "            bound += 28\n",
    "            bound_u = bound\n",
    "            bounds_list += [(bound_l,bound_u)]\n",
    "        #now use these bounds to show the train image\n",
    "        print(\"label:\",self.training_df.iloc[k,0])\n",
    "        img = np.array([list(self.training_df.iloc[k,x[0]:x[1]]) for x in bounds_list])\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    def generate_phat_k(self,k):\n",
    "        vec = self.train_arr_01[k]\n",
    "        p3 = self.f(self.b[3] + ( self.f(self.b[2] + ( self.f(self.b[1] + ( vec @ self.W[1] )) @ self.W[2])) @ self.W[3]))\n",
    "        expvec = self.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        return phat\n",
    "\n",
    "    \n",
    "    def predict_k(self,k):\n",
    "        vec = self.train_arr_01[k]\n",
    "        p3 = self.f(self.b[3] + ( self.f(self.b[2] + ( self.f(self.b[1] + ( vec @ self.W[1] )) @ self.W[2])) @ self.W[3]))\n",
    "        expvec = self.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "    \n",
    "    def validation_generate_phat_k(self,k):\n",
    "        vec = self.valid_arr_01[k]\n",
    "        p3 = self.f(self.b[3] + ( self.f(self.b[2] + ( self.f(self.b[1] + ( vec @ self.W[1] )) @ self.W[2])) @ self.W[3]))\n",
    "        expvec = self.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        return phat\n",
    "    \n",
    "    def validation_loss(self):\n",
    "        # calculate loss on the validation dataset\n",
    "        # true labels for each validation image\n",
    "        vt = np.array(self.valid_df[0])\n",
    "        #number of training images\n",
    "        vK = len(vt)\n",
    "        #cross entropy loss\n",
    "        return (-1/vK)*sum([math.log(self.validation_generate_phat_k(idx)[t_val]) for idx,t_val in enumerate(vt)])\n",
    "    \n",
    "    def train_loss(self):\n",
    "        # calculate loss on the train ing dataset\n",
    "        # true labels for each image\n",
    "        t = np.array(self.training_df[0])\n",
    "        #number of training images\n",
    "        K = len(t)\n",
    "        #cross entropy loss\n",
    "        return (-1/K)*sum([math.log(self.generate_phat_k(idx)[t_val]) for idx,t_val in enumerate(t)])\n",
    "                    \n",
    "    def accuracy(self):\n",
    "        vec_predict = np.vectorize(self.predict_k)\n",
    "        pred_labels1 = vec_predict(np.array(range(len(self.t))))\n",
    "        # TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "        true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(self.t)]\n",
    "        total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "        return total_accuracy\n",
    "        \n",
    "    def all_performance(self):\n",
    "        vec_predict = np.vectorize(self.predict_k)\n",
    "        pred_labels1 = vec_predict(np.array(range(len(self.t))))\n",
    "\n",
    "        # TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "        true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(self.t)]\n",
    "        total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "        print(total_accuracy)\n",
    "        print(\"total accuracy =\", round(100*total_accuracy,2),\"%\")\n",
    "\n",
    "        # digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "        digit_accuracy = {}\n",
    "        for digit in range(10):\n",
    "            train_cases_containing_digit = [k for k,true_val in enumerate(self.t) if true_val==digit]\n",
    "            # now calculate how many of these cases we predicted correctly\n",
    "            checklist = [pred_labels1[k]==digit for k in train_cases_containing_digit]\n",
    "            digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "        print(\"digit accuracy:\")\n",
    "        print(digit_accuracy)\n",
    "\n",
    "        # digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "        digit_recall = {}\n",
    "        for digit in range(10):\n",
    "            cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "            # now calculate how many of these cases actually contained this digit correctly\n",
    "            checklist = [self.t[k]==digit for k in cases_we_predicted_digit]\n",
    "            digit_recall[digit] = sum(checklist)/len(checklist) if len(checklist) != 0 else None\n",
    "        print(\"digit_recall:\")\n",
    "        print(digit_recall)\n",
    "\n",
    "    def calc_wsums_and_phat_all_k(self,masks,k_list):\n",
    "        # we need every single weight sum for every single k, so create a data structure that allows us to store these conveniently\n",
    "\n",
    "\n",
    "        self.wsums[1] = {k:self.b[1] + (np.multiply(masks[0],self.train_arr_01[k]) @ self.W[1]) for k in k_list}\n",
    "        # now we need to define the second layer weight sums using wsums[1]\n",
    "        self.wsums[2] = {k:self.b[2] + np.multiply(masks[1],self.f(self.wsums[1][k])) @ self.W[2] for k in k_list}\n",
    "        # and define the 3rd layer weight sums using the second weight sums we just calculated\n",
    "        self.wsums[3] = {k:self.b[3] + np.multiply(masks[2],self.f(self.wsums[2][k])) @ self.W[3] for k in k_list}\n",
    "        #this way, we get all the weight sums we need and only have to do each matrix multiplication once! work samrter not harder?\n",
    "\n",
    "        # now we need to arrive at phat for each training case k from the layer 3 weightsums in wsums[3]\n",
    "        expvec_all_k = {k:self.allexp(self.f(self.wsums[3][k])) for k in k_list}\n",
    "        self.phat_all_k = {k:expvec_all_k[k]/sum(expvec_all_k[k]) for k in k_list}\n",
    "\n",
    "        #extra calcs done for all k, not k_list\n",
    "        self.i3W3_all_k_all_z_fast = {k:[\n",
    "            np.array(self.W[3][:,z]) if (self.wsums[3][k][z] > 0) else np.zeros(self.L[2]) for z in range(self.L[3])\n",
    "                ]\n",
    "                  for k in k_list       \n",
    "        }\n",
    "                \n",
    "        \n",
    "        self.i3thensum_all_k_all_z_3 = {\n",
    "                k:[\n",
    "                    np.sum(np.multiply(self.W[2][:,np.nonzero(self.wsums[2][k] > 0)[0]],\n",
    "                                #this is \"repeated\" we defined earlier\n",
    "                                np.repeat([self.W[3][np.nonzero(self.wsums[2][k] > 0)[0],z]],repeats=self.L[1],axis=0)),axis=1,initial=0)\n",
    "                                    if (self.wsums[3][k][z] > 0) else np.zeros(self.L[1]) for z in range(self.L[3])\n",
    "                ] for k in k_list\n",
    "            }\n",
    "\n",
    "\n",
    "    def calc_bias_gradients(self,masks,k_list):\n",
    "        #note that masks are not needed for the bias gradient calculations, so argument is unused!\n",
    "\n",
    "        # biases\n",
    "        self.g_b[3] = -sum([(self.wsums[3][k][self.t[k]] > 0) - sum([self.phat_all_k[k][z] for z in range(self.L[3]) if self.wsums[3][k][z] > 0]) for k in k_list]) / len(k_list)\n",
    "\n",
    "\n",
    "        self.g_b[2] = -sum([(sum([self.W[3][i][self.t[k]] for i in range(self.L[2]) if self.wsums[2][k][i] > 0]) if self.wsums[3][k][self.t[k]] > 0 else 0) \\\n",
    "        - sum([self.phat_all_k[k][z]*sum([self.W[3][i][z] for i in range(self.L[2]) if self.wsums[2][k][i] > 0]) for z in range(self.L[3]) if self.wsums[3][k][z] > 0]) for k in k_list]) / len(k_list)\n",
    "\n",
    "        self.g_b[1] = -sum(\n",
    "            [\n",
    "                (\n",
    "                    np.sum(\n",
    "                    np.multiply(self.W[3][np.nonzero(self.wsums[2][k][:] > 0),self.t[k]] , np.sum(self.W[2][np.nonzero(self.wsums[1][k][:] > 0)][:,np.nonzero(self.wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                        ) if self.wsums[3][k][self.t[k]] > 0 else 0) \\\n",
    "        - sum(\n",
    "            [\n",
    "                self.phat_all_k[k][z]*(np.sum(\n",
    "                    np.multiply(self.W[3][np.nonzero(self.wsums[2][k][:] > 0),z] , np.sum(self.W[2][np.nonzero(self.wsums[1][k][:] > 0)][:,np.nonzero(self.wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                        ) if self.wsums[3][k][z] > 0 else 0) for z in range(self.L[3]) if self.wsums[3][k][z] > 0]\n",
    "                        ) for k in k_list]) / len(k_list)\n",
    "    \n",
    "    def calc_weight_gradients(self,masks,k_list):\n",
    "\n",
    "        self.g_W[3] = np.transpose(-np.sum(np.array([np.multiply(np.array([np.repeat([( (b==self.t[k]) - self.phat_all_k[k][b] )],repeats=self.L[2]) for b in range(self.L[3])]),np.transpose(np.array(self.wsums[3][k] > 0,ndmin=2)) @ np.array(self.f(self.wsums[2][k]),ndmin=2))\n",
    "                                    for k in k_list]),axis=0) / len(k_list))\n",
    "        \n",
    "        # so, here is the final code I'll use for layer 2 weights\n",
    "        \n",
    "\n",
    "        self.g_W[2] = -sum([np.multiply(\n",
    "            #start np.multiply with the transpose of new_dotted\n",
    "            np.transpose(np.transpose(np.array(self.wsums[2][k] > 0,ndmin=2)) @ np.array(self.f(self.wsums[1][k]),ndmin=2)),\n",
    "                    #bracketsterm_repeated is defined as the right-part of this np.multiply\n",
    "                    np.repeat([self.i3W3_all_k_all_z_fast[k][self.t[k]] - np.sum(np.array([self.phat_all_k[k][z] * self.i3W3_all_k_all_z_fast[k][z] for z in range(self.L[3])]),axis=0)],repeats=self.L[1],axis=0))\n",
    "                    for k in k_list]) / len(k_list)\n",
    "        \n",
    "        # so in totality here is g_W[1]\n",
    "        \n",
    "\n",
    "        self.g_W[1] = -sum([np.multiply(\n",
    "            #start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\n",
    "            # MASK USED HERE since we actually access a neuron value here, not just a weightsum, so we zero neurons mask tells us to!\n",
    "            np.transpose(np.transpose(np.array(self.wsums[1][k] > 0,ndmin=2)) @ np.array(np.multiply(masks[0],self.train_arr_01[k]),ndmin=2)),\n",
    "                    #then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\n",
    "                    np.repeat([self.i3thensum_all_k_all_z_3[k][self.t[k]] - np.sum(np.array([self.phat_all_k[k][z] * self.i3thensum_all_k_all_z_3[k][z] for z in range(self.L[3])]),axis=0)],repeats=self.L[0],axis=0))\n",
    "                    for k in k_list]) / len(k_list)\n",
    "        \n",
    "    def take_1epoch_gradient_step(self):\n",
    "        # now divide entire train dataset into batches of batch_size size\n",
    "        batch_dict = {}\n",
    "        batch_id = 0\n",
    "        total_samples = 0\n",
    "        remaining_k = list(range(len(self.t)))\n",
    "        # shuffle the list so each epoch is batched differently\n",
    "        self.rng.shuffle(remaining_k)\n",
    "        while total_samples < self.K:\n",
    "            # take a sample of size self.batch_size from all remaining k\n",
    "            batch_dict[batch_id] = self.rng.choice(remaining_k, self.batch_size, replace=False)\n",
    "            # remove these from the remaining k\n",
    "            remaining_k = list(set(remaining_k) - set(batch_dict[batch_id]))\n",
    "            # and add to total samples and batch_id to reflect this new batch\n",
    "            total_samples += self.batch_size\n",
    "            batch_id += 1\n",
    "            # repeat until we've divided the entire dataset\n",
    "        \n",
    "        # now for each batch, perform a step\n",
    "        for batch_id,k_list in batch_dict.items():\n",
    "            # dropout mask for each layer of the network, that will randomly zero-out neurons during training\n",
    "            masks = {}\n",
    "            for layer_id in self.L:\n",
    "                #dropout for this batch with probability p, and re-scale using (1-p)\n",
    "                p = self.dropouts[layer_id]\n",
    "                masks[layer_id] = (self.rng.random(self.L[layer_id]) > p) / (1 - p)\n",
    "\n",
    "            #pre calcs - would be quicker if first method used k_list !\n",
    "            self.calc_wsums_and_phat_all_k(masks,k_list)\n",
    "            self.calc_bias_gradients(masks,k_list)\n",
    "            self.calc_weight_gradients(masks,k_list)\n",
    "\n",
    "            # now we have all the gradients calculated for this step, take the step\n",
    "            for l in [1,2,3]:\n",
    "                #gradient step for bias and weioghts corresponding to layer l\n",
    "                self.b[l] -= self.nu*self.g_b[l]\n",
    "                self.W[l] -= self.nu*self.g_W[l]\n",
    "\n",
    "            #increase number of steps taken\n",
    "            self.gradient_steps_taken += 1\n",
    "\n",
    "        #increase number of epochs\n",
    "        self.epochs += 1\n",
    "        \n",
    "        \n",
    "\n",
    "    def train_network_v2(self,eps=0.000001,max_wait_epochs=5,max_epochs=150):\n",
    "        #function to train the network via many calls to self.take_1epoch_gradient_step\n",
    "\n",
    "        # validation loss pre step\n",
    "        pre_vloss = self.validation_loss()\n",
    "\n",
    "        #initialise epochs_waited\n",
    "        epochs_waited=0\n",
    "\n",
    "        while self.epochs < max_epochs:\n",
    "\n",
    "            # step\n",
    "            self.take_1epoch_gradient_step()\n",
    "\n",
    "            #vloss after the step\n",
    "            post_vloss = self.validation_loss()\n",
    "            post_tloss = self.train_loss()\n",
    "            print(f\"After {self.epochs} epochs, train loss:\",post_tloss,\", valid loss:\",post_vloss)\n",
    "\n",
    "            # also print accuracy every 5 epochs\n",
    "            if self.epochs % 5 == 0:\n",
    "                print(f\"Train Accuracy: {round(self.accuracy(),3)} %\")\n",
    "\n",
    "            # if the vloss improved, continue\n",
    "            if post_vloss < pre_vloss:\n",
    "                pre_vloss = post_vloss\n",
    "                #also reset epochs waited\n",
    "                epochs_waited = 0\n",
    "                continue\n",
    "            else:\n",
    "                pre_vloss = post_vloss\n",
    "                #add to epochs waited\n",
    "                epochs_waited += 1\n",
    "                #if we have surpassed max steps waited we stop\n",
    "                if epochs_waited > max_wait_epochs:\n",
    "                    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Improvenet1 = Nnet_improved(train_df,initialization='He',nu=0.01,dropouts=[0,0,0,0],batch_size=128,random_seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 epochs, train loss: 1.2385195101780757 , valid loss: 1.240228655663118\n",
      "After 2 epochs, train loss: 0.7420300014663547 , valid loss: 0.7523659291612438\n",
      "After 3 epochs, train loss: 0.39697654910720415 , valid loss: 0.3985487005064886\n",
      "After 4 epochs, train loss: 0.34389680508662185 , valid loss: 0.3472790860752109\n",
      "After 5 epochs, train loss: 0.31512393910111264 , valid loss: 0.31935404409166873\n",
      "Train Accuracy: 0.911 %\n",
      "After 6 epochs, train loss: 0.29549767298828905 , valid loss: 0.3000774588545823\n",
      "After 7 epochs, train loss: 0.2786068703859967 , valid loss: 0.2841091099338159\n",
      "After 8 epochs, train loss: 0.26670534887545333 , valid loss: 0.27316052551744147\n",
      "After 9 epochs, train loss: 0.2561785463681064 , valid loss: 0.26379011753681714\n",
      "After 10 epochs, train loss: 0.24603989777954305 , valid loss: 0.25514067047517003\n",
      "Train Accuracy: 0.93 %\n",
      "After 11 epochs, train loss: 0.23896796805693918 , valid loss: 0.24833934962201826\n",
      "After 12 epochs, train loss: 0.22830713304112926 , valid loss: 0.23816262758868206\n",
      "After 13 epochs, train loss: 0.22237488607008674 , valid loss: 0.23328343296254506\n",
      "After 14 epochs, train loss: 0.2140295025965639 , valid loss: 0.2253158401823479\n",
      "After 15 epochs, train loss: 0.20714734655295164 , valid loss: 0.21936348961263868\n",
      "Train Accuracy: 0.941 %\n",
      "After 16 epochs, train loss: 0.20127368478478425 , valid loss: 0.21453785378517026\n",
      "After 17 epochs, train loss: 0.19628435028545277 , valid loss: 0.20938824089366323\n",
      "After 18 epochs, train loss: 0.18957655995476372 , valid loss: 0.20418575430595653\n",
      "After 19 epochs, train loss: 0.1847533028740874 , valid loss: 0.1992195834348647\n",
      "After 20 epochs, train loss: 0.18013473436643884 , valid loss: 0.19662902559498452\n",
      "Train Accuracy: 0.949 %\n",
      "After 21 epochs, train loss: 0.17451565971321936 , valid loss: 0.19179826169696118\n",
      "After 22 epochs, train loss: 0.170750971622182 , valid loss: 0.18684787513870366\n",
      "After 23 epochs, train loss: 0.16639561900194136 , valid loss: 0.18369405236832467\n",
      "After 24 epochs, train loss: 0.16133255129211635 , valid loss: 0.1797757784039326\n",
      "After 25 epochs, train loss: 0.15766923834957147 , valid loss: 0.17699058941949364\n",
      "Train Accuracy: 0.956 %\n",
      "After 26 epochs, train loss: 0.1538539715244154 , valid loss: 0.17303789895987023\n",
      "After 27 epochs, train loss: 0.15037542769269815 , valid loss: 0.1713767602029412\n",
      "After 28 epochs, train loss: 0.14657558342187627 , valid loss: 0.16707029604215848\n",
      "After 29 epochs, train loss: 0.143640292895379 , valid loss: 0.1645240708611694\n",
      "After 30 epochs, train loss: 0.140546786981981 , valid loss: 0.16201960459410988\n",
      "Train Accuracy: 0.961 %\n",
      "After 31 epochs, train loss: 0.13733478253019718 , valid loss: 0.15932451805441783\n",
      "After 32 epochs, train loss: 0.13419855418750024 , valid loss: 0.15675603032498142\n",
      "After 33 epochs, train loss: 0.13152154937191857 , valid loss: 0.15484352112291203\n",
      "After 34 epochs, train loss: 0.12865901911378091 , valid loss: 0.15187318553491386\n",
      "After 35 epochs, train loss: 0.12583386304687244 , valid loss: 0.1495991446073686\n",
      "Train Accuracy: 0.966 %\n",
      "After 36 epochs, train loss: 0.12392618447435014 , valid loss: 0.14899936886432083\n",
      "After 37 epochs, train loss: 0.12159768265855331 , valid loss: 0.14673186922577833\n",
      "After 38 epochs, train loss: 0.11890629697625527 , valid loss: 0.1446302164085831\n",
      "After 39 epochs, train loss: 0.11685859469763954 , valid loss: 0.14271558131403791\n",
      "After 40 epochs, train loss: 0.11591727798457582 , valid loss: 0.14173783751097557\n",
      "Train Accuracy: 0.968 %\n",
      "After 41 epochs, train loss: 0.1119797017493257 , valid loss: 0.1386718557547006\n",
      "After 42 epochs, train loss: 0.11059820650347986 , valid loss: 0.13724024239839883\n",
      "After 43 epochs, train loss: 0.10914280294011791 , valid loss: 0.13703101518892657\n",
      "After 44 epochs, train loss: 0.10614533988508913 , valid loss: 0.1340686415004596\n",
      "After 45 epochs, train loss: 0.10408090211157209 , valid loss: 0.13282933870091607\n",
      "Train Accuracy: 0.972 %\n",
      "After 46 epochs, train loss: 0.10279943813000315 , valid loss: 0.13140270835017812\n",
      "After 47 epochs, train loss: 0.10130101640603421 , valid loss: 0.13144945413674858\n",
      "After 48 epochs, train loss: 0.09879825485544086 , valid loss: 0.12865879163776356\n",
      "After 49 epochs, train loss: 0.0972388586864652 , valid loss: 0.12760397538964013\n",
      "After 50 epochs, train loss: 0.09574286441902745 , valid loss: 0.12681894421426998\n",
      "Train Accuracy: 0.974 %\n",
      "After 51 epochs, train loss: 0.09415164987078035 , valid loss: 0.12532004289644239\n",
      "After 52 epochs, train loss: 0.09286978146966074 , valid loss: 0.12432884941416561\n",
      "After 53 epochs, train loss: 0.09139138093847482 , valid loss: 0.1228554466540299\n",
      "After 54 epochs, train loss: 0.0896812289020898 , valid loss: 0.12220744035180535\n",
      "After 55 epochs, train loss: 0.08820387730593235 , valid loss: 0.1211584893995663\n",
      "Train Accuracy: 0.976 %\n",
      "After 56 epochs, train loss: 0.08707776090120173 , valid loss: 0.11934794157231533\n",
      "After 57 epochs, train loss: 0.08588333120023056 , valid loss: 0.11883260453157043\n",
      "After 58 epochs, train loss: 0.08428972185337004 , valid loss: 0.11805091262195315\n",
      "After 59 epochs, train loss: 0.08290052167640886 , valid loss: 0.11684799050173766\n",
      "After 60 epochs, train loss: 0.08115852813303556 , valid loss: 0.11595871325451226\n",
      "Train Accuracy: 0.978 %\n",
      "After 61 epochs, train loss: 0.08051132755423984 , valid loss: 0.11488681955450718\n",
      "After 62 epochs, train loss: 0.07947500062962766 , valid loss: 0.11496973583240618\n",
      "After 63 epochs, train loss: 0.07762608318355779 , valid loss: 0.11351864221867589\n",
      "After 64 epochs, train loss: 0.07634552908397903 , valid loss: 0.11274715207456643\n",
      "After 65 epochs, train loss: 0.07575086960960736 , valid loss: 0.1123510137950146\n",
      "Train Accuracy: 0.98 %\n",
      "After 66 epochs, train loss: 0.07453715978350582 , valid loss: 0.11104446285454493\n",
      "After 67 epochs, train loss: 0.07335663006404537 , valid loss: 0.11001557924944627\n",
      "After 68 epochs, train loss: 0.07246578875113806 , valid loss: 0.10983245126765166\n",
      "After 69 epochs, train loss: 0.07178259827000982 , valid loss: 0.10964707659216945\n",
      "After 70 epochs, train loss: 0.06984844684775701 , valid loss: 0.10779107876078821\n",
      "Train Accuracy: 0.982 %\n",
      "After 71 epochs, train loss: 0.06893625783447573 , valid loss: 0.10704625357185053\n",
      "After 72 epochs, train loss: 0.06777263771045573 , valid loss: 0.10646195188589641\n",
      "After 73 epochs, train loss: 0.06705139792332444 , valid loss: 0.10678605473241888\n",
      "After 74 epochs, train loss: 0.06608391075032302 , valid loss: 0.10598951652236782\n",
      "After 75 epochs, train loss: 0.06531121053142469 , valid loss: 0.10560224668505759\n",
      "Train Accuracy: 0.983 %\n",
      "After 76 epochs, train loss: 0.06441131465373363 , valid loss: 0.10479940243910696\n",
      "After 77 epochs, train loss: 0.06352461247056171 , valid loss: 0.10445228358449724\n",
      "After 78 epochs, train loss: 0.06257481635330166 , valid loss: 0.1038846156605746\n",
      "After 79 epochs, train loss: 0.061526913450318486 , valid loss: 0.10287897256500146\n",
      "After 80 epochs, train loss: 0.06056995696186573 , valid loss: 0.10232372076272017\n",
      "Train Accuracy: 0.984 %\n",
      "After 81 epochs, train loss: 0.059954161576967056 , valid loss: 0.10199362413783991\n",
      "After 82 epochs, train loss: 0.05931484890608319 , valid loss: 0.10212831739405978\n",
      "After 83 epochs, train loss: 0.05848507975140209 , valid loss: 0.10140263398663527\n",
      "After 84 epochs, train loss: 0.05743286228377 , valid loss: 0.1007530030379064\n",
      "After 85 epochs, train loss: 0.05652217968607368 , valid loss: 0.10025740758142633\n",
      "Train Accuracy: 0.985 %\n",
      "After 86 epochs, train loss: 0.05613797301056495 , valid loss: 0.0999083510504719\n",
      "After 87 epochs, train loss: 0.05594974025386936 , valid loss: 0.10072716415413521\n",
      "After 88 epochs, train loss: 0.054866288442491466 , valid loss: 0.09917759897173067\n",
      "After 89 epochs, train loss: 0.053883790251155125 , valid loss: 0.09859515716256874\n",
      "After 90 epochs, train loss: 0.053295468737361074 , valid loss: 0.09884853457800746\n",
      "Train Accuracy: 0.986 %\n",
      "After 91 epochs, train loss: 0.05263449674549389 , valid loss: 0.09813903008066709\n",
      "After 92 epochs, train loss: 0.052137453080030795 , valid loss: 0.09843244338443495\n",
      "After 93 epochs, train loss: 0.0510431126365952 , valid loss: 0.09737135644625448\n",
      "After 94 epochs, train loss: 0.0510741558210766 , valid loss: 0.0975450701092908\n",
      "After 95 epochs, train loss: 0.05116008027774662 , valid loss: 0.09816552115054836\n",
      "Train Accuracy: 0.987 %\n",
      "After 96 epochs, train loss: 0.049339393460159774 , valid loss: 0.0964645186286134\n",
      "After 97 epochs, train loss: 0.04858536730707208 , valid loss: 0.09573064242960529\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mImprovenet1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#1 epoch took around 1m15s\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[111], line 306\u001b[0m, in \u001b[0;36mNnet_improved.train_network_v2\u001b[0;34m(self, eps, max_wait_epochs, max_epochs)\u001b[0m\n\u001b[1;32m    301\u001b[0m epochs_waited\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m<\u001b[39m max_epochs:\n\u001b[1;32m    304\u001b[0m \n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_1epoch_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m#vloss after the step\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     post_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_loss()\n",
      "Cell \u001b[0;32mIn[111], line 278\u001b[0m, in \u001b[0;36mNnet_improved.take_1epoch_gradient_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_wsums_and_phat_all_k(masks,k_list)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_bias_gradients(masks,k_list)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_weight_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# now we have all the gradients calculated for this step, take the step\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m#gradient step for bias and weioghts corresponding to layer l\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[111], line 240\u001b[0m, in \u001b[0;36mNnet_improved.calc_weight_gradients\u001b[0;34m(self, masks, k_list)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_W[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m([np\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m#start np.multiply with the transpose of new_dotted\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwsums[\u001b[38;5;241m2\u001b[39m][k] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwsums[\u001b[38;5;241m1\u001b[39m][k]),ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m#bracketsterm_repeated is defined as the right-part of this np.multiply\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             np\u001b[38;5;241m.\u001b[39mrepeat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi3W3_all_k_all_z_fast[k][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt[k]] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphat_all_k[k][z] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi3W3_all_k_all_z_fast[k][z] \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL[\u001b[38;5;241m3\u001b[39m])]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)],repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL[\u001b[38;5;241m1\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_list]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_list)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# so in totality here is g_W[1]\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_W[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m([np\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m#start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# MASK USED HERE since we actually access a neuron value here, not just a weightsum, so we zero neurons mask tells us to!\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwsums[\u001b[38;5;241m1\u001b[39m][k] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mmultiply(masks[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_arr_01[k]),ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;66;03m#then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\u001b[39;00m\n\u001b[1;32m    245\u001b[0m             np\u001b[38;5;241m.\u001b[39mrepeat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi3thensum_all_k_all_z_3[k][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt[k]] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphat_all_k[k][z] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi3thensum_all_k_all_z_3[k][z] \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL[\u001b[38;5;241m3\u001b[39m])]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)],repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL[\u001b[38;5;241m0\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_list]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_list)\n",
      "Cell \u001b[0;32mIn[111], line 240\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_W[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m([np\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m#start np.multiply with the transpose of new_dotted\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwsums[\u001b[38;5;241m2\u001b[39m][k] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwsums[\u001b[38;5;241m1\u001b[39m][k]),ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m#bracketsterm_repeated is defined as the right-part of this np.multiply\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             np\u001b[38;5;241m.\u001b[39mrepeat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi3W3_all_k_all_z_fast[k][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt[k]] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphat_all_k[k][z] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi3W3_all_k_all_z_fast[k][z] \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL[\u001b[38;5;241m3\u001b[39m])]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)],repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL[\u001b[38;5;241m1\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_list]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_list)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# so in totality here is g_W[1]\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_W[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# MASK USED HERE since we actually access a neuron value here, not just a weightsum, so we zero neurons mask tells us to!\u001b[39;49;00m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwsums\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_arr_01\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\u001b[39;49;00m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi3thensum_all_k_all_z_3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphat_all_k\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi3thensum_all_k_all_z_3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrepeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_list]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_list)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "Improvenet1.train_network_v2()\n",
    "#1 epoch took around 1m15s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9877916666666666\n",
      "total accuracy = 98.78 %\n",
      "digit accuracy:\n",
      "{0: 0.9935010482180293, 1: 0.990909090909091, 2: 0.9922088860812802, 3: 0.9797119091093528, 4: 0.985501066098081, 5: 0.9884845693228926, 6: 0.9900592216582065, 7: 0.9866160607271275, 8: 0.9906143344709898, 9: 0.9802463891248938}\n",
      "digit_recall:\n",
      "{0: 0.9903866248693834, 1: 0.9929354898680052, 2: 0.9884623452905391, 3: 0.9915811088295687, 4: 0.9880290722530997, 5: 0.9862132352941176, 6: 0.9948990435706695, 7: 0.9895812462432378, 8: 0.9756302521008403, 9: 0.979206450244006}\n"
     ]
    }
   ],
   "source": [
    "Improvenet1.all_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
       "0       7    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1       2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2       1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4       4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9995    2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9997    4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9998    5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "9999    6    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "      778  779  780  781  782  783  784  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9995    0    0    0    0    0    0    0  \n",
       "9996    0    0    0    0    0    0    0  \n",
       "9997    0    0    0    0    0    0    0  \n",
       "9998    0    0    0    0    0    0    0  \n",
       "9999    0    0    0    0    0    0    0  \n",
       "\n",
       "[10000 rows x 785 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/Users/wilhelmlannin/Documents/Python Stuff/MNIST_stuff/MNIST_CSV/mnist_test.csv\",header=None)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr_01 = np.array(test_df.iloc[:,1:]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_kth_test_case(k):\n",
    "    # view image\n",
    "    #silly way that I define the pixels at the start and end of each row\n",
    "    bound=1\n",
    "    bounds_list = []\n",
    "    while bound < 784:\n",
    "        bound_l = bound\n",
    "        bound += 28\n",
    "        bound_u = bound\n",
    "        bounds_list += [(bound_l,bound_u)]\n",
    "    #now use these bounds to show the train image\n",
    "    print(\"label:\",test_df.iloc[k,0])\n",
    "    img = np.array([list(test_df.iloc[k,x[0]:x[1]]) for x in bounds_list])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "view_kth_test_case(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict kth row of the test data\n",
    "\n",
    "def predict_kth_test_case(k,Nnet_instance):\n",
    "        vec = test_arr_01[k]\n",
    "        p3 = Nnet_instance.f(Nnet_instance.b[3] + ( Nnet_instance.f(Nnet_instance.b[2] + ( Nnet_instance.f(Nnet_instance.b[1] + ( vec @ Nnet_instance.W[1] )) @ Nnet_instance.W[2])) @ Nnet_instance.W[3]))\n",
    "        expvec = Nnet_instance.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvenet1's prediction:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "view_kth_test_case(k)\n",
    "print(\"Improvenet1's prediction:\")\n",
    "print(predict_kth_test_case(k,Improvenet1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#make a copy of quicknet from 20250302\n",
    "# Improvenet1_20250322 = copy.deepcopy(Improvenet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on own images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# darya and I drew some of our own 28x28 pixel images to test the net on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the image\n",
    "\n",
    "image = Image.open(\"drawnimg1.png\").convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "# Resize to 28x28\n",
    "# image = image.resize((28, 28))\n",
    "\n",
    "# Convert to NumPy array\n",
    "array = np.array(image)\n",
    "\n",
    "# Flatten the array into 784 elements\n",
    "flattened_array = array.flatten()\n",
    "\n",
    "# Print shape to verify\n",
    "print(flattened_array.shape)  # Should print (784,)\n",
    "\n",
    "# Display the first few pixel values\n",
    "print(flattened_array[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_array(img_path):\n",
    "    image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    array = np.array(image)\n",
    "\n",
    "    # Flatten the array into 784 elements\n",
    "    return array.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       181, 181, 181, 181, 181, 181, 181, 181, 181,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 181, 181,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       181,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 181,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 181,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 181,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 181,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 181,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 181,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 181,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 181, 181, 181,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 181, 234, 234, 234, 181, 181,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       181, 181, 181, 181,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 181, 181, 181,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 181, 181,\n",
       "       181,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 181, 181,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 181,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 181,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 181,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 181, 181,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       181, 181, 181,   0,   0,   0,   0,   0,   0,   0,   0, 181, 181,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 181, 181, 181, 181, 181, 181, 181, 181,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAY8ElEQVR4nO3df0xV9/3H8ddF5Wpb7mWIcLkVLWqrS60sc8rQltlIBLYYfy2xXf/QxWh02ExZ28Vl1XZbwuaSruli7f7SNavamUxN/cNEsWA20UarMWYrEcYGRsDWhHsRCxr4fP/w27teBRW8l/fl8nwkn0TuOXDfPTvhucO9HDzOOScAAIZYivUAAICRiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATo60HuFNvb6+uXLmitLQ0eTwe63EAAAPknFNHR4eCwaBSUvq/zkm4AF25ckW5ubnWYwAAHlJzc7MmTpzY7/aE+xFcWlqa9QgAgBi43/fzuAVox44deuKJJzR27FgVFBTok08+eaDP48duAJAc7vf9PC4B+vDDD1VRUaFt27bp008/VX5+vkpKSnT16tV4PB0AYDhycTB37lxXXl4e+binp8cFg0FXWVl5388NhUJOEovFYrGG+QqFQvf8fh/zK6CbN2/q7NmzKi4ujjyWkpKi4uJi1dbW3rV/d3e3wuFw1AIAJL+YB+iLL75QT0+PsrOzox7Pzs5Wa2vrXftXVlbK7/dHFu+AA4CRwfxdcFu2bFEoFIqs5uZm65EAAEMg5r8HlJmZqVGjRqmtrS3q8ba2NgUCgbv293q98nq9sR4DAJDgYn4FlJqaqtmzZ6uqqiryWG9vr6qqqlRYWBjrpwMADFNxuRNCRUWFVq1ape985zuaO3eu3n77bXV2durHP/5xPJ4OADAMxSVAK1eu1Oeff66tW7eqtbVV3/rWt3TkyJG73pgAABi5PM45Zz3E14XDYfn9fusxAAAPKRQKyefz9bvd/F1wAICRiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEyMth4A9k6ePGk9AuJk3rx51iMA/eIKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw4XHOOeshvi4cDsvv91uPASScobxpLDcxRSyEQiH5fL5+t3MFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGG09AIAHM5gbhA7lDUyBgeIKCABgggABAEzEPEBvvPGGPB5P1JoxY0asnwYAMMzF5TWgp59+WseOHfvfk4zmpSYAQLS4lGH06NEKBALx+NIAgCQRl9eALl26pGAwqClTpuill15SU1NTv/t2d3crHA5HLQBA8ot5gAoKCrR7924dOXJEO3fuVGNjo5577jl1dHT0uX9lZaX8fn9k5ebmxnokAEAC8jjnXDyfoL29XZMnT9Zbb72lNWvW3LW9u7tb3d3dkY/D4TARAmJksL8HNJjfOQLuFAqF5PP5+t0e93cHpKen66mnnlJ9fX2f271er7xeb7zHAAAkmLj/HtD169fV0NCgnJyceD8VAGAYiXmAXnnlFdXU1Og///mPTp48qWXLlmnUqFF68cUXY/1UAIBhLOY/grt8+bJefPFFXbt2TRMmTNCzzz6rU6dOacKECbF+KgDAMBbzAO3bty/WXxIAkIS4FxwAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMdp6AAAP5uTJkwP+nHnz5sVhEiA2uAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJgYcoBMnTmjx4sUKBoPyeDw6ePBg1HbnnLZu3aqcnByNGzdOxcXFunTpUqzmBQAkiQEHqLOzU/n5+dqxY0ef27dv36533nlH7733nk6fPq1HH31UJSUl6urqeuhhAQDJY8B/EbWsrExlZWV9bnPO6e2339Yvf/lLLVmyRJL0/vvvKzs7WwcPHtQLL7zwcNMCAJJGTF8DamxsVGtrq4qLiyOP+f1+FRQUqLa2ts/P6e7uVjgcjloAgOQX0wC1trZKkrKzs6Mez87Ojmy7U2Vlpfx+f2Tl5ubGciQAQIIyfxfcli1bFAqFIqu5udl6JADAEIhpgAKBgCSpra0t6vG2trbItjt5vV75fL6oBQBIfjENUF5engKBgKqqqiKPhcNhnT59WoWFhbF8KgDAMDfgd8Fdv35d9fX1kY8bGxt1/vx5ZWRkaNKkSdq0aZN+85vf6Mknn1ReXp5ef/11BYNBLV26NJZzAwCGuQEH6MyZM3r++ecjH1dUVEiSVq1apd27d+u1115TZ2en1q1bp/b2dj377LM6cuSIxo4dG7upAQDDnsc556yH+LpwOCy/3289BhBXJ0+eHPDnzJs3Lw6TAPETCoXu+bq++bvgAAAjEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwM+M8xAIjGna2BweEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IkfAGc7PPocSNRYHB4QoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUgT2GBuwllYWBiHSWKntrZ2SJ6HG4QCiY8rIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhMc556yH+LpwOCy/3289BuJkMDdYTXTc+BToWygUks/n63c7V0AAABMECABgYsABOnHihBYvXqxgMCiPx6ODBw9GbV+9erU8Hk/UKi0tjdW8AIAkMeAAdXZ2Kj8/Xzt27Oh3n9LSUrW0tETW3r17H2pIAEDyGfBfRC0rK1NZWdk99/F6vQoEAoMeCgCQ/OLyGlB1dbWysrI0ffp0bdiwQdeuXet33+7uboXD4agFAEh+MQ9QaWmp3n//fVVVVel3v/udampqVFZWpp6enj73r6yslN/vj6zc3NxYjwQASEAP9XtAHo9HBw4c0NKlS/vd59///remTp2qY8eOaeHChXdt7+7uVnd3d+TjcDhMhJIYvwcEjBzmvwc0ZcoUZWZmqr6+vs/tXq9XPp8vagEAkl/cA3T58mVdu3ZNOTk58X4qAMAwMuB3wV2/fj3qaqaxsVHnz59XRkaGMjIy9Oabb2rFihUKBAJqaGjQa6+9pmnTpqmkpCSmgwMAhrcBB+jMmTN6/vnnIx9XVFRIklatWqWdO3fqwoUL+vOf/6z29nYFg0EtWrRIv/71r+X1emM3NQBg2ONmpMBDSuQ3VvAGCVgyfxMCAAB9IUAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIkB/zkGANES+Y7TiXynbimxjx3ijysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCExznnrIf4unA4LL/fbz0GgAEaqhufcgPT4SMUCsnn8/W7nSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEaOsBACSHwdwkdKhuYIrExBUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMDGgAFVWVmrOnDlKS0tTVlaWli5dqrq6uqh9urq6VF5ervHjx+uxxx7TihUr1NbWFtOhAQDD34ACVFNTo/Lycp06dUpHjx7VrVu3tGjRInV2dkb22bx5sz766CPt379fNTU1unLlipYvXx7zwQEAw5vHOecG+8mff/65srKyVFNTo6KiIoVCIU2YMEF79uzRD3/4Q0nSZ599pm9+85uqra3Vd7/73ft+zXA4LL/fP9iRAAwjg/mLqIP5y6uwEQqF5PP5+t3+UK8BhUIhSVJGRoYk6ezZs7p165aKi4sj+8yYMUOTJk1SbW1tn1+ju7tb4XA4agEAkt+gA9Tb26tNmzZp/vz5mjlzpiSptbVVqampSk9Pj9o3Oztbra2tfX6dyspK+f3+yMrNzR3sSACAYWTQASovL9fFixe1b9++hxpgy5YtCoVCkdXc3PxQXw8AMDyMHswnbdy4UYcPH9aJEyc0ceLEyOOBQEA3b95Ue3t71FVQW1ubAoFAn1/L6/XK6/UOZgwAwDA2oCsg55w2btyoAwcO6Pjx48rLy4vaPnv2bI0ZM0ZVVVWRx+rq6tTU1KTCwsLYTAwASAoDugIqLy/Xnj17dOjQIaWlpUVe1/H7/Ro3bpz8fr/WrFmjiooKZWRkyOfz6eWXX1ZhYeEDvQMOADByDChAO3fulCQtWLAg6vFdu3Zp9erVkqQ//OEPSklJ0YoVK9Td3a2SkhK9++67MRkWAJA8Hur3gOKB3wMCRg5+Dyi5xfX3gAAAGCwCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGNRfRAWAOw3mztYY2bgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDPSJJPoN4ScN2+e9Qh4AEN1HnE+jGxcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZaZJJ9Js7JvrNUnFbop9HSA5cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKYYUN7kE8BWugAAAJggQAMDEgAJUWVmpOXPmKC0tTVlZWVq6dKnq6uqi9lmwYIE8Hk/UWr9+fUyHBgAMfwMKUE1NjcrLy3Xq1CkdPXpUt27d0qJFi9TZ2Rm139q1a9XS0hJZ27dvj+nQAIDhb0BvQjhy5EjUx7t371ZWVpbOnj2roqKiyOOPPPKIAoFAbCYEACSlh3oNKBQKSZIyMjKiHv/ggw+UmZmpmTNnasuWLbpx40a/X6O7u1vhcDhqAQBGADdIPT097gc/+IGbP39+1ON/+tOf3JEjR9yFCxfcX/7yF/f444+7ZcuW9ft1tm3b5iSxWCwWK8lWKBS6Z0cGHaD169e7yZMnu+bm5nvuV1VV5SS5+vr6Prd3dXW5UCgUWc3NzeYHjcVisVgPv+4XoEH9IurGjRt1+PBhnThxQhMnTrznvgUFBZKk+vp6TZ069a7tXq9XXq93MGMAAIaxAQXIOaeXX35ZBw4cUHV1tfLy8u77OefPn5ck5eTkDGpAAEByGlCAysvLtWfPHh06dEhpaWlqbW2VJPn9fo0bN04NDQ3as2ePvv/972v8+PG6cOGCNm/erKKiIs2aNSsu/wEAgGFqIK/7qJ+f8+3atcs551xTU5MrKipyGRkZzuv1umnTprlXX331vj8H/LpQKGT+c0sWi8ViPfy63/d+z/+HJWGEw2H5/X7rMQAADykUCsnn8/W7nXvBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJFyAnHPWIwAAYuB+388TLkAdHR3WIwAAYuB+3889LsEuOXp7e3XlyhWlpaXJ4/FEbQuHw8rNzVVzc7N8Pp/RhPY4DrdxHG7jONzGcbgtEY6Dc04dHR0KBoNKSen/Omf0EM70QFJSUjRx4sR77uPz+Ub0CfYVjsNtHIfbOA63cRxusz4Ofr//vvsk3I/gAAAjAwECAJgYVgHyer3atm2bvF6v9SimOA63cRxu4zjcxnG4bTgdh4R7EwIAYGQYVldAAIDkQYAAACYIEADABAECAJgYNgHasWOHnnjiCY0dO1YFBQX65JNPrEcacm+88YY8Hk/UmjFjhvVYcXfixAktXrxYwWBQHo9HBw8ejNrunNPWrVuVk5OjcePGqbi4WJcuXbIZNo7udxxWr1591/lRWlpqM2ycVFZWas6cOUpLS1NWVpaWLl2qurq6qH26urpUXl6u8ePH67HHHtOKFSvU1tZmNHF8PMhxWLBgwV3nw/r1640m7tuwCNCHH36oiooKbdu2TZ9++qny8/NVUlKiq1evWo825J5++mm1tLRE1t///nfrkeKus7NT+fn52rFjR5/bt2/frnfeeUfvvfeeTp8+rUcffVQlJSXq6uoa4knj637HQZJKS0ujzo+9e/cO4YTxV1NTo/Lycp06dUpHjx7VrVu3tGjRInV2dkb22bx5sz766CPt379fNTU1unLlipYvX244dew9yHGQpLVr10adD9u3bzeauB9uGJg7d64rLy+PfNzT0+OCwaCrrKw0nGrobdu2zeXn51uPYUqSO3DgQOTj3t5eFwgE3O9///vIY+3t7c7r9bq9e/caTDg07jwOzjm3atUqt2TJEpN5rFy9etVJcjU1Nc652//bjxkzxu3fvz+yz7/+9S8nydXW1lqNGXd3HgfnnPve977nfvrTn9oN9QAS/gro5s2bOnv2rIqLiyOPpaSkqLi4WLW1tYaT2bh06ZKCwaCmTJmil156SU1NTdYjmWpsbFRra2vU+eH3+1VQUDAiz4/q6mplZWVp+vTp2rBhg65du2Y9UlyFQiFJUkZGhiTp7NmzunXrVtT5MGPGDE2aNCmpz4c7j8NXPvjgA2VmZmrmzJnasmWLbty4YTFevxLuZqR3+uKLL9TT06Ps7Oyox7Ozs/XZZ58ZTWWjoKBAu3fv1vTp09XS0qI333xTzz33nC5evKi0tDTr8Uy0trZKUp/nx1fbRorS0lItX75ceXl5amho0C9+8QuVlZWptrZWo0aNsh4v5np7e7Vp0ybNnz9fM2fOlHT7fEhNTVV6enrUvsl8PvR1HCTpRz/6kSZPnqxgMKgLFy7o5z//uerq6vS3v/3NcNpoCR8g/E9ZWVnk37NmzVJBQYEmT56sv/71r1qzZo3hZEgEL7zwQuTfzzzzjGbNmqWpU6equrpaCxcuNJwsPsrLy3Xx4sUR8TrovfR3HNatWxf59zPPPKOcnBwtXLhQDQ0Nmjp16lCP2aeE/xFcZmamRo0adde7WNra2hQIBIymSgzp6el66qmnVF9fbz2Kma/OAc6Pu02ZMkWZmZlJeX5s3LhRhw8f1scffxz151sCgYBu3ryp9vb2qP2T9Xzo7zj0paCgQJIS6nxI+AClpqZq9uzZqqqqijzW29urqqoqFRYWGk5m7/r162poaFBOTo71KGby8vIUCASizo9wOKzTp0+P+PPj8uXLunbtWlKdH845bdy4UQcOHNDx48eVl5cXtX327NkaM2ZM1PlQV1enpqampDof7ncc+nL+/HlJSqzzwfpdEA9i3759zuv1ut27d7t//vOfbt26dS49Pd21trZajzakfvazn7nq6mrX2Njo/vGPf7ji4mKXmZnprl69aj1aXHV0dLhz5865c+fOOUnurbfecufOnXP//e9/nXPO/fa3v3Xp6enu0KFD7sKFC27JkiUuLy/Pffnll8aTx9a9jkNHR4d75ZVXXG1trWtsbHTHjh1z3/72t92TTz7purq6rEePmQ0bNji/3++qq6tdS0tLZN24cSOyz/r1692kSZPc8ePH3ZkzZ1xhYaErLCw0nDr27ncc6uvr3a9+9St35swZ19jY6A4dOuSmTJniioqKjCePNiwC5Jxzf/zjH92kSZNcamqqmzt3rjt16pT1SENu5cqVLicnx6WmprrHH3/crVy50tXX11uPFXcff/yxk3TXWrVqlXPu9luxX3/9dZedne28Xq9buHChq6ursx06Du51HG7cuOEWLVrkJkyY4MaMGeMmT57s1q5dm3T/J62v/35JbteuXZF9vvzyS/eTn/zEfeMb33CPPPKIW7ZsmWtpabEbOg7udxyamppcUVGRy8jIcF6v102bNs29+uqrLhQK2Q5+B/4cAwDARMK/BgQASE4ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIn/Ay2AMLzlZYacAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_image(pixel_array):\n",
    "    # view image\n",
    "    #silly way that I define the pixels at the start and end of each row\n",
    "    bound=0\n",
    "    bounds_list = []\n",
    "    while bound < 783:\n",
    "        bound_l = bound\n",
    "        bound += 28\n",
    "        bound_u = bound\n",
    "        bounds_list += [(bound_l,bound_u)]\n",
    "    img = np.array([list(pixel_array[x[0]:x[1]]) for x in bounds_list])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "view_image(flattened_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict_image(pixel_array,Nnet_instance):\n",
    "        vec = pixel_array / 255\n",
    "        p3 = Nnet_instance.f(Nnet_instance.b[3] + ( Nnet_instance.f(Nnet_instance.b[2] + ( Nnet_instance.f(Nnet_instance.b[1] + ( vec @ Nnet_instance.W[1] )) @ Nnet_instance.W[2])) @ Nnet_instance.W[3]))\n",
    "        expvec = Nnet_instance.allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYx0lEQVR4nO3df0zU9x3H8depcNUWjiLCcRUpaqtJrSxzyoirayJR3GLqjz9c1z/sYmy0ZzN17RaXqO2yhM0mzdLFrPtLs6zazmRo6h8mioLZhja1GmPWEWFsYORwNeF7iIIGPvuD9dZTEME73nfn85F8Ern7Am++fsuzX+7LV59zzgkAgHE2wXoAAMCjiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATk6wHuNvAwICuXr2qnJwc+Xw+63EAAKPknFN3d7dCoZAmTBj+PCflAnT16lWVlJRYjwEAeEjt7e2aPn36sM+n3I/gcnJyrEcAACTASN/PkxagvXv36umnn9Zjjz2miooKffrppw/0fvzYDQAyw0jfz5MSoI8//ljbt2/X7t279fnnn6u8vFzLly/XtWvXkvHpAADpyCXBokWLXDgcjr3d39/vQqGQq6mpGfF9Pc9zklgsFouV5svzvPt+v0/4GdDt27d17tw5VVVVxR6bMGGCqqqq1NjYeM/2fX19ikajcQsAkPkSHqAvv/xS/f39Kioqinu8qKhIkUjknu1ramoUCARiiyvgAODRYH4V3I4dO+R5Xmy1t7dbjwQAGAcJ/z2ggoICTZw4UZ2dnXGPd3Z2KhgM3rO93++X3+9P9BgAgBSX8DOg7OxsLViwQHV1dbHHBgYGVFdXp8rKykR/OgBAmkrKnRC2b9+u9evX61vf+pYWLVqk3/zmN+rp6dGPfvSjZHw6AEAaSkqA1q1bp//85z/atWuXIpGIvvGNb+jYsWP3XJgAAHh0+ZxzznqIr4tGowoEAtZjAAAekud5ys3NHfZ586vgAACPJgIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGBikvUAAB6Mc856hITz+XzWI8AQZ0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRgp8TSrf8JMbdyLTcAYEADBBgAAAJhIeoLfffls+ny9uzZ07N9GfBgCQ5pLyGtBzzz2nEydO/P+TTOKlJgBAvKSUYdKkSQoGg8n40ACADJGU14AuX76sUCikmTNn6pVXXlFbW9uw2/b19SkajcYtAEDmS3iAKioqtH//fh07dky/+93v1NraqhdeeEHd3d1Dbl9TU6NAIBBbJSUliR4JAJCCfC7Jv/jQ1dWl0tJSvffee9qwYcM9z/f19amvry/2djQaJUIww+8BAYnjeZ5yc3OHfT7pVwfk5eXp2WefVXNz85DP+/1++f3+ZI8BAEgxSf89oBs3bqilpUXFxcXJ/lQAgDSS8AC9+eabamho0L/+9S/97W9/0+rVqzVx4kS9/PLLif5UAIA0lvAfwV25ckUvv/yyrl+/rmnTpuk73/mOzpw5o2nTpiX6UwEA0ljSL0IYrWg0qkAgYD0GUkiKHaL34OIAYGgjXYTAveAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABNJ/wfpgK8brxuLcoNQIPVxBgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3A0bY8adrQE8DM6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3IwU44obiwL4CmdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJbkaKceWcG/X7cANTIDNxBgQAMEGAAAAmRh2g06dPa+XKlQqFQvL5fDp8+HDc88457dq1S8XFxZo8ebKqqqp0+fLlRM0LAMgQow5QT0+PysvLtXfv3iGf37Nnj95//3198MEHOnv2rB5//HEtX75cvb29Dz0sACCDuIcgydXW1sbeHhgYcMFg0L377ruxx7q6upzf73cHDx58oI/peZ6TxEqDNV6sv04WizW25Xneff/bTuhrQK2trYpEIqqqqoo9FggEVFFRocbGxiHfp6+vT9FoNG4BADJfQgMUiUQkSUVFRXGPFxUVxZ67W01NjQKBQGyVlJQkciQAQIoyvwpux44d8jwvttrb261HAgCMg4QGKBgMSpI6OzvjHu/s7Iw9dze/36/c3Ny4BQDIfAkNUFlZmYLBoOrq6mKPRaNRnT17VpWVlYn8VACANDfqW/HcuHFDzc3NsbdbW1t14cIF5efna8aMGdq6dat++ctf6plnnlFZWZl27typUCikVatWJXJuAEC6G+0lsadOnRrycrv169c75wYvxd65c6crKipyfr/fLV261DU1NT3wx+cy7PRZ48X662SxWGNbI12G7fvff+ApIxqNKhAIWI+BB5Bih04cbmAK2PM8776v65tfBQcAeDQRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxKj/PSBknrHe1Xq87jg9lvlS/WsCwBkQAMAIAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5Ei5Y3lBqFjvRnpWN4vlW/KOp64kStGizMgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyNFRhrrjTHHcsPPVL5JaKrvB25g+mjjDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSIGv4eaYg8ayH1L5pqxITZwBAQBMECAAgIlRB+j06dNauXKlQqGQfD6fDh8+HPf8q6++Kp/PF7eqq6sTNS8AIEOMOkA9PT0qLy/X3r17h92murpaHR0dsXXw4MGHGhIAkHlGfRHCihUrtGLFivtu4/f7FQwGxzwUACDzJeU1oPr6ehUWFmrOnDnavHmzrl+/Puy2fX19ikajcQsAkPkSHqDq6mr94Q9/UF1dnX7961+roaFBK1asUH9//5Db19TUKBAIxFZJSUmiRwIApCCfe4iL930+n2pra7Vq1apht/nnP/+pWbNm6cSJE1q6dOk9z/f19amvry/2djQaJULjbKyHAL8zg68by3HEMZTZPM9Tbm7usM8n/TLsmTNnqqCgQM3NzUM+7/f7lZubG7cAAJkv6QG6cuWKrl+/ruLi4mR/KgBAGhn1VXA3btyIO5tpbW3VhQsXlJ+fr/z8fL3zzjtau3atgsGgWlpa9NOf/lSzZ8/W8uXLEzo4ACDNuVE6deqUk3TPWr9+vbt586ZbtmyZmzZtmsvKynKlpaVu48aNLhKJPPDH9zxvyI/PSt4aK+u5Wam1OIZYdy/P8+779/9QFyEkQzQaVSAQsB7jkTKehwAvOqeH8TomOB4ym/lFCAAADIUAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmRv3vASHzjPWOxGO5Y/JY3oc7JqcH/p4wWpwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkpxmwsN5/kBqbjayz7TmL/YXxwBgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBmpBhXqXwDUwDjizMgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyNFyhvLDUwBpD7OgAAAJggQAMDEqAJUU1OjhQsXKicnR4WFhVq1apWampritunt7VU4HNbUqVP1xBNPaO3aters7Ezo0ACA9DeqADU0NCgcDuvMmTM6fvy47ty5o2XLlqmnpye2zbZt2/TJJ5/o0KFDamho0NWrV7VmzZqEDw4ASHPuIVy7ds1Jcg0NDc4557q6ulxWVpY7dOhQbJsvvvjCSXKNjY0P9DE9z3OSWCwWi5Xmy/O8+36/f6jXgDzPkyTl5+dLks6dO6c7d+6oqqoqts3cuXM1Y8YMNTY2Dvkx+vr6FI1G4xYAIPONOUADAwPaunWrFi9erHnz5kmSIpGIsrOzlZeXF7dtUVGRIpHIkB+npqZGgUAgtkpKSsY6EgAgjYw5QOFwWJcuXdJHH330UAPs2LFDnufFVnt7+0N9PABAehjTL6Ju2bJFR48e1enTpzV9+vTY48FgULdv31ZXV1fcWVBnZ6eCweCQH8vv98vv949lDABAGhvVGZBzTlu2bFFtba1OnjypsrKyuOcXLFigrKws1dXVxR5rampSW1ubKisrEzMxACAjjOoMKBwO68CBAzpy5IhycnJir+sEAgFNnjxZgUBAGzZs0Pbt25Wfn6/c3Fy98cYbqqys1Le//e2kfAEAgDQ1msuuNcyldvv27Yttc+vWLff666+7J5980k2ZMsWtXr3adXR0PPDn4DJsFovFyow10mXYvv+FJWVEo1EFAgHrMQAAD8nzPOXm5g77PPeCAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJkYVoJqaGi1cuFA5OTkqLCzUqlWr1NTUFLfNiy++KJ/PF7c2bdqU0KEBAOlvVAFqaGhQOBzWmTNndPz4cd25c0fLli1TT09P3HYbN25UR0dHbO3ZsyehQwMA0t+k0Wx87NixuLf379+vwsJCnTt3TkuWLIk9PmXKFAWDwcRMCADISA/1GpDneZKk/Pz8uMc//PBDFRQUaN68edqxY4du3rw57Mfo6+tTNBqNWwCAR4Abo/7+fvf973/fLV68OO7x3//+9+7YsWPu4sWL7o9//KN76qmn3OrVq4f9OLt373aSWCwWi5Vhy/O8+3ZkzAHatGmTKy0tde3t7ffdrq6uzklyzc3NQz7f29vrPM+Lrfb2dvOdxmKxWKyHXyMFaFSvAX1ly5YtOnr0qE6fPq3p06ffd9uKigpJUnNzs2bNmnXP836/X36/fyxjAADS2KgC5JzTG2+8odraWtXX16usrGzE97lw4YIkqbi4eEwDAgAy06gCFA6HdeDAAR05ckQ5OTmKRCKSpEAgoMmTJ6ulpUUHDhzQ9773PU2dOlUXL17Utm3btGTJEs2fPz8pXwAAIE2N5nUfDfNzvn379jnnnGtra3NLlixx+fn5zu/3u9mzZ7u33nprxJ8Dfp3neeY/t2SxWCzWw6+Rvvf7/heWlBGNRhUIBKzHAAA8JM/zlJubO+zz3AsOAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAi5QLknLMeAQCQACN9P0+5AHV3d1uPAABIgJG+n/tcip1yDAwM6OrVq8rJyZHP54t7LhqNqqSkRO3t7crNzTWa0B77YRD7YRD7YRD7YVAq7AfnnLq7uxUKhTRhwvDnOZPGcaYHMmHCBE2fPv2+2+Tm5j7SB9hX2A+D2A+D2A+D2A+DrPdDIBAYcZuU+xEcAODRQIAAACbSKkB+v1+7d++W3++3HsUU+2EQ+2EQ+2EQ+2FQOu2HlLsIAQDwaEirMyAAQOYgQAAAEwQIAGCCAAEATKRNgPbu3aunn35ajz32mCoqKvTpp59ajzTu3n77bfl8vrg1d+5c67GS7vTp01q5cqVCoZB8Pp8OHz4c97xzTrt27VJxcbEmT56sqqoqXb582WbYJBppP7z66qv3HB/V1dU2wyZJTU2NFi5cqJycHBUWFmrVqlVqamqK26a3t1fhcFhTp07VE088obVr16qzs9No4uR4kP3w4osv3nM8bNq0yWjioaVFgD7++GNt375du3fv1ueff67y8nItX75c165dsx5t3D333HPq6OiIrb/85S/WIyVdT0+PysvLtXfv3iGf37Nnj95//3198MEHOnv2rB5//HEtX75cvb294zxpco20HySpuro67vg4ePDgOE6YfA0NDQqHwzpz5oyOHz+uO3fuaNmyZerp6Ylts23bNn3yySc6dOiQGhoadPXqVa1Zs8Zw6sR7kP0gSRs3bow7Hvbs2WM08TBcGli0aJELh8Oxt/v7+10oFHI1NTWGU42/3bt3u/LycusxTElytbW1sbcHBgZcMBh07777buyxrq4u5/f73cGDBw0mHB937wfnnFu/fr176aWXTOaxcu3aNSfJNTQ0OOcG/+6zsrLcoUOHYtt88cUXTpJrbGy0GjPp7t4Pzjn33e9+1/34xz+2G+oBpPwZ0O3bt3Xu3DlVVVXFHpswYYKqqqrU2NhoOJmNy5cvKxQKaebMmXrllVfU1tZmPZKp1tZWRSKRuOMjEAiooqLikTw+6uvrVVhYqDlz5mjz5s26fv269UhJ5XmeJCk/P1+SdO7cOd25cyfueJg7d65mzJiR0cfD3fvhKx9++KEKCgo0b9487dixQzdv3rQYb1gpdzPSu3355Zfq7+9XUVFR3ONFRUX6xz/+YTSVjYqKCu3fv19z5sxRR0eH3nnnHb3wwgu6dOmScnJyrMczEYlEJGnI4+Or5x4V1dXVWrNmjcrKytTS0qKf//znWrFihRobGzVx4kTr8RJuYGBAW7du1eLFizVv3jxJg8dDdna28vLy4rbN5ONhqP0gST/84Q9VWlqqUCikixcv6mc/+5mampr05z//2XDaeCkfIPzfihUrYn+eP3++KioqVFpaqj/96U/asGGD4WRIBT/4wQ9if37++ec1f/58zZo1S/X19Vq6dKnhZMkRDod16dKlR+J10PsZbj+89tprsT8///zzKi4u1tKlS9XS0qJZs2aN95hDSvkfwRUUFGjixIn3XMXS2dmpYDBoNFVqyMvL07PPPqvm5mbrUcx8dQxwfNxr5syZKigoyMjjY8uWLTp69KhOnToV98+3BINB3b59W11dXXHbZ+rxMNx+GEpFRYUkpdTxkPIBys7O1oIFC1RXVxd7bGBgQHV1daqsrDSczN6NGzfU0tKi4uJi61HMlJWVKRgMxh0f0WhUZ8+efeSPjytXruj69esZdXw457RlyxbV1tbq5MmTKisri3t+wYIFysrKijsempqa1NbWllHHw0j7YSgXLlyQpNQ6HqyvgngQH330kfP7/W7//v3u73//u3vttddcXl6ei0Qi1qONq5/85Ceuvr7etba2ur/+9a+uqqrKFRQUuGvXrlmPllTd3d3u/Pnz7vz5806Se++999z58+fdv//9b+ecc7/61a9cXl6eO3LkiLt48aJ76aWXXFlZmbt165bx5Il1v/3Q3d3t3nzzTdfY2OhaW1vdiRMn3De/+U33zDPPuN7eXuvRE2bz5s0uEAi4+vp619HREVs3b96MbbNp0yY3Y8YMd/LkSffZZ5+5yspKV1lZaTh14o20H5qbm90vfvEL99lnn7nW1lZ35MgRN3PmTLdkyRLjyeOlRYCcc+63v/2tmzFjhsvOznaLFi1yZ86csR5p3K1bt84VFxe77Oxs99RTT7l169a55uZm67GS7tSpU07SPWv9+vXOucFLsXfu3OmKioqc3+93S5cudU1NTbZDJ8H99sPNmzfdsmXL3LRp01xWVpYrLS11GzduzLj/SRvq65fk9u3bF9vm1q1b7vXXX3dPPvmkmzJlilu9erXr6OiwGzoJRtoPbW1tbsmSJS4/P9/5/X43e/Zs99ZbbznP82wHvwv/HAMAwETKvwYEAMhMBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJ/wKRtgpPVhN0oAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvenet1's prediction:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "pixel_array = img_to_array(\"drawnimg6.png\")\n",
    "view_image(pixel_array)\n",
    "print(\"Improvenet1's prediction:\")\n",
    "print(predict_image(pixel_array,Improvenet1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvenet1's prediction:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pixel_array = img_to_array(\"drawnimg22.png\")\n",
    "view_image(pixel_array)\n",
    "print(\"Improvenet1's prediction:\")\n",
    "print(predict_image(pixel_array,Improvenet1))\n",
    "\n",
    "#correct!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAY8ElEQVR4nO3df0xV9/3H8ddF5Wpb7mWIcLkVLWqrS60sc8rQltlIBLYYfy2xXf/QxWh02ExZ28Vl1XZbwuaSruli7f7SNavamUxN/cNEsWA20UarMWYrEcYGRsDWhHsRCxr4fP/w27teBRW8l/fl8nwkn0TuOXDfPTvhucO9HDzOOScAAIZYivUAAICRiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATo60HuFNvb6+uXLmitLQ0eTwe63EAAAPknFNHR4eCwaBSUvq/zkm4AF25ckW5ubnWYwAAHlJzc7MmTpzY7/aE+xFcWlqa9QgAgBi43/fzuAVox44deuKJJzR27FgVFBTok08+eaDP48duAJAc7vf9PC4B+vDDD1VRUaFt27bp008/VX5+vkpKSnT16tV4PB0AYDhycTB37lxXXl4e+binp8cFg0FXWVl5388NhUJOEovFYrGG+QqFQvf8fh/zK6CbN2/q7NmzKi4ujjyWkpKi4uJi1dbW3rV/d3e3wuFw1AIAJL+YB+iLL75QT0+PsrOzox7Pzs5Wa2vrXftXVlbK7/dHFu+AA4CRwfxdcFu2bFEoFIqs5uZm65EAAEMg5r8HlJmZqVGjRqmtrS3q8ba2NgUCgbv293q98nq9sR4DAJDgYn4FlJqaqtmzZ6uqqiryWG9vr6qqqlRYWBjrpwMADFNxuRNCRUWFVq1ape985zuaO3eu3n77bXV2durHP/5xPJ4OADAMxSVAK1eu1Oeff66tW7eqtbVV3/rWt3TkyJG73pgAABi5PM45Zz3E14XDYfn9fusxAAAPKRQKyefz9bvd/F1wAICRiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEyMth4A9k6ePGk9AuJk3rx51iMA/eIKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw4XHOOeshvi4cDsvv91uPASScobxpLDcxRSyEQiH5fL5+t3MFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGG09AIAHM5gbhA7lDUyBgeIKCABgggABAEzEPEBvvPGGPB5P1JoxY0asnwYAMMzF5TWgp59+WseOHfvfk4zmpSYAQLS4lGH06NEKBALx+NIAgCQRl9eALl26pGAwqClTpuill15SU1NTv/t2d3crHA5HLQBA8ot5gAoKCrR7924dOXJEO3fuVGNjo5577jl1dHT0uX9lZaX8fn9k5ebmxnokAEAC8jjnXDyfoL29XZMnT9Zbb72lNWvW3LW9u7tb3d3dkY/D4TARAmJksL8HNJjfOQLuFAqF5PP5+t0e93cHpKen66mnnlJ9fX2f271er7xeb7zHAAAkmLj/HtD169fV0NCgnJyceD8VAGAYiXmAXnnlFdXU1Og///mPTp48qWXLlmnUqFF68cUXY/1UAIBhLOY/grt8+bJefPFFXbt2TRMmTNCzzz6rU6dOacKECbF+KgDAMBbzAO3bty/WXxIAkIS4FxwAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMdp6AAAP5uTJkwP+nHnz5sVhEiA2uAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJgYcoBMnTmjx4sUKBoPyeDw6ePBg1HbnnLZu3aqcnByNGzdOxcXFunTpUqzmBQAkiQEHqLOzU/n5+dqxY0ef27dv36533nlH7733nk6fPq1HH31UJSUl6urqeuhhAQDJY8B/EbWsrExlZWV9bnPO6e2339Yvf/lLLVmyRJL0/vvvKzs7WwcPHtQLL7zwcNMCAJJGTF8DamxsVGtrq4qLiyOP+f1+FRQUqLa2ts/P6e7uVjgcjloAgOQX0wC1trZKkrKzs6Mez87Ojmy7U2Vlpfx+f2Tl5ubGciQAQIIyfxfcli1bFAqFIqu5udl6JADAEIhpgAKBgCSpra0t6vG2trbItjt5vV75fL6oBQBIfjENUF5engKBgKqqqiKPhcNhnT59WoWFhbF8KgDAMDfgd8Fdv35d9fX1kY8bGxt1/vx5ZWRkaNKkSdq0aZN+85vf6Mknn1ReXp5ef/11BYNBLV26NJZzAwCGuQEH6MyZM3r++ecjH1dUVEiSVq1apd27d+u1115TZ2en1q1bp/b2dj377LM6cuSIxo4dG7upAQDDnsc556yH+LpwOCy/3289BhBXJ0+eHPDnzJs3Lw6TAPETCoXu+bq++bvgAAAjEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwM+M8xAIjGna2BweEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IkfAGc7PPocSNRYHB4QoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUgT2GBuwllYWBiHSWKntrZ2SJ6HG4QCiY8rIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhMc556yH+LpwOCy/3289BuJkMDdYTXTc+BToWygUks/n63c7V0AAABMECABgYsABOnHihBYvXqxgMCiPx6ODBw9GbV+9erU8Hk/UKi0tjdW8AIAkMeAAdXZ2Kj8/Xzt27Oh3n9LSUrW0tETW3r17H2pIAEDyGfBfRC0rK1NZWdk99/F6vQoEAoMeCgCQ/OLyGlB1dbWysrI0ffp0bdiwQdeuXet33+7uboXD4agFAEh+MQ9QaWmp3n//fVVVVel3v/udampqVFZWpp6enj73r6yslN/vj6zc3NxYjwQASEAP9XtAHo9HBw4c0NKlS/vd59///remTp2qY8eOaeHChXdt7+7uVnd3d+TjcDhMhJIYvwcEjBzmvwc0ZcoUZWZmqr6+vs/tXq9XPp8vagEAkl/cA3T58mVdu3ZNOTk58X4qAMAwMuB3wV2/fj3qaqaxsVHnz59XRkaGMjIy9Oabb2rFihUKBAJqaGjQa6+9pmnTpqmkpCSmgwMAhrcBB+jMmTN6/vnnIx9XVFRIklatWqWdO3fqwoUL+vOf/6z29nYFg0EtWrRIv/71r+X1emM3NQBg2ONmpMBDSuQ3VvAGCVgyfxMCAAB9IUAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIkB/zkGANES+Y7TiXynbimxjx3ijysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCExznnrIf4unA4LL/fbz0GgAEaqhufcgPT4SMUCsnn8/W7nSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEaOsBACSHwdwkdKhuYIrExBUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMDGgAFVWVmrOnDlKS0tTVlaWli5dqrq6uqh9urq6VF5ervHjx+uxxx7TihUr1NbWFtOhAQDD34ACVFNTo/Lycp06dUpHjx7VrVu3tGjRInV2dkb22bx5sz766CPt379fNTU1unLlipYvXx7zwQEAw5vHOecG+8mff/65srKyVFNTo6KiIoVCIU2YMEF79uzRD3/4Q0nSZ599pm9+85uqra3Vd7/73ft+zXA4LL/fP9iRAAwjg/mLqIP5y6uwEQqF5PP5+t3+UK8BhUIhSVJGRoYk6ezZs7p165aKi4sj+8yYMUOTJk1SbW1tn1+ju7tb4XA4agEAkt+gA9Tb26tNmzZp/vz5mjlzpiSptbVVqampSk9Pj9o3Oztbra2tfX6dyspK+f3+yMrNzR3sSACAYWTQASovL9fFixe1b9++hxpgy5YtCoVCkdXc3PxQXw8AMDyMHswnbdy4UYcPH9aJEyc0ceLEyOOBQEA3b95Ue3t71FVQW1ubAoFAn1/L6/XK6/UOZgwAwDA2oCsg55w2btyoAwcO6Pjx48rLy4vaPnv2bI0ZM0ZVVVWRx+rq6tTU1KTCwsLYTAwASAoDugIqLy/Xnj17dOjQIaWlpUVe1/H7/Ro3bpz8fr/WrFmjiooKZWRkyOfz6eWXX1ZhYeEDvQMOADByDChAO3fulCQtWLAg6vFdu3Zp9erVkqQ//OEPSklJ0YoVK9Td3a2SkhK9++67MRkWAJA8Hur3gOKB3wMCRg5+Dyi5xfX3gAAAGCwCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGNRfRAWAOw3mztYY2bgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDPSJJPoN4ScN2+e9Qh4AEN1HnE+jGxcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZaZJJ9Js7JvrNUnFbop9HSA5cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKYYUN7kE8BWugAAAJggQAMDEgAJUWVmpOXPmKC0tTVlZWVq6dKnq6uqi9lmwYIE8Hk/UWr9+fUyHBgAMfwMKUE1NjcrLy3Xq1CkdPXpUt27d0qJFi9TZ2Rm139q1a9XS0hJZ27dvj+nQAIDhb0BvQjhy5EjUx7t371ZWVpbOnj2roqKiyOOPPPKIAoFAbCYEACSlh3oNKBQKSZIyMjKiHv/ggw+UmZmpmTNnasuWLbpx40a/X6O7u1vhcDhqAQBGADdIPT097gc/+IGbP39+1ON/+tOf3JEjR9yFCxfcX/7yF/f444+7ZcuW9ft1tm3b5iSxWCwWK8lWKBS6Z0cGHaD169e7yZMnu+bm5nvuV1VV5SS5+vr6Prd3dXW5UCgUWc3NzeYHjcVisVgPv+4XoEH9IurGjRt1+PBhnThxQhMnTrznvgUFBZKk+vp6TZ069a7tXq9XXq93MGMAAIaxAQXIOaeXX35ZBw4cUHV1tfLy8u77OefPn5ck5eTkDGpAAEByGlCAysvLtWfPHh06dEhpaWlqbW2VJPn9fo0bN04NDQ3as2ePvv/972v8+PG6cOGCNm/erKKiIs2aNSsu/wEAgGFqIK/7qJ+f8+3atcs551xTU5MrKipyGRkZzuv1umnTprlXX331vj8H/LpQKGT+c0sWi8ViPfy63/d+z/+HJWGEw2H5/X7rMQAADykUCsnn8/W7nXvBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJFyAnHPWIwAAYuB+388TLkAdHR3WIwAAYuB+3889LsEuOXp7e3XlyhWlpaXJ4/FEbQuHw8rNzVVzc7N8Pp/RhPY4DrdxHG7jONzGcbgtEY6Dc04dHR0KBoNKSen/Omf0EM70QFJSUjRx4sR77uPz+Ub0CfYVjsNtHIfbOA63cRxusz4Ofr//vvsk3I/gAAAjAwECAJgYVgHyer3atm2bvF6v9SimOA63cRxu4zjcxnG4bTgdh4R7EwIAYGQYVldAAIDkQYAAACYIEADABAECAJgYNgHasWOHnnjiCY0dO1YFBQX65JNPrEcacm+88YY8Hk/UmjFjhvVYcXfixAktXrxYwWBQHo9HBw8ejNrunNPWrVuVk5OjcePGqbi4WJcuXbIZNo7udxxWr1591/lRWlpqM2ycVFZWas6cOUpLS1NWVpaWLl2qurq6qH26urpUXl6u8ePH67HHHtOKFSvU1tZmNHF8PMhxWLBgwV3nw/r1640m7tuwCNCHH36oiooKbdu2TZ9++qny8/NVUlKiq1evWo825J5++mm1tLRE1t///nfrkeKus7NT+fn52rFjR5/bt2/frnfeeUfvvfeeTp8+rUcffVQlJSXq6uoa4knj637HQZJKS0ujzo+9e/cO4YTxV1NTo/Lycp06dUpHjx7VrVu3tGjRInV2dkb22bx5sz766CPt379fNTU1unLlipYvX244dew9yHGQpLVr10adD9u3bzeauB9uGJg7d64rLy+PfNzT0+OCwaCrrKw0nGrobdu2zeXn51uPYUqSO3DgQOTj3t5eFwgE3O9///vIY+3t7c7r9bq9e/caTDg07jwOzjm3atUqt2TJEpN5rFy9etVJcjU1Nc652//bjxkzxu3fvz+yz7/+9S8nydXW1lqNGXd3HgfnnPve977nfvrTn9oN9QAS/gro5s2bOnv2rIqLiyOPpaSkqLi4WLW1tYaT2bh06ZKCwaCmTJmil156SU1NTdYjmWpsbFRra2vU+eH3+1VQUDAiz4/q6mplZWVp+vTp2rBhg65du2Y9UlyFQiFJUkZGhiTp7NmzunXrVtT5MGPGDE2aNCmpz4c7j8NXPvjgA2VmZmrmzJnasmWLbty4YTFevxLuZqR3+uKLL9TT06Ps7Oyox7Ozs/XZZ58ZTWWjoKBAu3fv1vTp09XS0qI333xTzz33nC5evKi0tDTr8Uy0trZKUp/nx1fbRorS0lItX75ceXl5amho0C9+8QuVlZWptrZWo0aNsh4v5np7e7Vp0ybNnz9fM2fOlHT7fEhNTVV6enrUvsl8PvR1HCTpRz/6kSZPnqxgMKgLFy7o5z//uerq6vS3v/3NcNpoCR8g/E9ZWVnk37NmzVJBQYEmT56sv/71r1qzZo3hZEgEL7zwQuTfzzzzjGbNmqWpU6equrpaCxcuNJwsPsrLy3Xx4sUR8TrovfR3HNatWxf59zPPPKOcnBwtXLhQDQ0Nmjp16lCP2aeE/xFcZmamRo0adde7WNra2hQIBIymSgzp6el66qmnVF9fbz2Kma/OAc6Pu02ZMkWZmZlJeX5s3LhRhw8f1scffxz151sCgYBu3ryp9vb2qP2T9Xzo7zj0paCgQJIS6nxI+AClpqZq9uzZqqqqijzW29urqqoqFRYWGk5m7/r162poaFBOTo71KGby8vIUCASizo9wOKzTp0+P+PPj8uXLunbtWlKdH845bdy4UQcOHNDx48eVl5cXtX327NkaM2ZM1PlQV1enpqampDof7ncc+nL+/HlJSqzzwfpdEA9i3759zuv1ut27d7t//vOfbt26dS49Pd21trZajzakfvazn7nq6mrX2Njo/vGPf7ji4mKXmZnprl69aj1aXHV0dLhz5865c+fOOUnurbfecufOnXP//e9/nXPO/fa3v3Xp6enu0KFD7sKFC27JkiUuLy/Pffnll8aTx9a9jkNHR4d75ZVXXG1trWtsbHTHjh1z3/72t92TTz7purq6rEePmQ0bNji/3++qq6tdS0tLZN24cSOyz/r1692kSZPc8ePH3ZkzZ1xhYaErLCw0nDr27ncc6uvr3a9+9St35swZ19jY6A4dOuSmTJniioqKjCePNiwC5Jxzf/zjH92kSZNcamqqmzt3rjt16pT1SENu5cqVLicnx6WmprrHH3/crVy50tXX11uPFXcff/yxk3TXWrVqlXPu9luxX3/9dZedne28Xq9buHChq6ursx06Du51HG7cuOEWLVrkJkyY4MaMGeMmT57s1q5dm3T/J62v/35JbteuXZF9vvzyS/eTn/zEfeMb33CPPPKIW7ZsmWtpabEbOg7udxyamppcUVGRy8jIcF6v102bNs29+uqrLhQK2Q5+B/4cAwDARMK/BgQASE4ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIn/Ay2AMLzlZYacAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvenet1's prediction:\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pixel_array = img_to_array(\"drawnimg1.png\")\n",
    "view_image(pixel_array)\n",
    "print(\"Improvenet1's prediction:\")\n",
    "print(predict_image(pixel_array,Improvenet1))\n",
    "print()\n",
    "\n",
    "# it got this one wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all test data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent of the vector t of true labels for the test set:\n",
    "test_t = list(test_df.iloc[:,0])\n",
    "len(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific version of predict kth test case function\n",
    "def inet_predict_kth_test_case(k):\n",
    "        vec = test_arr_01[k]\n",
    "        p3 = Improvenet1.f(Improvenet1.b[3] + ( Improvenet1.f(Improvenet1.b[2] + ( Improvenet1.f(Improvenet1.b[1] + ( vec @ Improvenet1.W[1] )) @ Improvenet1.W[2])) @ Improvenet1.W[3]))\n",
    "        expvec = Improvenet1    .allexp(p3)\n",
    "        phat = expvec/sum(expvec)\n",
    "        #return the first label that has the highest softmax probability in phat\n",
    "        return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rework the method used to check performance in my nnet class\n",
    "def test_performance():\n",
    "    vec_predict = np.vectorize(inet_predict_kth_test_case)\n",
    "    pred_labels1 = vec_predict(np.array(range(len(test_t))))\n",
    "\n",
    "    # TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "    true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(test_t)]\n",
    "    total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "    print(total_accuracy)\n",
    "    print(\"total test-set accuracy =\", round(100*total_accuracy,2),\"%\")\n",
    "\n",
    "    # digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "    digit_accuracy = {}\n",
    "    for digit in range(10):\n",
    "        train_cases_containing_digit = [k for k,true_val in enumerate(test_t) if true_val==digit]\n",
    "        # now calculate how many of these cases we predicted correctly\n",
    "        checklist = [pred_labels1[k]==digit for k in train_cases_containing_digit]\n",
    "        digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "    print(\"test-set digit accuracy:\")\n",
    "    print(digit_accuracy)\n",
    "\n",
    "    # digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "    digit_recall = {}\n",
    "    for digit in range(10):\n",
    "        cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "        # now calculate how many of these cases actually contained this digit correctly\n",
    "        checklist = [test_t[k]==digit for k in cases_we_predicted_digit]\n",
    "        digit_recall[digit] = sum(checklist)/len(checklist) if len(checklist) != 0 else None\n",
    "    print(\"test-set digit recall:\")\n",
    "    print(digit_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714\n",
      "total test-set accuracy = 97.14 %\n",
      "test-set digit accuracy:\n",
      "{0: 0.9857142857142858, 1: 0.9885462555066079, 2: 0.9718992248062015, 3: 0.9722772277227723, 4: 0.9643584521384929, 5: 0.9697309417040358, 6: 0.9728601252609603, 7: 0.9678988326848249, 8: 0.9671457905544147, 9: 0.9514370664023786}\n",
      "test-set digit recall:\n",
      "{0: 0.972809667673716, 1: 0.9894179894179894, 2: 0.9775828460038987, 3: 0.9665354330708661, 4: 0.9742798353909465, 5: 0.958980044345898, 6: 0.9789915966386554, 7: 0.9726295210166178, 8: 0.9563451776649746, 9: 0.9628886659979939}\n"
     ]
    }
   ],
   "source": [
    "test_performance()\n",
    "# overall I am VERY happy to see how our neural net performs on the unseen test-set!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save quicknet weights and biases as json\n",
    "# biases = Improvenet1.b\n",
    "\n",
    "# print(biases)\n",
    "\n",
    "\n",
    "# with open('Improvenet1_biases.json', 'w') as fp:\n",
    "#     json.dump(biases, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = Improvenet1.W\n",
    "\n",
    "# with open('Improvenet1_weights.json', 'w') as fp:\n",
    "#     json.dump(weights, fp, cls=NumpyArrayEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
