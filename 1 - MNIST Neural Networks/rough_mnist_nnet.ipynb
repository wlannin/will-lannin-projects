{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_NNET (MNIST Dataset Neural Network Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a 128 bit random number as the seed for my random number generation (when initialising parameters)\n",
    "\n",
    "# import secrets\n",
    "# secrets.randbits(128)\n",
    "\n",
    "# here is the one I got\n",
    "# 136194553992213785217382377961235308297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 136194553992213785217382377961235308297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
       "0        5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4        9    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "59995    8    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59997    5    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59998    6    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59999    8    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       778  779  780  781  782  783  784  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "59995    0    0    0    0    0    0    0  \n",
       "59996    0    0    0    0    0    0    0  \n",
       "59997    0    0    0    0    0    0    0  \n",
       "59998    0    0    0    0    0    0    0  \n",
       "59999    0    0    0    0    0    0    0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/wilhelmlannin/Documents/Python Stuff/MNIST_stuff/MNIST_CSV/mnist_train.csv\",header=None)\n",
    "train_df\n",
    "\n",
    "# each row is a training case\n",
    "# 0th column tells us the label for the case in that row\n",
    "# rest of the columns tell us the pixel vals (reading left to right from top left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    784.000000\n",
       "mean      35.108418\n",
       "std       79.699674\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max      255.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just wanted to verify that these pixel values are between 0 and 255\n",
    "train_df.iloc[0,1:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 29),\n",
       " (29, 57),\n",
       " (57, 85),\n",
       " (85, 113),\n",
       " (113, 141),\n",
       " (141, 169),\n",
       " (169, 197),\n",
       " (197, 225),\n",
       " (225, 253),\n",
       " (253, 281),\n",
       " (281, 309),\n",
       " (309, 337),\n",
       " (337, 365),\n",
       " (365, 393),\n",
       " (393, 421),\n",
       " (421, 449),\n",
       " (449, 477),\n",
       " (477, 505),\n",
       " (505, 533),\n",
       " (533, 561),\n",
       " (561, 589),\n",
       " (589, 617),\n",
       " (617, 645),\n",
       " (645, 673),\n",
       " (673, 701),\n",
       " (701, 729),\n",
       " (729, 757),\n",
       " (757, 785)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here i define the lower and upper bounds needed to get each set of 28 pixels that form each row of the image!\n",
    "# keep adding 28 to 1 until we hit 784\n",
    "bound=1\n",
    "bounds_list = []\n",
    "while bound < 784:\n",
    "    bound_l = bound\n",
    "    bound += 28\n",
    "    bound_u = bound\n",
    "    bounds_list += [(bound_l,bound_u)]\n",
    "bounds_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#get pixels of kth image in 28x28 format in array called img\n",
    "k=87\n",
    "print(\"label:\",train_df.iloc[k,0])\n",
    "img = np.array([list(train_df.iloc[k,x[0]:x[1]]) for x in bounds_list])\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap this in a function so we can see the label and drawn image of any of the k training images\n",
    "def view_train_image(k):\n",
    "    print(\"label:\",train_df.iloc[k,0])\n",
    "    img = np.array([list(train_df.iloc[k,x[0]:x[1]]) for x in bounds_list])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Initialise NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create all the objects that describe the parameters of the network\n",
    "# may be worth wrapping in a class? we will see\n",
    "# also, I'd like to use numpy arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Random Number Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3265827391056191"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random state in np.random\n",
    "rng = np.random.default_rng(seed)\n",
    "# now this function randomly fenerates nums between 0 and 1 according to the seed I have set (for reproduceability)\n",
    "rng.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2497092 , 0.15885385]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can generate arrays with nxm dimensions\n",
    "rng.random(size=([1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.204648370637754"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need a function that randomly generates initial values for the parameters\n",
    "4*rng.random() - 2\n",
    "#this generates numbers between -2 and 2, I will start with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4645553837137615"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wrap in my own function\n",
    "def wrand(size=None):\n",
    "    return 4*rng.random(size=size) - 2\n",
    "\n",
    "wrand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.40833147, -0.57100389]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrand([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Initialise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of neurons in each layer\n",
    "L = {0:784, 1:128, 2:64, 3:10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1.6971597505793126, 2: 1.362263912476521, 3: -0.39469122467489415}\n"
     ]
    }
   ],
   "source": [
    "#bias vals (randomly initialised)\n",
    "b = {1:wrand(), 2:wrand(), 3:wrand()}\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight matrices (randomly initialised)\n",
    "W = {1:wrand(size=[L[0],L[1]]),\n",
    "     2:wrand(size=[L[1],L[2]]),\n",
    "     3:wrand(size=[L[2],L[3]])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all our parameters are initialised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Try a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try running our first image through this randomly generated neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[1].shape\n",
    "# start with matrix multiplication of W[1] with the 784x1 vector that is the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "# get the 784x1 vector of the kth image\n",
    "k=0\n",
    "vec = np.array(train_df.iloc[k,1:]).flatten()\n",
    "print(vec.size)\n",
    "print(vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the 0th image... a bunch of pixel values between 0 and 255\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2249.46980599,  5103.70621148,  3158.96796707, -1205.59407543,\n",
       "       -8106.66051858, -2261.95985173, -1877.25618508, -4326.74083757,\n",
       "         481.95995709, -1235.17194763,   673.94820086,   302.69982739,\n",
       "       -1927.02633018, -3850.59918319,  3803.67592698,  -541.69138849,\n",
       "        3160.5810435 ,  2159.98464147, -1319.2569832 ,  -117.69165678,\n",
       "         876.29094464, -3974.6452943 ,  -618.44068167,  2177.1786375 ,\n",
       "        4444.75923709,   276.93998349,   477.08630129,  2120.32477124,\n",
       "       -1272.45882615,  2574.39433638, -4557.39075206,  3379.23200056,\n",
       "       -2785.86367381,  1797.50227155,   929.53280674,  1523.63355248,\n",
       "         945.2956367 , -2086.08345714, -3731.0803025 ,  -637.21567858,\n",
       "       -5255.93055397, -2190.20473144, -3481.92330414,  5322.38582612,\n",
       "       -2615.35303974,  4171.80589354,    38.89902141,  2359.71965605,\n",
       "       -5019.89719651,  1705.84211536, -3581.61967924, -1679.52457618,\n",
       "        2577.4804081 ,  2304.85044601,  3831.86063674, -2277.33959771,\n",
       "         -11.64021386,  -197.17271966, -5500.66100129, -3452.13065088,\n",
       "       -3443.10379417,  2539.00828532,  1558.83443111, -1464.01796758,\n",
       "        4708.65136955, -5622.36993697, -1212.74153154,  1425.03633037,\n",
       "        -419.90409425,  2397.37654943,   -31.47177265,   939.39222589,\n",
       "       -2098.19777274,  3157.62604709,   132.22014238, -2912.15021216,\n",
       "        4184.93109423,  -748.10001278,  2327.82657656,   520.86551257,\n",
       "        3339.64780931,  3770.13619271, -2733.62115058, -2081.98936748,\n",
       "       -1514.43594309, -2289.80703835,  -608.38903497,  5386.8661029 ,\n",
       "       -7589.11496879, -1263.07247642, -1226.12499755, -1374.5562167 ,\n",
       "       -2916.04095509,  2021.08379818,  1281.49263063, -1259.36320434,\n",
       "       -3409.56491471,  1583.78795113, -2495.23942806,   494.75147946,\n",
       "       -5845.19197838,  -666.06040195,  1975.83462544,  1196.74060469,\n",
       "       -3265.98860566,  1687.44357929,  -378.758705  , -3977.45756896,\n",
       "        2948.90224599,  3095.43262501, -4143.97876138, -5357.67740925,\n",
       "       -6277.92435696,   565.93483508, -2907.67502011,  3214.02600592,\n",
       "        -314.55777113,  -331.98301026, -2280.24174102, -2100.27490785,\n",
       "        5069.72256535,  -826.51533566,  5261.82603031, -1232.47608147,\n",
       "        -727.89541225, -1739.09468889, -1067.08190557, -1015.78714544])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do the matrix multiplication\n",
    "vec @ W[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2247.77264624,  5105.40337123,  3160.66512683, -1203.89691568,\n",
       "       -8104.96335883, -2260.26269198, -1875.55902533, -4325.04367782,\n",
       "         483.65711684, -1233.47478787,   675.64536061,   304.39698714,\n",
       "       -1925.32917043, -3848.90202344,  3805.37308673,  -539.99422874,\n",
       "        3162.27820325,  2161.68180122, -1317.55982345,  -115.99449703,\n",
       "         877.98810439, -3972.94813455,  -616.74352192,  2178.87579725,\n",
       "        4446.45639684,   278.63714324,   478.78346104,  2122.021931  ,\n",
       "       -1270.7616664 ,  2576.09149613, -4555.69359231,  3380.92916031,\n",
       "       -2784.16651406,  1799.1994313 ,   931.22996649,  1525.33071223,\n",
       "         946.99279645, -2084.38629739, -3729.38314275,  -635.51851883,\n",
       "       -5254.23339422, -2188.50757168, -3480.22614439,  5324.08298587,\n",
       "       -2613.65587999,  4173.50305329,    40.59618116,  2361.41681581,\n",
       "       -5018.20003676,  1707.53927512, -3579.92251949, -1677.82741643,\n",
       "        2579.17756785,  2306.54760576,  3833.55779649, -2275.64243796,\n",
       "          -9.94305411,  -195.47555991, -5498.96384154, -3450.43349112,\n",
       "       -3441.40663442,  2540.70544507,  1560.53159086, -1462.32080783,\n",
       "        4710.3485293 , -5620.67277722, -1211.04437179,  1426.73349013,\n",
       "        -418.2069345 ,  2399.07370918,   -29.7746129 ,   941.08938564,\n",
       "       -2096.50061298,  3159.32320684,   133.91730213, -2910.45305241,\n",
       "        4186.62825398,  -746.40285303,  2329.52373631,   522.56267232,\n",
       "        3341.34496906,  3771.83335246, -2731.92399083, -2080.29220773,\n",
       "       -1512.73878334, -2288.1098786 ,  -606.69187522,  5388.56326265,\n",
       "       -7587.41780904, -1261.37531666, -1224.4278378 , -1372.85905695,\n",
       "       -2914.34379534,  2022.78095793,  1283.18979038, -1257.66604459,\n",
       "       -3407.86775496,  1585.48511088, -2493.54226831,   496.44863921,\n",
       "       -5843.49481863,  -664.3632422 ,  1977.53178519,  1198.43776444,\n",
       "       -3264.29144591,  1689.14073904,  -377.06154525, -3975.76040921,\n",
       "        2950.59940574,  3097.12978476, -4142.28160163, -5355.9802495 ,\n",
       "       -6276.22719721,   567.63199483, -2905.97786036,  3215.72316567,\n",
       "        -312.86061138,  -330.28585051, -2278.54458127, -2098.5777481 ,\n",
       "        5071.4197251 ,  -824.81817591,  5263.52319006, -1230.77892171,\n",
       "        -726.1982525 , -1737.39752914, -1065.38474582, -1014.08998569])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to build on this, we want to add that layer's bias and then apply ReLU elementwise to the whole thing... so do that\n",
    "(b[1] + (vec @ W[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        , 5105.40337123, 3160.66512683,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "        483.65711684,    0.        ,  675.64536061,  304.39698714,\n",
       "          0.        ,    0.        , 3805.37308673,    0.        ,\n",
       "       3162.27820325, 2161.68180122,    0.        ,    0.        ,\n",
       "        877.98810439,    0.        ,    0.        , 2178.87579725,\n",
       "       4446.45639684,  278.63714324,  478.78346104, 2122.021931  ,\n",
       "          0.        , 2576.09149613,    0.        , 3380.92916031,\n",
       "          0.        , 1799.1994313 ,  931.22996649, 1525.33071223,\n",
       "        946.99279645,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        , 5324.08298587,\n",
       "          0.        , 4173.50305329,   40.59618116, 2361.41681581,\n",
       "          0.        , 1707.53927512,    0.        ,    0.        ,\n",
       "       2579.17756785, 2306.54760576, 3833.55779649,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        , 2540.70544507, 1560.53159086,    0.        ,\n",
       "       4710.3485293 ,    0.        ,    0.        , 1426.73349013,\n",
       "          0.        , 2399.07370918,    0.        ,  941.08938564,\n",
       "          0.        , 3159.32320684,  133.91730213,    0.        ,\n",
       "       4186.62825398,    0.        , 2329.52373631,  522.56267232,\n",
       "       3341.34496906, 3771.83335246,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        , 5388.56326265,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        , 2022.78095793, 1283.18979038,    0.        ,\n",
       "          0.        , 1585.48511088,    0.        ,  496.44863921,\n",
       "          0.        ,    0.        , 1977.53178519, 1198.43776444,\n",
       "          0.        , 1689.14073904,    0.        ,    0.        ,\n",
       "       2950.59940574, 3097.12978476,    0.        ,    0.        ,\n",
       "          0.        ,  567.63199483,    0.        , 3215.72316567,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "       5071.4197251 ,    0.        , 5263.52319006,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try np.vectorize for applying max([0,x]) ?\n",
    "def ReLU(x):\n",
    "    return max([0,x])\n",
    "\n",
    "# vectorise it, and ensure output type is float so any decimals are retained\n",
    "f = np.vectorize(ReLU,otypes=[float])\n",
    "\n",
    "# okay, that worked VERY WELL\n",
    "f(b[1] + (vec @ W[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22359.70679153,      0.        ,      0.        ,  65092.81758966,\n",
       "        78201.32236168, 278243.59138348,      0.        ,      0.        ,\n",
       "            0.        ,  23978.45736383])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now carry on this pattern to get the final neuron values in the 3rd layer!\n",
    "# for each layer, we add the bias to the matrix multiplication of the last layer's neurons, then apply ReLU and REPEAT\n",
    "f(b[3] + ( f(b[2] + ( f(b[1] + ( vec @ W[1] )) @ W[2])) @ W[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the math.exp function so we can calculate the softmax probabilities using e^\n",
    "allexp = np.vectorize(math.exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell errors, so commenting out\n",
    "\n",
    "# now calculate the softmax probabilities\n",
    "# allexp = np.vectorize(math.exp)\n",
    "# p3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( vec @ W[1] )) @ W[2])) @ W[3]))\n",
    "# allexp(p3)\n",
    "# oh no, some numbers must be too big\n",
    "# to try and stop this, I may divide all the initial pixel values by 255 to limit their vals to [0,1] range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell errors, so commenting out\n",
    "\n",
    "# divide all the initial pixel values by 255 to limit their vals to [0,1] range\n",
    "# k=0\n",
    "# smallvec = np.array(train_df.iloc[k,1:]).flatten() / 255\n",
    "\n",
    "# smallp3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( smallvec @ W[1] )) @ W[2])) @ W[3]))\n",
    "# print(smallp3)\n",
    "# allexp(smallp3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell errors, so commenting out\n",
    "\n",
    "# I am thinking of rounding after the decimal place before taking exp, since I only need 3dp at most (probably?)\n",
    "# allround = np.vectorize(lambda x: round(x,3))\n",
    "# roundsmallp3 = allround(f(b[3] + ( f(b[2] + ( f(b[1] + ( smallvec @ W[1] )) @ W[2])) @ W[3])))\n",
    "# print(roundsmallp3)\n",
    "# allexp(roundsmallp3)\n",
    "#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.218407461554972e+307\n"
     ]
    }
   ],
   "source": [
    "print(math.exp(709))\n",
    "# print(math.exp(710))  #tried it and 710 seems to be where errors start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Xavier Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041527442646279326"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I read about initialising parameter values using Xavier distribution:\n",
    "# initialise biases as =0\n",
    "# initialise weights in layer L via normal dist with mean 0 and variance = 1/(num_neurons_in_layer_(L-1))\n",
    "rng.normal(loc=0,scale=(1/L[0])**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00423504,  0.0115572 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can you use normal to return a random ARRAY? YES\n",
    "rng.normal(loc=0,scale=(1/L[0])**0.5,size=[1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of neurons in each layer\n",
    "L = {0:784, 1:128, 2:64, 3:10}\n",
    "#bias vals initialised at 0\n",
    "b = {1:0, 2:0, 3:0}\n",
    "# weight matrices (randomly initialised using xavier)\n",
    "W = {1:rng.normal(loc=0,scale=(1/L[0])**0.5,size=[L[0],L[1]]),\n",
    "     2:rng.normal(loc=0,scale=(1/L[1])**0.5,size=[L[1],L[2]]),\n",
    "     3:rng.normal(loc=0,scale=(1/L[2])**0.5,size=[L[2],L[3]])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Prediction with Xavier Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.02928147 0.         0.19179514 0.09803207 0.08261278\n",
      " 0.         0.         0.20108049 0.01316989]\n"
     ]
    }
   ],
   "source": [
    "# divide all the initial pixel values by 255 to limit their vals to [0,1] range\n",
    "k=0\n",
    "smallvec = np.array(train_df.iloc[k,1:]).flatten() / 255\n",
    "\n",
    "smallp3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( smallvec @ W[1] )) @ W[2])) @ W[3]))\n",
    "print(smallp3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09375378, 0.09653962, 0.09375378, 0.11357543, 0.10341025,\n",
       "       0.10182797, 0.09375378, 0.09375378, 0.11463492, 0.09499668])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now calculate the softmax probabilities\n",
    "expvec = allexp(smallp3)\n",
    "phat = expvec/sum(expvec)\n",
    "phat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the prediction made by my network (the index of phat containing the largest value)\n",
    "pred_label = [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now look at the image and the true label... was my network correct (I imagine not!)\n",
    "view_train_image(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also from now on I will use the training image pixel values mapped to the interval [0,1] via this array:\n",
    "train_arr_01 = np.array(train_df.iloc[:,1:]) / 255\n",
    "train_arr_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my network prediction: 0\n",
      "label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# so, to wrap this section up... here is my code to make a prediction on the kth image (not a good prediction, at that!!)\n",
    "k=9999\n",
    "vec = train_arr_01[k]\n",
    "p3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( vec @ W[1] )) @ W[2])) @ W[3]))\n",
    "expvec = allexp(p3)\n",
    "phat = expvec/sum(expvec)\n",
    "pred_label = [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "print(\"my network prediction:\",pred_label)\n",
    "view_train_image(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11105736, 0.09716992, 0.09556072, 0.1014054 , 0.10258829,\n",
       "       0.10143117, 0.09372359, 0.09372359, 0.10961636, 0.09372359])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can put my prediction code in one function\n",
    "def generate_phat_k(k):\n",
    "    vec = train_arr_01[k]\n",
    "    p3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( vec @ W[1] )) @ W[2])) @ W[3]))\n",
    "    expvec = allexp(p3)\n",
    "    phat = expvec/sum(expvec)\n",
    "    # pred_label = [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "    return phat\n",
    "\n",
    "generate_phat_k(13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can put my prediction code in one function\n",
    "def predict_k(k):\n",
    "    vec = train_arr_01[k]\n",
    "    p3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( vec @ W[1] )) @ W[2])) @ W[3]))\n",
    "    expvec = allexp(p3)\n",
    "    phat = expvec/sum(expvec)\n",
    "    return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "\n",
    "predict_k(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the next section, we build out the code needed to train the network!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Code Needed for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before diving into training, I will play with the code I think we need to do said training of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cost (Cross Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using Cross Entropy to quantify the \"cost\"/\"loss\" of my network (i.e. how wrong it is)\n",
    "# calculate the cross entropy loss for the network over ALL training examples\n",
    "# for the kth training example, add log(phat_j) to the cost (where j is the true label of the kth example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, start by creating a vector t of all the true labels (where element i of this vector will be the true label of example i)\n",
    "t = np.array(train_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each training example k, find the vector phat_k and then use t to tell us which element to use\n",
    "log_phats = [math.log(generate_phat_k(idx)[t_val]) for idx,t_val in enumerate(t)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss = 2.310810136532411\n"
     ]
    }
   ],
   "source": [
    "# now calculate cross entropy loss by summing these log_phats and multiplying by -1/K (where K is the number of training examples)\n",
    "K = len(t)\n",
    "cost = (-1/K)*sum([math.log(generate_phat_k(idx)[t_val]) for idx,t_val in enumerate(t)])\n",
    "print(\"Cross Entropy Loss =\",cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = [predict_k(k) for k,t_val in enumerate(t)]i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it quicker than 13s vectorized? Yes, a bit\n",
    "vec_predict = np.vectorize(predict_k)\n",
    "pred_labels1 = vec_predict(np.array(range(len(t))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07725\n",
      "total accuracy = 7.72 %\n"
     ]
    }
   ],
   "source": [
    "# TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(t)]\n",
    "total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "print(total_accuracy)\n",
    "print(\"total accuracy =\", round(100*total_accuracy,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.21492486915414485,\n",
       " 1: 0.026105013349154552,\n",
       " 2: 0.01544142329640819,\n",
       " 3: 0.43793834610993315,\n",
       " 4: 0.09619993153029785,\n",
       " 5: 0.055524810920494376,\n",
       " 6: 0.0023656640757012504,\n",
       " 7: 0.002553870710295291,\n",
       " 8: 0.1298923260981029,\n",
       " 9: 0.01781812069255337}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "digit_accuracy = {}\n",
    "for digit in range(10):\n",
    "    train_cases_containing_digit = [k for k,true_val in enumerate(t) if true_val==digit]\n",
    "    # now calculate how many of these cases we predicted correctly\n",
    "    checklist = [pred_labels1[k]==digit for k,true_val in enumerate(train_cases_containing_digit)]\n",
    "    digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "\n",
    "digit_accuracy\n",
    "#can see we currently get a lot of 0 cases correct, but this is because 0 is chosen if we encounter a tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.10035814388041109,\n",
       " 1: 0.11507009345794393,\n",
       " 2: 0.10186335403726708,\n",
       " 3: 0.10356740809751175,\n",
       " 4: 0.10379746835443038,\n",
       " 5: 0.08957415565345081,\n",
       " 6: 0.10434782608695652,\n",
       " 7: 0.1,\n",
       " 8: 0.0940576011666059,\n",
       " 9: 0.09962756052141528}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "\n",
    "digit_recall = {}\n",
    "for digit in range(10):\n",
    "    cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "    # now calculate how many of these cases actually contained this digit correctly\n",
    "    checklist = [t[k]==digit for k,true_val in enumerate(cases_we_predicted_digit)]\n",
    "    digit_recall[digit] = sum(checklist)/len(checklist)\n",
    "\n",
    "digit_recall\n",
    "# weirdly 6 has 0 recall, perhaps we never predicted 6? other than that we see a pretty even spread here... 1 is doing weirdly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 12844,\n",
       " 1: 1712,\n",
       " 2: 805,\n",
       " 3: 25761,\n",
       " 4: 5925,\n",
       " 5: 3405,\n",
       " 6: 115,\n",
       " 7: 130,\n",
       " 8: 8229,\n",
       " 9: 1074}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking how much we predicted each digit\n",
    "pred_counts = {digit:len([k for k,pred in enumerate(pred_labels1) if pred==digit]) for digit in range(10)}\n",
    "pred_counts\n",
    "# all are equally low except of course 0 but also 3 stands out as surprisingly high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pre-reqs to Gradient Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my derivation for the layer 3 bias gradient has 2 distinct terms in it that we need to calculate for each training case k:\n",
    "# 1) phat (the vector of softmax probabilities)\n",
    "# 2) layer 3 weight sums i.e. the value at each neuron in layer 3 BEFORE applying activation function ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) phat (the vector of softmax probabilities)\n",
    "phat_all_k = np.array([generate_phat_k(k) for k in range(len(t))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16143577, -0.17386073,  0.06627675, -0.0677996 ,  0.10667387,\n",
       "        0.02265627, -0.03565025, -0.09097051,  0.05963224, -0.07038977])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) layer 3 weight sums i.e. the value at each neuron in layer 3 BEFORE applying activation function ReLU\n",
    "k=8  #doing this calculation for one training case here\n",
    "weightsums_3 = b[3] + ( f(b[2] + ( f(b[1] + ( train_arr_01[k] @ W[1] )) @ W[2])) @ W[3])\n",
    "weightsums_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea: 1 pass through network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just having a thought while I code this... since we eventually are going to need all the weight sums from each layer AND THEN the phat...\n",
    "#...vectors, we should try to minimise how much we make python recalculate the same thing over and over again. So, can I...\n",
    "#...take the weight sums etc that I need during one single pass throught the network, going step by step until final step of phat?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need every single weight sum for every single k, so create a data structure that allows us to store these conveniently\n",
    "\n",
    "#defining this dictionary is making me think about having a 3d numpy array where the dimensions are:\n",
    "# layer l, training case K, and the neurons (in layer l there are L[l] many)\n",
    "wsums = {l:{} for l in L.keys()}\n",
    "\n",
    "wsums[1] = {k:b[1] + (train_arr_01[k] @ W[1]) for k in range(len(t))}\n",
    "# now we need to define the second layer weight sums using wsums[1]\n",
    "wsums[2] = {k:b[2] + f(wsums[1][k]) @ W[2] for k in range(len(t))}\n",
    "# and define the 3rd layer weight sums using the second weight sums we just calculated\n",
    "wsums[3] = {k:b[3] + f(wsums[2][k]) @ W[3] for k in range(len(t))}\n",
    "#this way, we get all the weight sums we need and only have to do each matrix multiplication once! work samrter not harder?\n",
    "\n",
    "# original formula here for reference\n",
    "# f(b[3] + ( f(b[2] + ( f(b[1] + ( train_arr_01[k] @ W[1] )) @ W[2])) @ W[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arr_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of phat for the 0th training case: [0.09375378 0.09653962 0.09375378 0.11357543 0.10341025 0.10182797\n",
      " 0.09375378 0.09375378 0.11463492 0.09499668]\n"
     ]
    }
   ],
   "source": [
    "# now we need to arrive at phat for each training case k from the layer 3 weightsums in wsums[3]\n",
    "expvec_all_k = [allexp(f(wsums[3][k])) for k in range(len(t))]\n",
    "phat_all_k = [expvec_all_k[k]/sum(expvec_all_k[k]) for k in range(len(t))]\n",
    "print(\"example of phat for the 0th training case:\",phat_all_k[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have all this, can easily calculate the gradients with respect to each parameter (see following sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Gradient Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have derived formulas for the gradients needed separately, see attached material hopefully (upload a PDF future Will?)\n",
    "# Here I will simply implement those calculations and hope they're correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Bias Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 0, 3: 0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a copy of the bias parameters with a g on the front to signify gradients\n",
    "g_b = b.copy()\n",
    "g_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16143577 -0.17386073  0.06627675 -0.0677996   0.10667387  0.02265627\n",
      " -0.03565025 -0.09097051  0.05963224 -0.07038977]\n",
      "-0.1738607257478075\n",
      "indicator: False\n",
      "\n",
      "[0.11255973 0.09577946 0.1023425  0.09577946 0.10656149 0.09797424\n",
      " 0.09577946 0.09577946 0.10166474 0.09577946]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11255973294529902,\n",
       " 0.10234249934473547,\n",
       " 0.10656148572176075,\n",
       " 0.09797423573139616,\n",
       " 0.10166473778993206]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g_b[3] = \n",
    "\n",
    "# for training case k, let m be the true label. We need the mth weight sum at layer 3\n",
    "k=8\n",
    "print(wsums[3][k])\n",
    "print(wsums[3][k][t[k]])\n",
    "\n",
    "# we calculate an indicator that equals 1 if this value is >0, and 0 otherwise\n",
    "print(\"indicator:\",wsums[3][k][t[k]] > 0)\n",
    "\n",
    "# now we want to sum the following: for every single digit z from 0 to 9 in layer 3, find this indicator multiplied by the zth element of phat\n",
    "#I am going to be cheeky and instead only return the non-zero elements of this sum (so I use an \"if\" to do this)\n",
    "print()\n",
    "print(phat_all_k[k])\n",
    "[phat_all_k[k][z] for z in range(L[3]) if wsums[3][k][z] > 0]\n",
    "#notice that we only retained the elements of phat that had a positive weight sum - noice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of the cost function wrt the 3rd bias = 0.056731498034807606\n"
     ]
    }
   ],
   "source": [
    "# with all the above pieces in play, we calculate the gradient of the cost function wrt the 3rd bias like so:\n",
    "g_b[3] = -sum([(wsums[3][k][t[k]] > 0) - sum([phat_all_k[k][z] for z in range(L[3]) if wsums[3][k][z] > 0]) for k in range(len(t))]) / K\n",
    "print(\"gradient of the cost function wrt the 3rd bias =\",g_b[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_to_sum: []\n",
      "notice the above list is empty because -0.1738607257478075 is not positive\n",
      "weightsum: 0\n",
      "values_to_sum_over_z: [0.09885440322507436, 0.0, 0.030838473043653288, 0.0, 0.02779059218521728, 0.031198453341450817, 0.0, 0.0, 0.06624207669169925, 0.0]\n",
      "same thing? except 0s, which contribute nothing to sums [0.09885440322507436, 0.030838473043653288, 0.02779059218521728, 0.031198453341450817, 0.06624207669169925]\n"
     ]
    }
   ],
   "source": [
    "# now things get hairier with the 2nd bias so let me build up my calculation in this cell\n",
    "k=8\n",
    "#first, sum all the weights w_i_m in W[3] for which the ith weightsum in layer 2 is >0 AND the mth weightsum in...\n",
    "#  layer 3 is >0 AND m is the correct label for this training case.\n",
    "# I have realised we can bring the if outside the loop, so I have done that below too\n",
    "weights_to_sum = [W[3][i][t[k]] for i in range(L[2]) if wsums[2][k][i] > 0] if wsums[3][k][t[k]] > 0 else []\n",
    "print(\"weights_to_sum:\",weights_to_sum)\n",
    "print(f\"notice the above list is empty because {wsums[3][k][t[k]]} is not positive\")\n",
    "weightsum = (sum([W[3][i][t[k]] for i in range(L[2]) if wsums[2][k][i] > 0]) if wsums[3][k][t[k]] > 0 else 0)\n",
    "print(\"weightsum:\",weightsum)\n",
    "\n",
    "# then the second thing to calculate is a sum over all digits z between 0 and 9 which involves phat (similar to b[3] gradient calculation!)\n",
    "#the inner sum is the same as the sum above but with the condition on m equalling the correct label relaxed, so all labels z contribute!\n",
    "values_to_sum_over_z = [phat_all_k[k][z]*sum([W[3][i][z] for i in range(L[2]) if wsums[2][k][i] > 0 and wsums[3][k][z] > 0]) for z in range(L[3])]\n",
    "print(\"values_to_sum_over_z:\",values_to_sum_over_z)\n",
    "print(\"same thing? except 0s, which contribute nothing to sums\",[phat_all_k[k][z]*sum([W[3][i][z] for i in range(L[2]) if wsums[2][k][i] > 0]) for z in range(L[3]) if wsums[3][k][z] > 0])\n",
    "# it is more efficient to pull the \"if wsums[3][k][z] > 0\" out of the innermost list comprehension, so I use this approach below!\n",
    "#now combine these elements in the next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of the cost function wrt the 2nd bias = 0.05089663755853589\n"
     ]
    }
   ],
   "source": [
    "g_b[2] = -sum([(sum([W[3][i][t[k]] for i in range(L[2]) if wsums[2][k][i] > 0]) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "- sum([phat_all_k[k][z]*sum([W[3][i][z] for i in range(L[2]) if wsums[2][k][i] > 0]) for z in range(L[3]) if wsums[3][k][z] > 0]) for k in range(len(t))]) / K\n",
    "print(\"gradient of the cost function wrt the 2nd bias =\",g_b[2])\n",
    "# took 4.4s to work this one out, I dread to think how long the deeper calculations will take!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_to_sum: []\n",
      "once again the above list is empty because -0.1738607257478075 is not positive\n",
      "actualsum: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03725902839667246,\n",
       " 0.005408002096197179,\n",
       " 0.06130519629868697,\n",
       " -0.04060157525747576,\n",
       " 0.033798659096650994]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start calculating 1st layer bias gradient for example case k=8\n",
    "k=8\n",
    "#similar to the last bias but we squeeze another sum within this term, over j in range(L[1]) i.e. over all 128 neurons in layer 1.\n",
    "#the elements of this sum are products of weight (i,m) in layer 3 and weight (j,i) in layer 2 (where m is the true label for this training case)\n",
    "weightproducts_to_sum = [W[3][i][t[k]]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]) for i in range(L[2]) if \n",
    "                          wsums[2][k][i] > 0] if wsums[3][k][t[k]] > 0 else []\n",
    "print(\"weights_to_sum:\",weights_to_sum)\n",
    "print(f\"once again the above list is empty because {wsums[3][k][t[k]]} is not positive\")\n",
    "actualsum = (sum([W[3][i][t[k]]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]) for i in range(L[2]) if \n",
    "                          wsums[2][k][i] > 0]) if wsums[3][k][t[k]] > 0 else 0)\n",
    "print(\"actualsum:\",actualsum)\n",
    "\n",
    "#now similar to the second sum we've had to do in each other bias gradient, we have a product of phat and some weights summed over every digit z\n",
    "# I am realising the ifs should not be part of one single if statement, since it may be more efficient to pull each if out as far as they can go\n",
    "values_to_sum_over_z = [phat_all_k[k][z]*sum([W[3][i][z]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]) for i in range(L[2]) if wsums[2][k][i] > 0]) for z in range(L[3]) if wsums[3][k][z] > 0]\n",
    "values_to_sum_over_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of the cost function wrt the 1st bias = 0.004083139634485991\n"
     ]
    }
   ],
   "source": [
    "g_b[1] = -sum([(sum([W[3][i][t[k]]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]) for i in range(L[2]) if wsums[2][k][i] > 0]) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "- sum([phat_all_k[k][z]*sum([W[3][i][z]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]) for i in range(L[2]) if wsums[2][k][i] > 0]) for z in range(L[3]) if wsums[3][k][z] > 0]) for k in range(len(t))]) / K\n",
    "print(\"gradient of the cost function wrt the 1st bias =\",g_b[1])\n",
    "# took 4m7s, do-able\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we speed this calculation up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_b[1] = -sum(\n",
    "[\n",
    "    (\n",
    "        sum(\n",
    "            [W[3][i][t[k]]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0])\n",
    "              for i in range(L[2]) if wsums[2][k][i] > 0]) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "- sum([phat_all_k[k][z]*sum([W[3][i][z]*sum([W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]) for i in range(L[2]) if wsums[2][k][i] > 0]) for z in range(L[3]) if wsums[3][k][z] > 0]) for k in range(len(t))]) / K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6243467772871342"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# focus on cracking this inner part\n",
    "k=9\n",
    "sum(\n",
    "    [\n",
    "        W[3][i][t[k]] * sum(\n",
    "            [W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]\n",
    "                )\n",
    "    for i in range(L[2]) if wsums[2][k][i] > 0]\n",
    "              ) if wsums[3][k][t[k]] > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  3,  4,  5,  7,  8, 20, 21, 23, 28, 29, 31, 32, 33, 34, 37, 42,\n",
      "       43, 44, 45, 46, 47, 52, 54, 56, 57, 58, 61, 62, 63]),)\n",
      "(array([  2,   4,   6,   7,   8,  11,  17,  19,  20,  21,  24,  25,  26,\n",
      "        28,  29,  35,  37,  39,  41,  43,  45,  48,  49,  50,  52,  60,\n",
      "        61,  63,  64,  67,  68,  69,  70,  71,  75,  76,  77,  78,  79,\n",
      "        81,  82,  83,  85,  87,  92,  95,  98,  99, 103, 105, 110, 115,\n",
      "       118, 119, 120, 122, 127]),)\n"
     ]
    }
   ],
   "source": [
    "k=9\n",
    "#remove the need for i in range(L[2]) if wsums[2][k][i] > 0 by using this condition as an indexer for the i index\n",
    "i_index = np.nonzero(wsums[2][k][:] > 0)\n",
    "print(i_index)\n",
    "\n",
    "# do the same for j index: for j in range(L[1]) if wsums[1][k][j] > 0\n",
    "j_index = np.nonzero(wsums[1][k][:] > 0)\n",
    "print(j_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01320224,  0.06662953,  0.05664409,  0.04361186,  0.27119871,\n",
       "        -0.04032074, -0.00497303,  0.0414925 ,  0.13860661,  0.18963525,\n",
       "        -0.04283642,  0.0381924 ,  0.08085448,  0.01796495, -0.00211413,\n",
       "         0.20442345, -0.06653222,  0.05380364, -0.01719256, -0.07097498,\n",
       "        -0.05909686, -0.13569812,  0.00208287,  0.01806047, -0.2048839 ,\n",
       "         0.22983924, -0.07905968, -0.01978598, -0.01899574, -0.09383732]])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(W[3][i_index,t[k]].shape)\n",
    "W[3][i_index,t[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0993001 ,  0.08975795, -0.05275159, ...,  0.01211506,\n",
       "          0.1806115 ,  0.0071188 ]],\n",
       "\n",
       "       [[-0.17934181,  0.05593696, -0.01330883, ...,  0.15451894,\n",
       "         -0.06750442, -0.01139237]],\n",
       "\n",
       "       [[ 0.02449313, -0.03710994,  0.07248845, ..., -0.0011383 ,\n",
       "          0.00813584,  0.12102853]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.02199665,  0.03454335,  0.03133997, ..., -0.08159778,\n",
       "          0.04234261, -0.16050193]],\n",
       "\n",
       "       [[ 0.11416512,  0.10091481, -0.08363767, ...,  0.24280997,\n",
       "         -0.00391684, -0.09063259]],\n",
       "\n",
       "       [[ 0.13360259,  0.08682506,  0.09803333, ..., -0.05085488,\n",
       "         -0.00553527, -0.06382721]]])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(W[2][j_index][:,i_index].shape)\n",
    "W[2][j_index][:,i_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.32809016,  0.62214971, -0.04945722,  0.47466477,  1.16584501,\n",
       "         0.01494799,  0.14210468,  0.112388  ,  0.36377631,  0.09531187,\n",
       "        -0.28786522,  0.08700248,  1.79403064,  0.19141931,  0.72798707,\n",
       "         0.29719565,  0.07563587, -0.50234687,  1.2375092 ,  0.08551817,\n",
       "         1.06929207, -0.9066226 ,  0.17910953,  0.20931509,  0.56108266,\n",
       "         0.1266011 , -0.99483566,  0.62884737,  0.92327345,  0.10079154]])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sum we multiply by is supposed to be summed over j, so try np.sum over j\n",
    "print(np.sum(W[2][j_index][:,i_index],initial=0,axis=0).shape)\n",
    "np.sum(W[2][j_index][:,i_index],initial=0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00433153,  0.04145354, -0.00280146,  0.02070101,  0.31617566,\n",
       "        -0.00060271, -0.00070669,  0.00466326,  0.0504218 ,  0.01807449,\n",
       "         0.01233111,  0.00332283,  0.14505541,  0.00343884, -0.00153906,\n",
       "         0.06075376, -0.00503222, -0.02702809, -0.02127596, -0.00606965,\n",
       "        -0.0631918 ,  0.12302698,  0.00037306,  0.00378033, -0.1149568 ,\n",
       "         0.0290979 ,  0.07865139, -0.01244236, -0.01753826, -0.00945801]])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now try multiplying together the (1,30) shape arrays we have arrived at\n",
    "np.multiply(W[3][i_index,t[k]] , np.sum(W[2][j_index][:,i_index],initial=0,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so could we reduce it down to something like this? but we need to ensure the indices align\n",
    "k = 8\n",
    "np.sum(\n",
    "        np.multiply(W[3][i_index,t[k]] , np.sum(W[2][j_index][:,i_index],initial=0,axis=0))\n",
    "              ) if wsums[3][k][t[k]] > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.000001\n",
    "for k in range(1000):\n",
    "\n",
    "    oldway = sum(\n",
    "        [\n",
    "            W[3][i][t[k]] * sum(\n",
    "                [W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]\n",
    "                    )\n",
    "        for i in range(L[2]) if wsums[2][k][i] > 0]\n",
    "                ) if wsums[3][k][t[k]] > 0 else 0\n",
    "\n",
    "    newway = np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),t[k]] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][t[k]] > 0 else 0\n",
    "    if oldway - newway > eps:\n",
    "        print(k)\n",
    "# yay the new way is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use this new way for all z terms now too (not just when z is t[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the z-dependent-side of the original code I had for bias layer 1 gradient calculation:\n",
    "k=9\n",
    "oldz = sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*sum(\n",
    "            [\n",
    "                W[3][i][z]*sum(\n",
    "                    [\n",
    "                        W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]\n",
    "                        ) for i in range(L[2]) if wsums[2][k][i] > 0]\n",
    "                        ) for z in range(L[3]) if wsums[3][k][z] > 0])  #for k in range(len(t))]) / K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "newz = sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*(np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),z] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][z] > 0 else 0) for z in range(L[3]) if wsums[3][k][z] > 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.000001\n",
    "for k in range(1000):\n",
    "\n",
    "    oldz = sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*sum(\n",
    "            [\n",
    "                W[3][i][z]*sum(\n",
    "                    [\n",
    "                        W[2][j][i] for j in range(L[1]) if wsums[1][k][j] > 0]\n",
    "                        ) for i in range(L[2]) if wsums[2][k][i] > 0]\n",
    "                        ) for z in range(L[3]) if wsums[3][k][z] > 0])  #for k in range(len(t))]) / K\n",
    "\n",
    "\n",
    "\n",
    "    newz = sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*(np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),z] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][z] > 0 else 0) for z in range(L[3]) if wsums[3][k][z] > 0]) \n",
    "    if oldz - newz > eps:\n",
    "        print(k)\n",
    "# yay the new way is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of the cost function wrt the 1st bias = 0.004083139634485991\n",
      "fast calculation got this value: 0.004083139634485993\n",
      "CPU times: user 6.64 s, sys: 69.9 ms, total: 6.71 s\n",
      "Wall time: 6.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g_b1_fast = -sum(\n",
    "    [\n",
    "        (\n",
    "            np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),t[k]] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "\n",
    "- sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*(np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),z] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][z] > 0 else 0) for z in range(L[3]) if wsums[3][k][z] > 0]\n",
    "                ) for k in range(len(t))]) / K\n",
    "\n",
    "print(\"gradient of the cost function wrt the 1st bias =\",g_b[1])\n",
    "print(\"fast calculation got this value:\",g_b1_fast)\n",
    "#woooooo down from 4 minutes to 6 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of the cost function wrt the 1st bias = 0.004083139634485993\n",
      "CPU times: user 6.68 s, sys: 15.2 ms, total: 6.7 s\n",
      "Wall time: 6.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g_b[1] = -sum(\n",
    "    [\n",
    "        (\n",
    "            np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),t[k]] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "\n",
    "- sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*(np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),z] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][z] > 0 else 0) for z in range(L[3]) if wsums[3][k][z] > 0]\n",
    "                ) for k in range(len(t))]) / K\n",
    "\n",
    "print(\"gradient of the cost function wrt the 1st bias =\",g_b[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Weight Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have derived the gradients for a general weight at location (alpha,beta) on paper. I will use a as alpha, b as beta in this code.\n",
    "# lets begin with weights in layer 3 and work our way backwards to arrive at gradients for every single weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n",
      "weight in 3rd layer at position (a=63,b=9): -0.10437290387609338\n"
     ]
    }
   ],
   "source": [
    "# for W[3], we are using a in range(L[2]) and b in range(L[3]) to pull out the weight W[3][a][b]\n",
    "print(W[3].shape)\n",
    "print(\"weight in 3rd layer at position (a=63,b=9):\",W[3][63][9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.87447938e-04,  1.45452182e-01,  2.26306590e-01,\n",
       "        -3.92453681e-02,  2.25063160e-01, -1.21180300e-01,\n",
       "         2.87275692e-02,  6.26622195e-02,  6.45229307e-02,\n",
       "        -2.32097862e-01],\n",
       "       [-2.42870171e-01,  9.99865252e-02, -6.49397240e-02,\n",
       "         1.58056115e-02, -1.32022422e-02,  4.47440518e-02,\n",
       "         5.38091780e-03, -1.99428810e-01,  3.62763557e-02,\n",
       "        -1.93576732e-01],\n",
       "       [ 2.07985845e-01, -4.95418253e-02,  1.77876684e-01,\n",
       "         1.36737981e-01,  1.20648523e-01, -6.98344926e-02,\n",
       "        -2.88498330e-01,  7.35755868e-02,  2.25617212e-01,\n",
       "         2.73680122e-01],\n",
       "       [-4.40760192e-02,  2.58952729e-01,  5.73080939e-02,\n",
       "         2.64734152e-01,  6.66295265e-02,  7.25002158e-02,\n",
       "         5.88195958e-02, -4.60838914e-02,  1.74546725e-01,\n",
       "         2.52758435e-01],\n",
       "       [-4.37921823e-02,  1.27994671e-01, -1.20271490e-01,\n",
       "        -4.48291234e-03,  5.66440854e-02,  8.81803536e-02,\n",
       "         1.68375231e-01, -9.18308849e-02, -5.01334237e-02,\n",
       "        -9.92710117e-02],\n",
       "       [-8.64486293e-03, -2.92914923e-01,  9.02923822e-02,\n",
       "         3.59792105e-02,  4.36118594e-02, -9.91384505e-02,\n",
       "        -1.44938414e-02,  1.22245578e-01, -3.37484500e-02,\n",
       "        -2.05663757e-01],\n",
       "       [ 1.41211999e-02,  4.15891628e-02,  1.50547699e-01,\n",
       "        -2.88931844e-01, -7.36505576e-02, -5.62675791e-02,\n",
       "        -8.10569011e-02, -3.76087484e-02,  5.83933507e-02,\n",
       "        -5.60032336e-02],\n",
       "       [ 7.16291034e-02, -1.11420747e-01,  5.21782666e-02,\n",
       "        -4.31444581e-02,  2.71198706e-01,  1.51453733e-01,\n",
       "        -7.98585535e-02, -1.14367146e-01, -1.64458467e-01,\n",
       "        -9.57772228e-02],\n",
       "       [-2.41722204e-02,  7.61336799e-02, -1.42806853e-01,\n",
       "        -9.89183467e-02, -4.03207421e-02, -1.85046946e-01,\n",
       "         9.48131153e-02,  7.88261547e-02,  1.54471105e-01,\n",
       "        -1.05923606e-01],\n",
       "       [-1.13083369e-01, -9.26638970e-03, -3.79984793e-02,\n",
       "         3.44349646e-02,  5.21193082e-03,  6.65219551e-02,\n",
       "         9.11029575e-02,  5.22362185e-02,  1.91342343e-01,\n",
       "         9.14183393e-02],\n",
       "       [-8.08752480e-02, -3.04728813e-02,  3.61014030e-02,\n",
       "        -1.10517459e-01, -2.78680412e-02, -1.33025695e-01,\n",
       "         5.76540611e-02,  1.51385702e-01, -2.90766906e-01,\n",
       "        -3.78607596e-01],\n",
       "       [ 2.87335908e-01, -5.64337761e-03, -8.65744212e-02,\n",
       "         2.12850401e-01,  7.40460908e-02, -1.16749802e-01,\n",
       "        -9.83064855e-02, -2.38798608e-02, -1.04998961e-01,\n",
       "         8.23564982e-02],\n",
       "       [ 3.74371996e-02, -1.40023181e-02,  8.85828852e-03,\n",
       "        -5.45605474e-02,  6.57381518e-02, -1.19152204e-01,\n",
       "         2.06339756e-02, -4.69460260e-02, -1.45989043e-01,\n",
       "        -7.34513835e-02],\n",
       "       [ 1.14897884e-01,  5.56494674e-02, -2.17379290e-01,\n",
       "         3.47640269e-02,  4.25298874e-02, -8.01496385e-02,\n",
       "         1.39584932e-01,  1.39400754e-03, -2.31727347e-02,\n",
       "         9.70664308e-02],\n",
       "       [-5.18484962e-02,  9.26850090e-02,  1.03240591e-02,\n",
       "         4.80526274e-02,  1.12585143e-01, -9.59154582e-03,\n",
       "         5.15405701e-02,  6.60702301e-03,  2.52607904e-02,\n",
       "         1.02813799e-02],\n",
       "       [ 1.47332053e-01, -1.15069335e-01,  1.92896595e-01,\n",
       "        -6.80756755e-02,  5.82870492e-02, -1.26136117e-01,\n",
       "         3.35496612e-02,  4.46997251e-02, -1.46096680e-01,\n",
       "        -1.09682084e-01],\n",
       "       [ 2.14406370e-01,  1.61026648e-01,  1.56789039e-01,\n",
       "         3.86497971e-02,  1.05009886e-01,  1.05638447e-01,\n",
       "        -4.79901778e-02,  7.33526500e-02, -7.42516128e-02,\n",
       "         1.21965760e-01],\n",
       "       [ 5.46252914e-02, -2.08775065e-01,  3.30217531e-03,\n",
       "         1.91738370e-02,  3.89641115e-04,  1.33203060e-02,\n",
       "         8.77689609e-02,  1.00976624e-01, -1.60171340e-01,\n",
       "        -8.67906690e-03],\n",
       "       [ 8.43623852e-02, -1.08388989e-02, -1.02974257e-01,\n",
       "         1.72428536e-01, -6.27249013e-02,  2.73266398e-01,\n",
       "         4.46218600e-02,  6.41550258e-02, -8.05648860e-03,\n",
       "        -1.02117513e-01],\n",
       "       [ 5.20591422e-03,  1.81626512e-01, -4.95120383e-02,\n",
       "        -8.46101757e-02, -5.89154984e-02, -2.68808759e-01,\n",
       "         9.01802028e-03, -1.76231139e-01,  6.08894058e-04,\n",
       "         3.72956550e-02],\n",
       "       [ 9.21649243e-02, -1.25295818e-01, -1.50775063e-02,\n",
       "        -9.86682212e-02, -4.97302922e-03,  9.16969657e-02,\n",
       "        -1.53791601e-02,  2.08058801e-01, -1.29611178e-01,\n",
       "         8.44527902e-02],\n",
       "       [-1.08307283e-03, -5.55428864e-02,  4.97147858e-02,\n",
       "         1.44055772e-01,  4.14925004e-02,  1.49035918e-01,\n",
       "         2.02964393e-01, -1.65440456e-01,  5.55136542e-02,\n",
       "         1.57269160e-01],\n",
       "       [ 6.66745236e-02, -7.75276707e-02,  1.90529274e-02,\n",
       "        -7.45529998e-02, -8.45236959e-02,  8.55771391e-02,\n",
       "         1.09511662e-01,  1.35563315e-01,  8.02445025e-03,\n",
       "         8.77763854e-02],\n",
       "       [ 1.70296269e-01,  4.64599946e-02, -1.62657450e-01,\n",
       "        -6.63652489e-02,  1.38606607e-01, -5.05242577e-02,\n",
       "        -2.14823602e-01,  3.92098182e-02,  3.60432913e-02,\n",
       "         1.54495101e-01],\n",
       "       [ 4.00824866e-03,  6.79447972e-02,  3.47458772e-02,\n",
       "         2.39213852e-01, -6.69104726e-03, -5.88169522e-02,\n",
       "         3.00424191e-02, -1.45775965e-01, -2.32918024e-02,\n",
       "        -1.49821160e-01],\n",
       "       [-1.37756916e-01,  3.51186530e-02,  2.26194913e-01,\n",
       "        -1.80940116e-01, -1.03448194e-01,  9.21777617e-02,\n",
       "         3.24372979e-01, -1.48838398e-01, -6.68489022e-02,\n",
       "        -7.79992354e-02],\n",
       "       [ 9.45220534e-02, -2.27948899e-01, -8.33351793e-02,\n",
       "         3.56438310e-02, -4.83717135e-02, -1.40549319e-01,\n",
       "         1.81639879e-01, -4.87558842e-02, -1.51566949e-01,\n",
       "         1.88300802e-01],\n",
       "       [-1.48151415e-01, -3.48164326e-02,  1.41154290e-01,\n",
       "        -8.82601983e-02,  3.23510560e-02,  8.04218529e-02,\n",
       "        -1.69388832e-01, -3.42232326e-02, -1.80038730e-02,\n",
       "        -2.11072980e-01],\n",
       "       [ 8.12099078e-02, -2.81771229e-02,  1.28629945e-01,\n",
       "        -5.26402633e-02,  1.89635252e-01,  5.30380563e-02,\n",
       "        -1.02778315e-01, -1.39699570e-01,  3.42571313e-02,\n",
       "         1.07552770e-01],\n",
       "       [-9.84963327e-02, -2.84531479e-01,  2.87707422e-01,\n",
       "         1.11866427e-01, -4.28364179e-02, -1.99863819e-01,\n",
       "         1.00154648e-01,  9.00968323e-02,  2.04406617e-01,\n",
       "        -2.60710253e-02],\n",
       "       [ 9.62859188e-02,  2.00530749e-01, -2.77514866e-01,\n",
       "         4.76829429e-03, -1.19942068e-01, -8.96409938e-02,\n",
       "        -2.56102374e-01, -2.40744380e-02,  1.11142900e-01,\n",
       "         2.51813377e-01],\n",
       "       [ 9.08667259e-02,  1.76589557e-01,  5.60937310e-02,\n",
       "         2.64646748e-02,  3.81923956e-02,  2.09041691e-01,\n",
       "        -1.28525654e-01,  2.51442090e-01, -1.35856284e-02,\n",
       "         1.71884520e-01],\n",
       "       [-1.04449184e-01,  2.52639386e-01,  1.11149897e-01,\n",
       "         7.00591658e-03,  8.08544801e-02,  6.00072975e-02,\n",
       "         8.12707247e-03,  4.67606995e-02,  1.48088284e-01,\n",
       "        -5.62127848e-02],\n",
       "       [-1.55279606e-01,  1.32735470e-03,  9.67630503e-02,\n",
       "        -4.33993218e-02,  1.79649546e-02,  1.63139467e-02,\n",
       "        -1.01381373e-03, -8.61395631e-02, -1.06543509e-02,\n",
       "         1.41435330e-03],\n",
       "       [ 1.78642409e-01,  2.41292323e-01,  3.22281220e-02,\n",
       "         1.43308715e-02, -2.11412887e-03, -7.72606899e-02,\n",
       "         1.80775856e-01, -1.95060560e-02,  1.15498087e-01,\n",
       "        -3.52736814e-04],\n",
       "       [ 1.94825202e-02, -7.49820888e-02, -9.76254385e-03,\n",
       "         7.19395968e-02,  6.51553036e-02, -7.96546716e-02,\n",
       "        -2.93593295e-01, -2.31969659e-01,  1.52214147e-01,\n",
       "         2.76350837e-02],\n",
       "       [ 2.21329590e-02,  1.02497231e-02, -2.95504169e-02,\n",
       "         1.91256745e-01,  2.01701093e-02, -1.33196421e-01,\n",
       "        -2.01917448e-01,  9.04332410e-02,  2.03646254e-01,\n",
       "        -1.56507680e-01],\n",
       "       [-2.71861818e-02,  8.64279343e-02, -9.46919178e-02,\n",
       "         8.75168660e-02,  2.04423453e-01, -1.06531613e-01,\n",
       "        -1.69884810e-02,  1.96984738e-01, -4.19250762e-02,\n",
       "         8.06180303e-02],\n",
       "       [-1.08348355e-01,  7.79353955e-02, -1.60387882e-03,\n",
       "         2.19496988e-01,  1.07742458e-01,  1.72449987e-01,\n",
       "         1.65064596e-01,  2.69295774e-03,  7.33044923e-03,\n",
       "        -3.28298557e-02],\n",
       "       [ 1.56921952e-01, -1.47156942e-01, -6.80780686e-03,\n",
       "        -1.37098640e-02,  4.76376696e-02,  9.94742378e-03,\n",
       "         5.65357589e-02,  1.08174535e-02,  7.78999324e-03,\n",
       "         1.98413483e-01],\n",
       "       [-7.95590657e-02,  8.08689185e-02,  2.75626451e-01,\n",
       "         1.62471707e-02,  1.47083209e-02, -8.32544383e-02,\n",
       "        -1.72537435e-02, -4.19659674e-03,  1.48805314e-01,\n",
       "         1.56401868e-01],\n",
       "       [ 7.48747665e-02, -1.01592003e-02, -2.41471266e-02,\n",
       "         5.61999669e-02,  2.49761515e-01,  1.17821726e-01,\n",
       "         1.31420215e-01,  9.98552661e-02,  1.47151634e-01,\n",
       "         5.81971814e-02],\n",
       "       [-6.04770216e-03,  1.49218748e-02,  1.17356311e-02,\n",
       "        -1.20652774e-02, -6.65322170e-02,  1.07722778e-01,\n",
       "        -2.20622491e-01,  1.74193757e-01, -3.04979270e-01,\n",
       "         2.00323610e-01],\n",
       "       [ 2.90331578e-01,  2.60118292e-01,  1.76208156e-01,\n",
       "        -2.24824324e-03,  5.38036449e-02, -2.10997733e-01,\n",
       "         2.87439461e-02,  3.02926985e-01,  5.24276675e-02,\n",
       "         8.78070446e-02],\n",
       "       [-8.49952518e-02,  1.90030403e-01, -1.14124004e-01,\n",
       "        -1.01417185e-01, -1.71925638e-02,  3.74311147e-02,\n",
       "         1.09000376e-01, -1.81492846e-01, -2.67070911e-03,\n",
       "         8.56082283e-02],\n",
       "       [ 1.98555757e-01, -3.89125761e-02,  2.95638375e-02,\n",
       "        -5.85474607e-02, -7.09749775e-02,  9.12425546e-02,\n",
       "        -7.96127050e-02, -1.36219260e-01, -1.00657859e-02,\n",
       "        -1.76822936e-01],\n",
       "       [-6.67702395e-02, -3.37499240e-01, -7.93997092e-02,\n",
       "         2.22273009e-01, -5.90968594e-02, -4.94181106e-02,\n",
       "        -8.90006534e-02, -4.21875941e-02,  5.01930649e-02,\n",
       "        -1.01411623e-01],\n",
       "       [ 2.90530974e-02,  9.93141801e-02,  4.99465643e-02,\n",
       "         4.74685771e-02, -1.35698116e-01,  5.83930165e-02,\n",
       "        -7.11034665e-02, -8.74288836e-02, -2.33423361e-01,\n",
       "        -6.29791523e-02],\n",
       "       [-1.30451957e-01,  5.91783774e-02,  4.90682041e-02,\n",
       "         2.71655119e-02, -1.73820586e-01,  4.32392546e-02,\n",
       "         3.39532491e-01, -1.24294088e-01,  1.91911449e-01,\n",
       "        -8.92462082e-02],\n",
       "       [ 1.09322795e-02, -3.40973145e-02,  5.43527353e-02,\n",
       "         1.94210326e-01, -1.04532119e-01,  2.95969191e-02,\n",
       "        -1.01145292e-01, -9.30329002e-02, -1.20925130e-01,\n",
       "        -6.19308686e-02],\n",
       "       [-6.71270949e-02,  5.76039581e-02, -3.04900345e-01,\n",
       "         3.42221753e-02, -2.21401075e-01,  3.49455100e-01,\n",
       "        -1.14631752e-01,  4.05248062e-02, -8.46849837e-02,\n",
       "        -2.55953131e-02],\n",
       "       [ 1.51359499e-01,  2.19257155e-01, -3.19398439e-03,\n",
       "        -4.08122815e-02,  1.52483192e-02, -1.74388518e-01,\n",
       "         8.25300527e-02, -1.58083987e-01,  1.80988619e-01,\n",
       "         1.99008393e-02],\n",
       "       [-1.80068748e-02, -2.67321544e-02, -2.42053284e-02,\n",
       "        -3.08870003e-02,  2.08287055e-03, -1.09593791e-02,\n",
       "        -1.26370402e-01, -2.53508592e-01,  2.76449290e-02,\n",
       "        -4.16915284e-02],\n",
       "       [-3.74531573e-02,  8.12523195e-02,  9.74304086e-02,\n",
       "        -8.50731647e-03, -1.58800234e-01, -1.20455271e-02,\n",
       "         5.48361830e-02, -3.82960003e-02,  1.64170114e-02,\n",
       "         7.20836196e-02],\n",
       "       [ 2.13123366e-01, -2.20812467e-01,  1.38881012e-01,\n",
       "         2.73489909e-01,  1.80604677e-02, -9.70931435e-03,\n",
       "         8.02810806e-02, -7.85784344e-02, -6.51474899e-02,\n",
       "        -1.14263976e-01],\n",
       "       [ 3.44238841e-01,  2.90075044e-01, -1.26238279e-01,\n",
       "         1.45396008e-01,  8.67440155e-02, -4.84905119e-02,\n",
       "         2.64098872e-03,  2.14008998e-02,  1.89397897e-01,\n",
       "        -4.37532247e-02],\n",
       "       [ 2.19772808e-02, -2.35559041e-01,  3.11738667e-02,\n",
       "        -8.78631213e-02, -2.04883903e-01,  1.63423084e-01,\n",
       "        -1.37371798e-04, -1.26439106e-02,  2.51258016e-01,\n",
       "         2.09423133e-02],\n",
       "       [-9.88786800e-02, -1.20799527e-01, -1.92615387e-01,\n",
       "        -1.02991517e-01,  2.29839244e-01,  1.34103231e-02,\n",
       "        -1.44842921e-01, -3.58077314e-02,  7.81934378e-03,\n",
       "         2.04520290e-01],\n",
       "       [ 1.72121869e-01,  8.94059942e-02, -8.93746273e-02,\n",
       "        -1.71798655e-01, -7.90596823e-02,  1.17979268e-01,\n",
       "         6.29876668e-02, -6.95961183e-02,  9.34685150e-02,\n",
       "        -1.90863744e-01],\n",
       "       [ 3.56989385e-02,  5.88605999e-02, -7.41854722e-02,\n",
       "        -6.86452055e-02, -7.61417105e-02,  1.11018586e-01,\n",
       "        -2.11306956e-01, -1.59327904e-01,  9.48415938e-02,\n",
       "         6.49513852e-02],\n",
       "       [ 3.83348732e-02, -2.38424050e-01,  1.70731789e-01,\n",
       "         1.49061940e-01, -1.02078457e-01, -3.66119215e-02,\n",
       "        -9.24015081e-03, -1.68650199e-01,  1.90544909e-01,\n",
       "         1.71924540e-01],\n",
       "       [ 1.60223909e-01, -2.31620492e-01,  5.36508298e-02,\n",
       "        -1.34417049e-01, -1.97859791e-02, -3.92333934e-02,\n",
       "        -4.99274090e-02,  4.22065146e-02,  7.64586749e-03,\n",
       "        -2.25127719e-01],\n",
       "       [ 1.19058830e-01, -1.20512958e-01, -3.74034519e-02,\n",
       "         3.10847793e-01, -1.89957411e-02, -7.98867450e-02,\n",
       "         2.47984405e-02,  1.28906214e-02, -6.77737954e-02,\n",
       "        -1.09755894e-01],\n",
       "       [ 1.56765709e-01, -1.78714729e-01, -1.36027560e-01,\n",
       "        -2.65608302e-02, -9.38373192e-02, -2.51602351e-01,\n",
       "         1.37060686e-01, -6.80516612e-02,  1.48697117e-01,\n",
       "        -1.04372904e-01]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weightsum in layer 3 going into neuron b: -0.070389767775114\n",
      "value of neuron a in layer 2: 0.1013068605425941\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "k=8\n",
    "a=63\n",
    "b=9\n",
    "# for the W[3][a][b] weight gradient, we need the weightsum in layer 3 going into neuron b:\n",
    "print(\"weightsum in layer 3 going into neuron b:\",wsums[3][k][b])\n",
    "\n",
    "# and we need the value of neuron a in layer 2 (i.e. with our activation function - ReLU - applied)\n",
    "print(\"value of neuron a in layer 2:\",f(wsums[2][k])[a])\n",
    "\n",
    "# so the element of the sum that comes from training case k for the weight in layer 3 at position (a,b) is the following:\n",
    "sum_element_a_b_k = (wsums[3][k][b] > 0)*f(wsums[2][k])[a] * ( (b==t[k]) - phat_all_k[k][b] )\n",
    "print(sum_element_a_b_k)\n",
    "\n",
    "# there must be a cleverer way to approach this than calculating each weight's gradient one by one... I want to do it for all (a,b) in one!\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True False  True  True False False  True False]\n",
      "[0.         0.13139059 0.         0.         0.         0.07128883\n",
      " 0.         0.33769119 0.16523399 0.         0.         0.\n",
      " 0.01933263 0.         0.         0.07821427 0.         0.20319932\n",
      " 0.         0.         0.08602779 0.0321179  0.00512933 0.16903575\n",
      " 0.01273482 0.13141736 0.         0.         0.22408648 0.12734374\n",
      " 0.         0.         0.04231384 0.         0.1189067  0.\n",
      " 0.14082017 0.01026166 0.00609227 0.12231907 0.         0.10184215\n",
      " 0.11416016 0.         0.         0.0361915  0.00926809 0.\n",
      " 0.01697181 0.         0.01751762 0.         0.0787959  0.\n",
      " 0.         0.         0.         0.         0.14843978 0.1922957\n",
      " 0.16206729 0.15228808 0.         0.10130686]\n"
     ]
    }
   ],
   "source": [
    "print(wsums[3][k] > 0)\n",
    "print(f(wsums[2][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 64 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#dot product gets us the term (a,b) at the [b][a] index:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dot \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwsums\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwsums\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dot\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 64 is different from 10)"
     ]
    }
   ],
   "source": [
    "\n",
    "dot = np.transpose(wsums[3][k] > 0) @ f(wsums[2][k])\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.13139059, 0.        , 0.        , 0.        ,\n",
       "        0.07128883, 0.        , 0.33769119, 0.16523399, 0.        ,\n",
       "        0.        , 0.        , 0.01933263, 0.        , 0.        ,\n",
       "        0.07821427, 0.        , 0.20319932, 0.        , 0.        ,\n",
       "        0.08602779, 0.0321179 , 0.00512933, 0.16903575, 0.01273482,\n",
       "        0.13141736, 0.        , 0.        , 0.22408648, 0.12734374,\n",
       "        0.        , 0.        , 0.04231384, 0.        , 0.1189067 ,\n",
       "        0.        , 0.14082017, 0.01026166, 0.00609227, 0.12231907,\n",
       "        0.        , 0.10184215, 0.11416016, 0.        , 0.        ,\n",
       "        0.0361915 , 0.00926809, 0.        , 0.01697181, 0.        ,\n",
       "        0.01751762, 0.        , 0.0787959 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14843978, 0.1922957 ,\n",
       "        0.16206729, 0.15228808, 0.        , 0.10130686],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.13139059, 0.        , 0.        , 0.        ,\n",
       "        0.07128883, 0.        , 0.33769119, 0.16523399, 0.        ,\n",
       "        0.        , 0.        , 0.01933263, 0.        , 0.        ,\n",
       "        0.07821427, 0.        , 0.20319932, 0.        , 0.        ,\n",
       "        0.08602779, 0.0321179 , 0.00512933, 0.16903575, 0.01273482,\n",
       "        0.13141736, 0.        , 0.        , 0.22408648, 0.12734374,\n",
       "        0.        , 0.        , 0.04231384, 0.        , 0.1189067 ,\n",
       "        0.        , 0.14082017, 0.01026166, 0.00609227, 0.12231907,\n",
       "        0.        , 0.10184215, 0.11416016, 0.        , 0.        ,\n",
       "        0.0361915 , 0.00926809, 0.        , 0.01697181, 0.        ,\n",
       "        0.01751762, 0.        , 0.0787959 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14843978, 0.1922957 ,\n",
       "        0.16206729, 0.15228808, 0.        , 0.10130686],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.13139059, 0.        , 0.        , 0.        ,\n",
       "        0.07128883, 0.        , 0.33769119, 0.16523399, 0.        ,\n",
       "        0.        , 0.        , 0.01933263, 0.        , 0.        ,\n",
       "        0.07821427, 0.        , 0.20319932, 0.        , 0.        ,\n",
       "        0.08602779, 0.0321179 , 0.00512933, 0.16903575, 0.01273482,\n",
       "        0.13141736, 0.        , 0.        , 0.22408648, 0.12734374,\n",
       "        0.        , 0.        , 0.04231384, 0.        , 0.1189067 ,\n",
       "        0.        , 0.14082017, 0.01026166, 0.00609227, 0.12231907,\n",
       "        0.        , 0.10184215, 0.11416016, 0.        , 0.        ,\n",
       "        0.0361915 , 0.00926809, 0.        , 0.01697181, 0.        ,\n",
       "        0.01751762, 0.        , 0.0787959 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14843978, 0.1922957 ,\n",
       "        0.16206729, 0.15228808, 0.        , 0.10130686],\n",
       "       [0.        , 0.13139059, 0.        , 0.        , 0.        ,\n",
       "        0.07128883, 0.        , 0.33769119, 0.16523399, 0.        ,\n",
       "        0.        , 0.        , 0.01933263, 0.        , 0.        ,\n",
       "        0.07821427, 0.        , 0.20319932, 0.        , 0.        ,\n",
       "        0.08602779, 0.0321179 , 0.00512933, 0.16903575, 0.01273482,\n",
       "        0.13141736, 0.        , 0.        , 0.22408648, 0.12734374,\n",
       "        0.        , 0.        , 0.04231384, 0.        , 0.1189067 ,\n",
       "        0.        , 0.14082017, 0.01026166, 0.00609227, 0.12231907,\n",
       "        0.        , 0.10184215, 0.11416016, 0.        , 0.        ,\n",
       "        0.0361915 , 0.00926809, 0.        , 0.01697181, 0.        ,\n",
       "        0.01751762, 0.        , 0.0787959 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14843978, 0.1922957 ,\n",
       "        0.16206729, 0.15228808, 0.        , 0.10130686],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.13139059, 0.        , 0.        , 0.        ,\n",
       "        0.07128883, 0.        , 0.33769119, 0.16523399, 0.        ,\n",
       "        0.        , 0.        , 0.01933263, 0.        , 0.        ,\n",
       "        0.07821427, 0.        , 0.20319932, 0.        , 0.        ,\n",
       "        0.08602779, 0.0321179 , 0.00512933, 0.16903575, 0.01273482,\n",
       "        0.13141736, 0.        , 0.        , 0.22408648, 0.12734374,\n",
       "        0.        , 0.        , 0.04231384, 0.        , 0.1189067 ,\n",
       "        0.        , 0.14082017, 0.01026166, 0.00609227, 0.12231907,\n",
       "        0.        , 0.10184215, 0.11416016, 0.        , 0.        ,\n",
       "        0.0361915 , 0.00926809, 0.        , 0.01697181, 0.        ,\n",
       "        0.01751762, 0.        , 0.0787959 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14843978, 0.1922957 ,\n",
       "        0.16206729, 0.15228808, 0.        , 0.10130686],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dot product gets us the term (a,b) at the [b][a] index:\n",
    "dotted = np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2)\n",
    "print(dotted.shape)\n",
    "#perfect, now we just need to multiply this by ( (b==t[k]) - phat_all_k[k][b] ) for each corresponding b value... lets do with matrices!\n",
    "dotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label for case k=8 is 1, so we see that comes out biggest in calculation below\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.11255973294529902,\n",
       " 0.9042205383066246,\n",
       " -0.10234249934473547,\n",
       " -0.09577946169337533,\n",
       " -0.10656148572176075,\n",
       " -0.09797423573139616,\n",
       " -0.09577946169337533,\n",
       " -0.09577946169337533,\n",
       " -0.10166473778993206,\n",
       " -0.09577946169337533]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is code for the vector we want to multiply the above matrix by\n",
    "print(f\"true label for case k={k} is {t[k]}, so we see element 1 comes out biggest in vector below\")\n",
    "[( (b==t[k]) - phat_all_k[k][b] ) for b in range(L[3])]\n",
    "#we need the b element of this vector to multiply all b-related elements in the above matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]\n",
      " [-0.11255973  0.90422054 -0.1023425  -0.09577946 -0.10656149 -0.09797424\n",
      "  -0.09577946 -0.09577946 -0.10166474 -0.09577946]]\n"
     ]
    }
   ],
   "source": [
    "#try making a 10x10 version of the vector above, where column b contains the same b-related value 10 times\n",
    "tenten = np.array([[( (b==t[k]) - phat_all_k[k][b] ) for b in range(L[3])] for _ in range(L[3])])\n",
    "print(tenten)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128],\n",
       "       [ 0.        , -0.06846799,  0.        ,  0.        ,  0.        ,\n",
       "        -0.0371488 ,  0.        , -0.17597179, -0.08610388,  0.        ,\n",
       "         0.        ,  0.        , -0.01007429,  0.        ,  0.        ,\n",
       "        -0.04075767,  0.        , -0.10588772,  0.        ,  0.        ,\n",
       "        -0.04482931, -0.01673673, -0.00267291, -0.08808499, -0.00663615,\n",
       "        -0.06848194,  0.        ,  0.        , -0.11677207, -0.06635917,\n",
       "         0.        ,  0.        , -0.02204985,  0.        , -0.0619626 ,\n",
       "         0.        , -0.07338177, -0.00534738, -0.0031747 , -0.0637408 ,\n",
       "         0.        , -0.05307022, -0.05948917,  0.        ,  0.        ,\n",
       "        -0.01885949, -0.00482963,  0.        , -0.00884406,  0.        ,\n",
       "        -0.00912848,  0.        , -0.04106076,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.07735237, -0.10020581,\n",
       "        -0.0844537 , -0.07935773,  0.        , -0.05279128]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now try matrix multiplication to result in a 10x64 matrix with all the elements of our weightsum for training case k that we need\n",
    "tenten @ dotted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1: -0.0\n",
      "method 2: -0.05279127769951657\n"
     ]
    }
   ],
   "source": [
    "#compare the element of this relating to weight (a,b) vs the calculation we did for (a,b) earlier\n",
    "k=8\n",
    "a=63\n",
    "b=9 \n",
    "\n",
    "#method 1\n",
    "sum_element_a_b_k = (wsums[3][k][b] > 0)*f(wsums[2][k])[a] * ( (b==t[k]) - phat_all_k[k][b] )\n",
    "print(\"method 1:\",sum_element_a_b_k)\n",
    "\n",
    "#method 2\n",
    "print(\"method 2:\",(tenten @ dotted)[b][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.1013068605425941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug what is going on inside the matrix multiplications (this is the one to make dotted)\n",
    "print(np.transpose(np.array(wsums[3][k] > 0,ndmin=2))[b][0])\n",
    "print(np.array(f(wsums[2][k]),ndmin=2)[0][a])\n",
    "np.transpose(np.array(wsums[3][k] > 0,ndmin=2))[b][0] * np.array(f(wsums[2][k]),ndmin=2)[0][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is 0, so our corresponding element of the final matrix should also have been 0... my matrix multiplication is wrong!\n",
    "#if this is 0 I need the next matrix along in the calculation to show a 0\n",
    "dotted[b][a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973, -0.11255973,\n",
       "        -0.11255973, -0.11255973, -0.11255973, -0.11255973],\n",
       "       [ 0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054,  0.90422054,\n",
       "         0.90422054,  0.90422054,  0.90422054,  0.90422054],\n",
       "       [-0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ,\n",
       "        -0.1023425 , -0.1023425 , -0.1023425 , -0.1023425 ],\n",
       "       [-0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946],\n",
       "       [-0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149, -0.10656149,\n",
       "        -0.10656149, -0.10656149, -0.10656149, -0.10656149],\n",
       "       [-0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424, -0.09797424,\n",
       "        -0.09797424, -0.09797424, -0.09797424, -0.09797424],\n",
       "       [-0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946],\n",
       "       [-0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946],\n",
       "       [-0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474, -0.10166474,\n",
       "        -0.10166474, -0.10166474, -0.10166474, -0.10166474],\n",
       "       [-0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946, -0.09577946,\n",
       "        -0.09577946, -0.09577946, -0.09577946, -0.09577946]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I have realised my mistake... I need element-wise multiplication, not matrix multiplication. So, I need to use a 10x64 matrix\n",
    "ten64 = np.array([[( (b==t[k]) - phat_all_k[k][b] ) for _ in range(L[2])] for b in range(L[3])])\n",
    "print(ten64.shape)\n",
    "ten64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1: -0.02379698186227527\n",
      "method 2 revisited: -0.02379698186227527\n"
     ]
    }
   ],
   "source": [
    "#compare the element of this relating to weight (a,b) vs the calculation we did for (a,b) earlier\n",
    "k=9\n",
    "a=63\n",
    "b=9 \n",
    "\n",
    "#method 1\n",
    "sum_element_a_b_k = (wsums[3][k][b] > 0)*f(wsums[2][k])[a] * ( (b==t[k]) - phat_all_k[k][b] )\n",
    "print(\"method 1:\",sum_element_a_b_k)\n",
    "\n",
    "#method 2\n",
    "dotted = np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2)\n",
    "ten64 = np.array([[( (b==t[k]) - phat_all_k[k][b] ) for _ in range(L[2])] for b in range(L[3])])\n",
    "print(\"method 2 revisited:\",np.multiply(ten64,dotted)[b][a])\n",
    "\n",
    "#done, they match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so, now we have what we need to define the matrix of gradients for 3rd layer weights, see below\n",
    "\n",
    "# first make a copy of W that we will store the corresponding gradients in\n",
    "g_W = W.copy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 64)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get familiar with np.sum using the first 3 training cases\n",
    "first3 = np.array([np.multiply(np.array([[( (b==t[k]) - phat_all_k[k][b] ) for _ in range(L[2])] for b in range(L[3])]),np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2)) for k in range(3)])\n",
    "np.sum(first3,axis=0).shape\n",
    "#this must be the correct sum since the shape comes out as desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we should transpose the final answer so that the dimension matches the original weight matrix\n",
    "W[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 3 weights has shape: (64, 10)\n",
      "element a=L[2]-1,b=L[3]-1 has value: -0.002586417254026912\n"
     ]
    }
   ],
   "source": [
    "# now we have what we need to define the matrix of gradients for 3rd layer weights\n",
    "g_W[3] = np.transpose(-np.sum(np.array([np.multiply(np.array([[( (b==t[k]) - phat_all_k[k][b] ) for _ in range(L[2])] for b in range(L[3])]),np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2))\n",
    "                            for k in range(len(t))]),axis=0) / K)\n",
    "print(\"matrix of gradients relating to layer 3 weights has shape:\",g_W[3].shape)\n",
    "print(\"element a=L[2]-1,b=L[3]-1 has value:\",g_W[3][L[2]-1][L[3]-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=8\n",
    "np.array([( (b==t[k]) - phat_all_k[k][b] ) for _ in range(L[2])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=8\n",
    "np.repeat([( (b==t[k]) - phat_all_k[k][b] )],repeats=L[2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "gW3_fast = np.transpose(-np.sum(np.array([np.multiply(np.array([np.repeat([( (b==t[k]) - phat_all_k[k][b] )],repeats=L[2]) for b in range(L[3])]),np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2))\n",
    "                            for k in range(len(t))]),axis=0) / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this new approach is faster and equals the same, so lets use it below\n",
    "np.allclose(g_W[3],gW3_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 3 weights has shape: (64, 10)\n",
      "element a=L[2]-1,b=L[3]-1 has value: -0.002586417254026912\n"
     ]
    }
   ],
   "source": [
    "g_W[3] = np.transpose(-np.sum(np.array([np.multiply(np.array([np.repeat([( (b==t[k]) - phat_all_k[k][b] )],repeats=L[2]) for b in range(L[3])]),np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2))\n",
    "                            for k in range(len(t))]),axis=0) / K)\n",
    "\n",
    "print(\"matrix of gradients relating to layer 3 weights has shape:\",g_W[3].shape)\n",
    "print(\"element a=L[2]-1,b=L[3]-1 has value:\",g_W[3][L[2]-1][L[3]-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n"
     ]
    }
   ],
   "source": [
    "# now for the second layer weights. I will try to re-use the approach used for the 3rd layer weights, calculating the entire matrix at once!\n",
    "\n",
    "#recall we made \"dotted\" by dot-producting a vector of b-indexed \"weightsum>0 indicators\" with a vector of a-indexed neurons\n",
    "# the difference now is that b-index is layer 2 and a-index is layer 1\n",
    "weightsum_inds = np.transpose(np.array(wsums[2][k] > 0,ndmin=2))\n",
    "neurons = np.array(f(wsums[1][k]),ndmin=2)\n",
    "new_dotted = weightsum_inds @ neurons\n",
    "print(new_dotted.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22506316, -0.01320224,  0.12064852,  0.06662953,  0.05664409,\n",
       "        0.04361186, -0.07365056,  0.27119871, -0.04032074,  0.00521193,\n",
       "       -0.02786804,  0.07404609,  0.06573815,  0.04252989,  0.11258514,\n",
       "        0.05828705,  0.10500989,  0.00038964, -0.0627249 , -0.0589155 ,\n",
       "       -0.00497303,  0.0414925 , -0.0845237 ,  0.13860661, -0.00669105,\n",
       "       -0.10344819, -0.04837171,  0.03235106,  0.18963525, -0.04283642,\n",
       "       -0.11994207,  0.0381924 ,  0.08085448,  0.01796495, -0.00211413,\n",
       "        0.0651553 ,  0.02017011,  0.20442345,  0.10774246,  0.04763767,\n",
       "        0.01470832,  0.24976151, -0.06653222,  0.05380364, -0.01719256,\n",
       "       -0.07097498, -0.05909686, -0.13569812, -0.17382059, -0.10453212,\n",
       "       -0.22140107,  0.01524832,  0.00208287, -0.15880023,  0.01806047,\n",
       "        0.08674402, -0.2048839 ,  0.22983924, -0.07905968, -0.07614171,\n",
       "       -0.10207846, -0.01978598, -0.01899574, -0.09383732])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the analogue in this layer-2-weight setting to the \"ten64\" matrix from the layer-3-weights is quite different! We need to...\n",
    "# multiply the vector of all weights going into layer 3's neuron t[k] by this scalar (boolean) value: wsums[2][k][t[k]] > 0\n",
    "i3W3_method1 = np.array([W[3][b][t[k]] for b in range(L[2])]) * (wsums[3][k][t[k]] > 0)\n",
    "#however this is wasteful, since the scalar is boolean so we can avoid doing any of these multiplications by checking if is is true or false once\n",
    "i3W3_method2 = np.array([W[3][b][t[k]] for b in range(L[2])] if (wsums[3][k][t[k]] > 0) else [0 for _ in range(L[2])])\n",
    "i3W3_method2\n",
    "#next we need to do a similar thing but rather than do it for only the true label t[k], we do it for all digits z in range(L[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.14545218,  0.09998653, -0.04954183,  0.25895273,  0.12799467,\n",
       "        -0.29291492,  0.04158916, -0.11142075,  0.07613368, -0.00926639,\n",
       "        -0.03047288, -0.00564338, -0.01400232,  0.05564947,  0.09268501,\n",
       "        -0.11506934,  0.16102665, -0.20877506, -0.0108389 ,  0.18162651,\n",
       "        -0.12529582, -0.05554289, -0.07752767,  0.04645999,  0.0679448 ,\n",
       "         0.03511865, -0.2279489 , -0.03481643, -0.02817712, -0.28453148,\n",
       "         0.20053075,  0.17658956,  0.25263939,  0.00132735,  0.24129232,\n",
       "        -0.07498209,  0.01024972,  0.08642793,  0.0779354 , -0.14715694,\n",
       "         0.08086892, -0.0101592 ,  0.01492187,  0.26011829,  0.1900304 ,\n",
       "        -0.03891258, -0.33749924,  0.09931418,  0.05917838, -0.03409731,\n",
       "         0.05760396,  0.21925715, -0.02673215,  0.08125232, -0.22081247,\n",
       "         0.29007504, -0.23555904, -0.12079953,  0.08940599,  0.0588606 ,\n",
       "        -0.23842405, -0.23162049, -0.12051296, -0.17871473],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.03924537,  0.01580561,  0.13673798,  0.26473415, -0.00448291,\n",
       "         0.03597921, -0.28893184, -0.04314446, -0.09891835,  0.03443496,\n",
       "        -0.11051746,  0.2128504 , -0.05456055,  0.03476403,  0.04805263,\n",
       "        -0.06807568,  0.0386498 ,  0.01917384,  0.17242854, -0.08461018,\n",
       "        -0.09866822,  0.14405577, -0.074553  , -0.06636525,  0.23921385,\n",
       "        -0.18094012,  0.03564383, -0.0882602 , -0.05264026,  0.11186643,\n",
       "         0.00476829,  0.02646467,  0.00700592, -0.04339932,  0.01433087,\n",
       "         0.0719396 ,  0.19125674,  0.08751687,  0.21949699, -0.01370986,\n",
       "         0.01624717,  0.05619997, -0.01206528, -0.00224824, -0.10141718,\n",
       "        -0.05854746,  0.22227301,  0.04746858,  0.02716551,  0.19421033,\n",
       "         0.03422218, -0.04081228, -0.030887  , -0.00850732,  0.27348991,\n",
       "         0.14539601, -0.08786312, -0.10299152, -0.17179866, -0.06864521,\n",
       "         0.14906194, -0.13441705,  0.31084779, -0.02656083],\n",
       "       [ 0.22506316, -0.01320224,  0.12064852,  0.06662953,  0.05664409,\n",
       "         0.04361186, -0.07365056,  0.27119871, -0.04032074,  0.00521193,\n",
       "        -0.02786804,  0.07404609,  0.06573815,  0.04252989,  0.11258514,\n",
       "         0.05828705,  0.10500989,  0.00038964, -0.0627249 , -0.0589155 ,\n",
       "        -0.00497303,  0.0414925 , -0.0845237 ,  0.13860661, -0.00669105,\n",
       "        -0.10344819, -0.04837171,  0.03235106,  0.18963525, -0.04283642,\n",
       "        -0.11994207,  0.0381924 ,  0.08085448,  0.01796495, -0.00211413,\n",
       "         0.0651553 ,  0.02017011,  0.20442345,  0.10774246,  0.04763767,\n",
       "         0.01470832,  0.24976151, -0.06653222,  0.05380364, -0.01719256,\n",
       "        -0.07097498, -0.05909686, -0.13569812, -0.17382059, -0.10453212,\n",
       "        -0.22140107,  0.01524832,  0.00208287, -0.15880023,  0.01806047,\n",
       "         0.08674402, -0.2048839 ,  0.22983924, -0.07905968, -0.07614171,\n",
       "        -0.10207846, -0.01978598, -0.01899574, -0.09383732],\n",
       "       [-0.1211803 ,  0.04474405, -0.06983449,  0.07250022,  0.08818035,\n",
       "        -0.09913845, -0.05626758,  0.15145373, -0.18504695,  0.06652196,\n",
       "        -0.13302569, -0.1167498 , -0.1191522 , -0.08014964, -0.00959155,\n",
       "        -0.12613612,  0.10563845,  0.01332031,  0.2732664 , -0.26880876,\n",
       "         0.09169697,  0.14903592,  0.08557714, -0.05052426, -0.05881695,\n",
       "         0.09217776, -0.14054932,  0.08042185,  0.05303806, -0.19986382,\n",
       "        -0.08964099,  0.20904169,  0.0600073 ,  0.01631395, -0.07726069,\n",
       "        -0.07965467, -0.13319642, -0.10653161,  0.17244999,  0.00994742,\n",
       "        -0.08325444,  0.11782173,  0.10772278, -0.21099773,  0.03743111,\n",
       "         0.09124255, -0.04941811,  0.05839302,  0.04323925,  0.02959692,\n",
       "         0.3494551 , -0.17438852, -0.01095938, -0.01204553, -0.00970931,\n",
       "        -0.04849051,  0.16342308,  0.01341032,  0.11797927,  0.11101859,\n",
       "        -0.03661192, -0.03923339, -0.07988674, -0.25160235],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.06266222, -0.19942881,  0.07357559, -0.04608389, -0.09183088,\n",
       "         0.12224558, -0.03760875, -0.11436715,  0.07882615,  0.05223622,\n",
       "         0.1513857 , -0.02387986, -0.04694603,  0.00139401,  0.00660702,\n",
       "         0.04469973,  0.07335265,  0.10097662,  0.06415503, -0.17623114,\n",
       "         0.2080588 , -0.16544046,  0.13556331,  0.03920982, -0.14577597,\n",
       "        -0.1488384 , -0.04875588, -0.03422323, -0.13969957,  0.09009683,\n",
       "        -0.02407444,  0.25144209,  0.0467607 , -0.08613956, -0.01950606,\n",
       "        -0.23196966,  0.09043324,  0.19698474,  0.00269296,  0.01081745,\n",
       "        -0.0041966 ,  0.09985527,  0.17419376,  0.30292698, -0.18149285,\n",
       "        -0.13621926, -0.04218759, -0.08742888, -0.12429409, -0.0930329 ,\n",
       "         0.04052481, -0.15808399, -0.25350859, -0.038296  , -0.07857843,\n",
       "         0.0214009 , -0.01264391, -0.03580773, -0.06959612, -0.1593279 ,\n",
       "        -0.1686502 ,  0.04220651,  0.01289062, -0.06805166],\n",
       "       [ 0.06452293,  0.03627636,  0.22561721,  0.17454672, -0.05013342,\n",
       "        -0.03374845,  0.05839335, -0.16445847,  0.1544711 ,  0.19134234,\n",
       "        -0.29076691, -0.10499896, -0.14598904, -0.02317273,  0.02526079,\n",
       "        -0.14609668, -0.07425161, -0.16017134, -0.00805649,  0.00060889,\n",
       "        -0.12961118,  0.05551365,  0.00802445,  0.03604329, -0.0232918 ,\n",
       "        -0.0668489 , -0.15156695, -0.01800387,  0.03425713,  0.20440662,\n",
       "         0.1111429 , -0.01358563,  0.14808828, -0.01065435,  0.11549809,\n",
       "         0.15221415,  0.20364625, -0.04192508,  0.00733045,  0.00778999,\n",
       "         0.14880531,  0.14715163, -0.30497927,  0.05242767, -0.00267071,\n",
       "        -0.01006579,  0.05019306, -0.23342336,  0.19191145, -0.12092513,\n",
       "        -0.08468498,  0.18098862,  0.02764493,  0.01641701, -0.06514749,\n",
       "         0.1893979 ,  0.25125802,  0.00781934,  0.09346852,  0.09484159,\n",
       "         0.19054491,  0.00764587, -0.0677738 ,  0.14869712],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we need to do a similar thing but rather than do it for only the true label t[k], we do it for all digits z in range(L[3])\n",
    "# so, this prompts me to think we should calculate this the same for all 9 digits, store it, and recall the t[k] entry for the bit we just did above\n",
    "#also, use np.repeat for more efficient 0ing\n",
    "i3W3_all_z = np.array([np.array([W[3][b][z] for b in range(L[2])]) if (wsums[3][k][z] > 0) else np.repeat([0],L[2]) for z in range(L[3])])\n",
    "print(i3W3_all_z.shape)\n",
    "i3W3_all_z\n",
    "#now we can take i3W3_all_z[t[k]] when we want the boolean-multiplied weights that feed into the true labal m=t[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "# what if we break out the gradient calculation so I statically create this for all k and all z? only takes 4.5s, could be good\n",
    "i3W3_all_k_all_z = np.array([[np.array([W[3][b][z] for b in range(L[2])]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])] for k in range(len(t))])\n",
    "print(i3W3_all_k_all_z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.39313316e-02  9.57665556e-03 -4.74508936e-03  2.48023530e-02\n",
      "   1.22592607e-02 -2.80552336e-02  3.98338762e-03 -1.06718192e-02\n",
      "   7.29204288e-03 -8.87529817e-04 -2.91867617e-03 -5.40519670e-04\n",
      "  -1.34113449e-03  5.33007604e-03  8.87732027e-03 -1.10212790e-02\n",
      "   1.54230457e-02 -1.99963633e-02 -1.03814391e-03  1.73960895e-02\n",
      "  -1.20007660e-02 -5.31986776e-03 -7.42555857e-03  4.44991327e-03\n",
      "   6.50771610e-03  3.36364568e-03 -2.18328228e-02 -3.33469918e-03\n",
      "  -2.69878966e-03 -2.72522719e-02  1.92067272e-02  1.69136527e-02\n",
      "   2.41976644e-02  1.27133319e-04  2.31108488e-02 -7.18174410e-03\n",
      "   9.81712966e-04  8.27802103e-03  7.46461023e-03 -1.40946127e-02\n",
      "   7.74558148e-03 -9.73042740e-04  1.42920914e-03  2.49139900e-02\n",
      "   1.82010097e-02 -3.72702559e-03 -3.23254956e-02  9.51225871e-03\n",
      "   5.66807313e-03 -3.26582243e-03  5.51727610e-03  2.10003323e-02\n",
      "  -2.56039136e-03  7.78230342e-03 -2.11492992e-02  2.77832316e-02\n",
      "  -2.25617182e-02 -1.15701136e-02  8.56325800e-03  5.63763657e-03\n",
      "  -2.28361271e-02 -2.21844861e-02 -1.15426663e-02 -1.71172005e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.18725473e-02  4.38376428e-03 -6.84198104e-03  7.10315324e-03\n",
      "   8.63940275e-03 -9.71301392e-03 -5.51277306e-03  1.48385637e-02\n",
      "  -1.81298331e-02  6.51743771e-03 -1.30330908e-02 -1.14384726e-02\n",
      "  -1.16738461e-02 -7.85259957e-03 -9.39724372e-04 -1.23580896e-02\n",
      "   1.03498461e-02  1.30504680e-03  2.67730665e-02 -2.63363327e-02\n",
      "   8.98394013e-03  1.46016802e-02  8.38435480e-03 -4.95007553e-03\n",
      "  -5.76254594e-03  9.03104576e-03 -1.37702121e-02  7.87926957e-03\n",
      "   5.19636303e-03 -1.95815049e-02 -8.78250786e-03  2.04807000e-02\n",
      "   5.87916911e-03  1.59834646e-03 -7.56955704e-03 -7.80410557e-03\n",
      "  -1.30498176e-02 -1.04373534e-02  1.68956557e-02  9.74591242e-04\n",
      "  -8.15678996e-03  1.15434935e-02  1.05540568e-02 -2.06723417e-02\n",
      "   3.66728486e-03  8.93941955e-03 -4.84170161e-03  5.72101116e-03\n",
      "   4.23633293e-03  2.89973553e-03  3.42375964e-02 -1.70855818e-02\n",
      "  -1.07373679e-03 -1.18015132e-03 -9.51262653e-04 -4.75082085e-03\n",
      "   1.60112517e-02  1.31386616e-03  1.15589287e-02  1.08769611e-02\n",
      "  -3.58702503e-03 -3.84386174e-03 -7.82684278e-03 -2.46505481e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.00175365e-03 -1.91011841e-02  7.04703010e-03 -4.41389031e-03\n",
      "  -8.79551272e-03  1.17086157e-02 -3.60214568e-03 -1.09540237e-02\n",
      "   7.54992666e-03  5.00315689e-03  1.44996411e-02 -2.28720021e-03\n",
      "  -4.49646510e-03  1.33517292e-04  6.32817107e-04  4.28131561e-03\n",
      "   7.02567733e-03  9.67148673e-03  6.14473384e-03 -1.68793236e-02\n",
      "   1.99277599e-02 -1.58457979e-02  1.29841813e-02  3.75549528e-03\n",
      "  -1.39623435e-02 -1.42556616e-02 -4.66981234e-03 -3.27788280e-03\n",
      "  -1.33803496e-02  8.62942610e-03 -2.30583671e-03  2.40829880e-02\n",
      "   4.47871462e-03 -8.25040098e-03 -1.86827955e-03 -2.22179291e-02\n",
      "   8.66164714e-03  1.88670922e-02  2.57930043e-04  1.03608987e-03\n",
      "  -4.01947777e-04  9.56408364e-03  1.66841843e-02  2.90141835e-02\n",
      "  -1.73832871e-02 -1.30470074e-02 -4.04070505e-03 -8.37389140e-03\n",
      "  -1.19048209e-02 -8.91064110e-03  3.88144412e-03 -1.51411992e-02\n",
      "  -2.42809165e-02 -3.66797030e-03 -7.52620015e-03  2.04976666e-03\n",
      "  -1.21102695e-03 -3.42964523e-03 -6.66587875e-03 -1.52603409e-02\n",
      "  -1.61532253e-02  4.04251725e-03  1.23465678e-03 -6.51795148e-03]\n",
      " [ 6.55970683e-03  3.68802619e-03  2.29373147e-02  1.77452470e-02\n",
      "  -5.09680138e-03 -3.43102732e-03  5.93654469e-03 -1.67196270e-02\n",
      "   1.57042643e-02  1.94527692e-02 -2.95607413e-02 -1.06746918e-02\n",
      "  -1.48419378e-02 -2.35585000e-03  2.56813163e-03 -1.48528806e-02\n",
      "  -7.54877075e-03 -1.62837773e-02 -8.19060801e-04  6.19030548e-05\n",
      "  -1.31768864e-02  5.64378110e-03  8.15803630e-04  3.66433176e-03\n",
      "  -2.36795498e-03 -6.79617612e-03 -1.54090141e-02 -1.83035902e-03\n",
      "   3.48274227e-03  2.07809452e-02  1.12993138e-02 -1.38117935e-03\n",
      "   1.50553565e-02 -1.08317179e-03  1.17420827e-02  1.54748113e-02\n",
      "   2.07036430e-02 -4.26230188e-03  7.45248199e-04  7.91967620e-04\n",
      "   1.51282532e-02  1.49601323e-02 -3.10056375e-02  5.33004507e-03\n",
      "  -2.71516942e-04 -1.02333549e-03  5.10286478e-03 -2.37309247e-02\n",
      "   1.95106271e-02 -1.22938216e-02 -8.60947666e-03  1.84001605e-02\n",
      "   2.81051446e-03  1.66903116e-03 -6.62320248e-03  1.92550876e-02\n",
      "   2.55440804e-02  7.94951535e-04  9.50245207e-03  9.64204576e-03\n",
      "   1.93716982e-02  7.77315113e-04 -6.89020514e-03  1.51172534e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "(64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.13083194,  0.10143926, -0.0679391 ,  0.21371587,  0.12098832,\n",
       "       -0.26342426,  0.04078415, -0.08791384,  0.06371728, -0.03935222,\n",
       "        0.00053999,  0.01929751,  0.01835107,  0.06039432,  0.08154646,\n",
       "       -0.0811184 ,  0.13577685, -0.18347146, -0.04189949,  0.20738418,\n",
       "       -0.12902987, -0.05462268, -0.09228645,  0.03954033,  0.08352993,\n",
       "        0.0437758 , -0.17226704, -0.03425276, -0.02077709, -0.26710807,\n",
       "        0.18111305,  0.1164934 ,  0.20302848,  0.00893545,  0.21587723,\n",
       "       -0.05325312, -0.00704746,  0.07398248,  0.05257195, -0.13586498,\n",
       "        0.06655382, -0.04525387,  0.01726006,  0.22153242,  0.18581691,\n",
       "       -0.03005463, -0.3013942 ,  0.11618573,  0.04166817, -0.01252676,\n",
       "        0.02257712,  0.21208344, -0.00162762,  0.07664911, -0.1845625 ,\n",
       "        0.24573778, -0.25334163, -0.10790859,  0.06644723,  0.0479643 ,\n",
       "       -0.21521937, -0.21041198, -0.0954879 , -0.14554628])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so, the term in brackets (in my notes) will be something along the lines of this, using our big i3W3_all_k_all_z object\n",
    "k=8\n",
    "#to illustrate what I'm doing let me show you what this npsumterm is that we will be summing along axis=0 i.e. sum one i3W3 matrix per digit z\n",
    "npsumterm = np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])])\n",
    "print(npsumterm.shape)\n",
    "print(npsumterm)\n",
    "\n",
    "# then we arrive at this final expression in brackets in my notes, as below:\n",
    "bracketsterm = i3W3_all_k_all_z[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])]),axis=0)\n",
    "print(bracketsterm.shape)\n",
    "bracketsterm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64, 128)\n"
     ]
    }
   ],
   "source": [
    "# so now we need the product of the matrix \"new_dotted\" and the array \"bracketsterm\" above\n",
    "print(bracketsterm.shape)\n",
    "print(new_dotted.shape)\n",
    "#it will be possible with transposition. The result will be a 128x1 array is this what we expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember we want to end up with this shape... perhaps it needs to be elementwise np.multiply again?\n",
    "W[2].shape\n",
    "#YES it makes sense that we need to repeat this bracketsterm 64x1 vector 128 times to get a 64x128 matrix we can np.multiply\n",
    "#(the reason for this is that bracketsterm has no a dependency, so for a in range(L[1]) we need to repeat bracketsterm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bracketsterm_repeated = np.array([bracketsterm for _ in range(L[1])])\n",
    "bracketsterm_repeated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -0.        , ..., -0.        ,\n",
       "        -0.        , -0.        ],\n",
       "       [ 0.        ,  0.        , -0.        , ..., -0.        ,\n",
       "        -0.        , -0.        ],\n",
       "       [ 0.        ,  0.03620485, -0.        , ..., -0.07509847,\n",
       "        -0.03408074, -0.05194715],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        , -0.        , ..., -0.        ,\n",
       "        -0.        , -0.        ],\n",
       "       [ 0.        ,  0.        , -0.        , ..., -0.        ,\n",
       "        -0.        , -0.        ],\n",
       "       [ 0.        ,  0.03119779, -0.        , ..., -0.0647125 ,\n",
       "        -0.02936744, -0.04476296]])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now with transposition we can use np.multiply to get the entire term within the sum we have derived for weight-layer-2 gradients!\n",
    "sumterm = np.multiply(np.transpose(new_dotted),bracketsterm_repeated)\n",
    "sumterm\n",
    "#now this needs summing over all K, so lets bring together all we need in the next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 2 weights has shape: (128, 64)\n",
      "element a=L[1]-1,b=L[2]-1 has value: -0.0011051033336220632\n"
     ]
    }
   ],
   "source": [
    "i3W3_all_k_all_z = np.array([[np.array([W[3][b][z] for b in range(L[2])]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])] for k in range(len(t))])\n",
    "\n",
    "#include bracketsterm and others in comments here to see comparison, but I will do all next bits in one step\n",
    "# bracketsterm = i3W3_all_k_all_z[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])]),axis=0)\n",
    "# bracketsterm_repeated = np.array([bracketsterm for _ in range(L[1])])\n",
    "# sumterm = np.multiply(np.transpose(new_dotted),bracketsterm_repeated)\n",
    "# new_dotted = np.transpose(np.array(wsums[2][k] > 0,ndmin=2)) @ np.array(f(wsums[1][k]),ndmin=2)\n",
    "# AND FINALLY: sumterm = np.multiply(np.transpose(new_dotted),bracketsterm_repeated)\n",
    "\n",
    "g_W[2] = -sum([np.multiply(\n",
    "    #start np.multiply with the transpose of new_dotted\n",
    "    np.transpose(np.transpose(np.array(wsums[2][k] > 0,ndmin=2)) @ np.array(f(wsums[1][k]),ndmin=2)),\n",
    "            #bracketsterm_repeated is defined as the right-part of this np.multiply\n",
    "            np.array([i3W3_all_k_all_z[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])]),axis=0) for _ in range(L[1])]))\n",
    "            for k in range(len(t))]) / K\n",
    "\n",
    "print(\"matrix of gradients relating to layer 2 weights has shape:\",g_W[2].shape)\n",
    "print(\"element a=L[1]-1,b=L[2]-1 has value:\",g_W[2][L[1]-1][L[2]-1])\n",
    "#took 1m52s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64)\n",
      "(128, 64)\n"
     ]
    }
   ],
   "source": [
    "#repeat this for _ in range(L[1]) in a flat list, since that is all the \"for _ in range(L[1])\" loop does above\n",
    "gW2_repeat = np.repeat([i3W3_all_k_all_z[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])]),axis=0)],repeats=L[1],axis=0)\n",
    "print(gW2_repeat.shape)\n",
    "\n",
    "gW2_norepeat = np.array([i3W3_all_k_all_z[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])]),axis=0) for _ in range(L[1])])\n",
    "print(gW2_norepeat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#check all elements are equal\n",
    "eps = 0.000000000001\n",
    "for j in range(64):\n",
    "    for i in range(128):\n",
    "        if gW2_norepeat[i][j] - gW2_repeat[i][j] > eps:\n",
    "            print(i,j)\n",
    "\n",
    "# all equal, so sub this in to potentially speed up this learning step\n",
    "# but before I do that, I will try to speed up the generation of i3W3_all_k_all_z too\n",
    "print(np.allclose(gW2_norepeat,gW2_repeat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10, 64)\n",
      "CPU times: user 4 s, sys: 124 ms, total: 4.12 s\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i3W3_all_k_all_z = np.array([[np.array([W[3][b][z] for b in range(L[2])]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])] for k in range(len(t))])\n",
    "print(i3W3_all_k_all_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10, 64)\n",
      "CPU times: user 499 ms, sys: 155 ms, total: 654 ms\n",
      "Wall time: 756 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "i3W3_all_k_all_z_fast = np.array(\n",
    "    [\n",
    "        [np.array(W[3][:,z]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])\n",
    "         ] for k in range(len(t))\n",
    "         ]\n",
    "         )\n",
    "print(i3W3_all_k_all_z_fast.shape)\n",
    "#much quicker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all elements are equal\n",
    "eps = 0.000000000001\n",
    "for k in range(len(t)):\n",
    "    for z in range(10):\n",
    "        for b in range(64):\n",
    "            if i3W3_all_k_all_z[k][z][b] - i3W3_all_k_all_z_fast[k][z][b] > eps:\n",
    "                print(k,z,b)\n",
    "\n",
    "# all equal, so sub this in to potentially speed up this learning step too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all close to each other\n",
    "np.allclose(i3W3_all_k_all_z,i3W3_all_k_all_z_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 2.48 s, total: 6.82 s\n",
      "Wall time: 8.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# faster layer 2 weights\n",
    "i3W3_all_k_all_z_fast = np.array(\n",
    "    [\n",
    "        [np.array(W[3][:,z]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])\n",
    "         ] for k in range(len(t))\n",
    "         ]\n",
    "         )\n",
    "\n",
    "fast_g_W2 = -sum([np.multiply(\n",
    "    #start np.multiply with the transpose of new_dotted\n",
    "    np.transpose(np.transpose(np.array(wsums[2][k] > 0,ndmin=2)) @ np.array(f(wsums[1][k]),ndmin=2)),\n",
    "            #bracketsterm_repeated is defined as the right-part of this np.multiply\n",
    "            np.repeat([i3W3_all_k_all_z_fast[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z_fast[k][z] for z in range(L[3])]),axis=0)],repeats=L[1],axis=0))\n",
    "            for k in range(len(t))]) / K\n",
    "\n",
    "\n",
    "# np.multiply(np.transpose(np.transpose(np.array(wsums[2][k] > 0,ndmin=2)) @ np.array(f(wsums[1][k]),ndmin=2)),\n",
    "#                      #brackets term repeated unpacked w darya:)\n",
    "#                      np.repeat([i3W3_all_k_all_z[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z[k][z] for z in range(L[3])]),axis=0)],repeats=L[1],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equal: True\n",
      "allclose: True\n"
     ]
    }
   ],
   "source": [
    "# check equality, also dumb dumb finally learnt to use np.array_equal(arr1, arr2)  \n",
    "print(\"equal:\",np.array_equal(g_W[2], fast_g_W2))  #floats so instead use allcose\n",
    "print(\"allclose:\",np.allclose(g_W[2], fast_g_W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 2 weights has shape: (128, 64)\n",
      "element a=L[1]-1,b=L[2]-1 has value: -0.0011051033336220632\n",
      "CPU times: user 4.37 s, sys: 2.08 s, total: 6.45 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# so, here is the final code I'll use for layer 2 weights\n",
    "i3W3_all_k_all_z_fast = np.array(\n",
    "    [\n",
    "        [np.array(W[3][:,z]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])\n",
    "         ] for k in range(len(t))\n",
    "         ]\n",
    "         )\n",
    "\n",
    "g_W[2] = -sum([np.multiply(\n",
    "    #start np.multiply with the transpose of new_dotted\n",
    "    np.transpose(np.transpose(np.array(wsums[2][k] > 0,ndmin=2)) @ np.array(f(wsums[1][k]),ndmin=2)),\n",
    "            #bracketsterm_repeated is defined as the right-part of this np.multiply\n",
    "            np.repeat([i3W3_all_k_all_z_fast[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z_fast[k][z] for z in range(L[3])]),axis=0)],repeats=L[1],axis=0))\n",
    "            for k in range(len(t))]) / K\n",
    "\n",
    "print(\"matrix of gradients relating to layer 2 weights has shape:\",g_W[2].shape)\n",
    "print(\"element a=L[1]-1,b=L[2]-1 has value:\",g_W[2][L[1]-1][L[2]-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now layer 1 weights... it has all been building up to this point....... looks hairy!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "# the equivalent of our friend i3W3_all_k_all_z in the last step will be one sum (over i in range(L[2])) we need for each digit z in range(L[3])\n",
    "# the trick to save compute time is that we can skip the sum over i if we see that the weightsum at digit z is 0 (since then we do np.zeros)\n",
    "\n",
    "\n",
    "i3thensum_all_k_all_z = np.array([[np.array([sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])]) if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])] for k in range(len(t))])\n",
    "#can I not take the indicator (wsums[2][k][i] > 0) out of the inner bit to save us some loops when that value is just 0?\n",
    "print(i3thensum_all_k_all_z.shape)\n",
    "\n",
    "#to speed this up we should make it use more matrix/vector multiplication & summing (np.multiply and np.sum), rather than list comprehension loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i3thensum_all_k_all_z_1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            np.array([\n",
    "                    sum([\n",
    "                        W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])\n",
    "                        ]) for b in range(L[1])\n",
    "                      ]) if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])\n",
    "        ] for k in range(len(t))\n",
    "    ]\n",
    "                                    )\n",
    "\n",
    "# I think we could remove the need for the \"for b in range(L[1])\" from this bit:\n",
    "# sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we could remove the need for the \"for b in range(L[1])\" from this bit:\n",
    "# sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])\n",
    "\n",
    "#set a example value for z and k\n",
    "z=1  \n",
    "k=8\n",
    "# tryit = np.array([sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])])\n",
    "\n",
    "tryit2 = [\n",
    "            np.array([\n",
    "                    sum([\n",
    "                        W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])\n",
    "                        ]) for b in range(L[1])\n",
    "                      ]) if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])\n",
    "        ]\n",
    "#if this is that quick, 60k of them shouldnt take tooooooo long right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.666666666666668"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay if we take 1s per training example we would need 16 hours, I understand the danger now\n",
    "(60000/60)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try for 1000 training cases i.e. less than 2% of the training cases: 23s\n",
    "i3thensum_all_k_all_z_2 = [\n",
    "        [\n",
    "            np.array([\n",
    "                    sum([\n",
    "                        W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])\n",
    "                        ]) for b in range(L[1])\n",
    "                      ]) if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])\n",
    "        ] for k in range(1000)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.9 minutes, checks out\n",
      "SO WE NEED QUICKER CODE!\n"
     ]
    }
   ],
   "source": [
    "print(23.9*10*6 / 60,\"minutes, checks out\")\n",
    "print(\"SO WE NEED QUICKER CODE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we could remove the need for the \"for b in range(L[1])\" from this bit:\n",
    "# sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])\n",
    "\n",
    "#set a example value for z and k\n",
    "z=1  \n",
    "k=8\n",
    "tryit = np.array([sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0, -0.24287017092549698, 0.0, -0.0, -0.0, -0.008644862926931642, 0.0, 0.07162910344923537, -0.024172220378284436, -0.0, -0.0, 0.0, 0.03743719958845998, 0.0, -0.0, 0.1473320530124626, 0.0, 0.05462529140985817, 0.0, 0.0, 0.0921649242500377, -0.0010830728307881978, 0.06667452355090851, 0.1702962685551397, 0.004008248659195136, -0.13775691579536153, 0.0, -0.0, 0.08120990776678016, -0.09849633269099269, 0.0, 0.0, -0.10444918430217955, -0.0, 0.17864240901882328, 0.0, 0.022132958951758975, -0.027186181839476364, -0.1083483551224283, 0.15692195183875637, -0.0, 0.07487476653092606, -0.006047702162799587, 0.0, -0.0, 0.19855575738875414, -0.06677023951016371, 0.0, -0.1304519571476314, 0.0, -0.06712709486479407, 0.0, -0.01800687481850146, -0.0, 0.0, 0.0, 0.0, -0.0, 0.17212186876210242, 0.03569893846000474, 0.038334873225478044, 0.16022390921362215, 0.0, 0.1567657085124227]\n",
      "(array([ 1,  5,  7,  8, 12, 15, 17, 20, 21, 22, 23, 24, 25, 28, 29, 32, 34,\n",
      "       36, 37, 38, 39, 41, 42, 45, 46, 48, 50, 52, 58, 59, 60, 61, 63]),)\n",
      "33\n",
      "(33, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.24287017, -0.00864486,  0.0716291 , -0.02417222,  0.0374372 ,\n",
       "        0.14733205,  0.05462529,  0.09216492, -0.00108307,  0.06667452,\n",
       "        0.17029627,  0.00400825, -0.13775692,  0.08120991, -0.09849633,\n",
       "       -0.10444918,  0.17864241,  0.02213296, -0.02718618, -0.10834836,\n",
       "        0.15692195,  0.07487477, -0.0060477 ,  0.19855576, -0.06677024,\n",
       "       -0.13045196, -0.06712709, -0.01800687,  0.17212187,  0.03569894,\n",
       "        0.03833487,  0.16022391,  0.15676571])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there is an i dimension (i in range(L[2]) and a b dimension involved (b in range L[1])\n",
    "z=0  \n",
    "k=8\n",
    "#this needs repeating for each b, and the ith term in the bth copy must be multiplied by W[2][b][i]\n",
    "repeat_for_each_b = [W[3][i][z]*(wsums[2][k][i] > 0) for i in range(L[2])]\n",
    "print(repeat_for_each_b)\n",
    "\n",
    "#while we're at it, can do this sparsely using (wsums[2][k][i] > 0) to tell us which i to assign non-zero vals for\n",
    "# repeat_for_each_b_2 = [W[3][i][z]*(wsums[2][k][i] > 0) for i in range(L[2])] \n",
    "\n",
    "#does this bypass the need for z? for each z, we have grabbed all 3rd layer weights that contribute non-zero amount to the sum\n",
    "i_indices_we_need = np.nonzero(wsums[2][k] > 0)\n",
    "print(i_indices_we_need)\n",
    "print(len(i_indices_we_need[0]))\n",
    "#okay to get the values we want we need to take the zth element of each sub-array in the object below\n",
    "take_zth_element_of_each = W[3][np.nonzero(wsums[2][k] > 0)]\n",
    "print(take_zth_element_of_each.shape)\n",
    "#since there should be 33 elements per digit z (due to the no.nonzero length being 33), and our array is exactly 33x10!\n",
    "\n",
    "#so, try taking zth element of each of the 33 arrays\n",
    "take_zth_element_of_each[:,z]\n",
    "#it worked! The first vector we printed is the same as this final vector we've printed when summed (since 0 adds nothing to sums)\n",
    "#we may even eventually manage to remove the need to loop through z, but lets not get carried away... finish removing the b and i loops in next cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.24287017, -0.00864486,  0.0716291 , ...,  0.03833487,\n",
       "         0.16022391,  0.15676571],\n",
       "       [-0.24287017, -0.00864486,  0.0716291 , ...,  0.03833487,\n",
       "         0.16022391,  0.15676571],\n",
       "       [-0.24287017, -0.00864486,  0.0716291 , ...,  0.03833487,\n",
       "         0.16022391,  0.15676571],\n",
       "       ...,\n",
       "       [-0.24287017, -0.00864486,  0.0716291 , ...,  0.03833487,\n",
       "         0.16022391,  0.15676571],\n",
       "       [-0.24287017, -0.00864486,  0.0716291 , ...,  0.03833487,\n",
       "         0.16022391,  0.15676571],\n",
       "       [-0.24287017, -0.00864486,  0.0716291 , ...,  0.03833487,\n",
       "         0.16022391,  0.15676571]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finish removing the b and i loops\n",
    "z=0  \n",
    "k=8\n",
    "#like we said, this needs repeating for each b, (also note that I brought the np.nonzero and z indexing into one square bracket)\n",
    "repeat_for_each_b = W[3][np.nonzero(wsums[2][k] > 0)[0],z]\n",
    "#recall that b is in range(L[1]) i.e. 128 neurons in layer 1\n",
    "repeated = np.repeat([repeat_for_each_b],repeats=L[1],axis=0)\n",
    "print(repeated.shape)\n",
    "repeated\n",
    "\n",
    "# and the ith term in the bth copy must be multiplied by W[2][b][i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 33)\n",
      "(128, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the ith term in the bth copy must be multiplied by W[2][b][i]\n",
    "#since we have filtered the i index of take_zth_element_of_each using np.nonzero(wsums[2][k] > 0), do the same here\n",
    "W2_b_i = W[2][:,np.nonzero(wsums[2][k] > 0)[0]]\n",
    "print(W2_b_i.shape)\n",
    "#this is always going to be the same regardless of z, so makes me think more and more how we can remove z-loop\n",
    "\n",
    "#now use np.multiply to multiply each (b,i) element of W2_b_i by the (b,i) element of repeated\n",
    "multipd = np.multiply(W2_b_i,repeated)\n",
    "print(multipd.shape)\n",
    "\n",
    "# now we need to sum over the i dimension to be left with 128 sum values\n",
    "sumd = np.sum(multipd,axis=1,initial=0)\n",
    "sumd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2,   3,   4,   6,   7,   9,  14,  15,  17,  18,  19,  21,  22,\n",
       "         25,  27,  28,  29,  30,  31,  32,  33,  34,  36,  37,  38,  39,\n",
       "         41,  43,  44,  46,  50,  51,  52,  53,  54,  55,  56,  57,  59,\n",
       "         60,  61,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,\n",
       "         76,  79,  81,  82,  84,  85,  86,  87,  88,  89,  90,  92,  93,\n",
       "         94,  95,  98,  99, 100, 101, 102, 104, 105, 106, 107, 108, 110,\n",
       "        111, 112, 113, 115, 118, 119, 121, 122, 123, 124, 126, 127]),)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sumd = np.array([sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])])\n",
    "\n",
    "#see if answers equal in sumd and old_sumd\n",
    "np.nonzero(old_sumd != sumd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 6, 7, 9, 14, 15, 17, 18, 19, 21, 22, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 43, 44, 46, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 110, 111, 112, 113, 115, 118, 119, 121, 122, 123, 124, 126, 127]\n"
     ]
    }
   ],
   "source": [
    "bad_list = []\n",
    "for idx,oldval in enumerate(old_sumd):\n",
    "    if oldval != sumd[idx]:\n",
    "        bad_list += [idx]\n",
    "print(bad_list)\n",
    "#weirdly a lot of the valyes are equal, but many of them are also not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -0.044727651303600235 -0.04472765130360025\n",
      "3 -0.010193516769686245 -0.010193516769686242\n",
      "4 0.09112300932599526 0.09112300932599524\n",
      "6 -0.02171385971205858 -0.021713859712058574\n",
      "7 -0.09131303503032628 -0.0913130350303263\n",
      "9 -0.05351805368294152 -0.053518053682941505\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for idx,oldval in enumerate(old_sumd):\n",
    "    count+=1\n",
    "    if count > 10:\n",
    "        break\n",
    "    if oldval != sumd[idx]:\n",
    "        print(idx,oldval,sumd[idx])\n",
    "#okay so these examples are essentially equal for my purposes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#do a similar test where we only print if the new answer is eps different from the old\n",
    "eps = 0.0000000001\n",
    "bad_list2 = []\n",
    "for idx,oldval in enumerate(old_sumd):\n",
    "    if abs(oldval - sumd[idx]) > eps:\n",
    "        bad_list2 += [idx]\n",
    "print(bad_list2)\n",
    "#excellent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=0  \n",
    "k=8\n",
    "\n",
    "# so to be clear, we can write this old_sumd\n",
    "old_sumd = np.array([sum([W[3][i][z]*(wsums[2][k][i] > 0)*W[2][b][i] for i in range(L[2])]) for b in range(L[1])])\n",
    "\n",
    "#like this (combining our steps leading up to defining sumd, so we write it in one line!):\n",
    "sumd = np.sum(np.multiply(W[2][:,np.nonzero(wsums[2][k] > 0)[0]],\n",
    "                          #this is \"repeated\" we defined earlier\n",
    "                          np.repeat([W[3][np.nonzero(wsums[2][k] > 0)[0],z]],repeats=L[1],axis=0)),axis=1,initial=0)\n",
    "sumd.shape\n",
    "# now we should explore how we can remove the need for the z loop that comes next in our original i3thensum_all_k_all_z_2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try just pasting what we have in our old i3thensum_all_k_all_z_2 code\n",
    "i3thensum_all_k_all_z_3 = [\n",
    "        [\n",
    "            np.sum(np.multiply(W[2][:,np.nonzero(wsums[2][k] > 0)[0]],\n",
    "                          #this is \"repeated\" we defined earlier\n",
    "                          np.repeat([W[3][np.nonzero(wsums[2][k] > 0)[0],z]],repeats=L[1],axis=0)),axis=1,initial=0)\n",
    "                            if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])\n",
    "        ] for k in range(len(t))\n",
    "    ]\n",
    "# yoooo we got it down to 7.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10, 128)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirming the shape of this is what we need\n",
    "np.array(i3thensum_all_k_all_z_3).shape\n",
    "# 60k training samples\n",
    "# 10 values for z (the digits in L[3])\n",
    "# 128 values for b (the neurons in L[1]... recall for these layer 1 weights a is in L[0] and b is in L[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to use i3thensum_all_k_all_z_3 in the final calculation of the layer 1 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_W[1] = -sum([np.multiply(\n",
    "    #start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\n",
    "    np.transpose(np.transpose(np.array(wsums[1][k] > 0,ndmin=2)) @ np.array(train_arr_01[k],ndmin=2)),\n",
    "            #then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\n",
    "            np.repeat([i3thensum_all_k_all_z_3[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3thensum_all_k_all_z_3[k][z] for z in range(L[3])]),axis=0)],repeats=L[0],axis=0))\n",
    "            for k in range(len(t))]) / K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 1 weights has shape: (784, 128)\n",
      "element a=L[0]-1,b=L[1]-1 has value: -0.0\n",
      "CPU times: user 20.9 s, sys: 18.5 s, total: 39.5 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# so in totality here is g_W[1]\n",
    "i3thensum_all_k_all_z_3 = [\n",
    "        [\n",
    "            np.sum(np.multiply(W[2][:,np.nonzero(wsums[2][k] > 0)[0]],\n",
    "                          #this is \"repeated\" we defined earlier\n",
    "                          np.repeat([W[3][np.nonzero(wsums[2][k] > 0)[0],z]],repeats=L[1],axis=0)),axis=1,initial=0)\n",
    "                            if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])\n",
    "        ] for k in range(len(t))\n",
    "    ]\n",
    "\n",
    "g_W[1] = -sum([np.multiply(\n",
    "    #start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\n",
    "    np.transpose(np.transpose(np.array(wsums[1][k] > 0,ndmin=2)) @ np.array(train_arr_01[k],ndmin=2)),\n",
    "            #then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\n",
    "            np.repeat([i3thensum_all_k_all_z_3[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3thensum_all_k_all_z_3[k][z] for z in range(L[3])]),axis=0)],repeats=L[0],axis=0))\n",
    "            for k in range(len(t))]) / K\n",
    "\n",
    "print(\"matrix of gradients relating to layer 1 weights has shape:\",g_W[1].shape)\n",
    "print(\"element a=L[0]-1,b=L[1]-1 has value:\",g_W[1][L[0]-1][L[1]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Try Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets crudely try performing a few training steps with learning rate 0.01 and see what happens to the accuracy of our neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Initialise Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise neural network\n",
    "\n",
    "#number of neurons in each layer\n",
    "L = {0:784, 1:128, 2:64, 3:10}\n",
    "#bias vals initialised at 0\n",
    "b = {1:0, 2:0, 3:0}\n",
    "# weight matrices (randomly initialised using xavier)\n",
    "W = {1:rng.normal(loc=0,scale=(1/L[0])**0.5,size=[L[0],L[1]]),\n",
    "     2:rng.normal(loc=0,scale=(1/L[1])**0.5,size=[L[1],L[2]]),\n",
    "     3:rng.normal(loc=0,scale=(1/L[2])**0.5,size=[L[2],L[3]])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Initial Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels1 = vec_predict(np.array(range(len(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07446666666666667\n",
      "total accuracy = 7.45 %\n"
     ]
    }
   ],
   "source": [
    "# TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(t)]\n",
    "total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "print(total_accuracy)\n",
    "print(\"total accuracy =\", round(100*total_accuracy,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7741009623501603,\n",
       " 1: 0.008751112429546128,\n",
       " 2: 0.00016784155756965425,\n",
       " 3: 0.0,\n",
       " 4: 0.0,\n",
       " 5: 0.01567976388120273,\n",
       " 6: 0.0003379520108144643,\n",
       " 7: 0.01596169193934557,\n",
       " 8: 0.16304905144419757,\n",
       " 9: 0.021011934778954446}"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "digit_accuracy = {}\n",
    "for digit in range(10):\n",
    "    train_cases_containing_digit = [k for k,true_val in enumerate(t) if true_val==digit]\n",
    "    # now calculate how many of these cases we predicted correctly\n",
    "    checklist = [pred_labels1[k]==digit for k,true_val in enumerate(train_cases_containing_digit)]\n",
    "    digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "\n",
    "digit_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09831962039092769,\n",
       " 1: 0.13275862068965516,\n",
       " 2: 0.16666666666666666,\n",
       " 3: None,\n",
       " 4: 0.3333333333333333,\n",
       " 5: 0.09155766944114149,\n",
       " 6: 0.09523809523809523,\n",
       " 7: 0.1101511879049676,\n",
       " 8: 0.09406912804399058,\n",
       " 9: 0.10240549828178694}"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "\n",
    "digit_recall = {}\n",
    "for digit in range(10):\n",
    "    cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "    # now calculate how many of these cases actually contained this digit correctly\n",
    "    checklist = [t[k]==digit for k,true_val in enumerate(cases_we_predicted_digit)]\n",
    "    digit_recall[digit] = sum(checklist)/len(checklist) if len(checklist) != 0 else None\n",
    "\n",
    "digit_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Perform 1 Gradient Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the code to perform one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need every single weight sum for every single k, so create a data structure that allows us to store these conveniently\n",
    "\n",
    "#defining this dictionary is making me think about having a 3d numpy array where the dimensions are:\n",
    "# layer l, training case K, and the neurons (in layer l there are L[l] many)\n",
    "wsums = {l:{} for l in L.keys()}\n",
    "\n",
    "wsums[1] = {k:b[1] + (train_arr_01[k] @ W[1]) for k in range(len(t))}\n",
    "# now we need to define the second layer weight sums using wsums[1]\n",
    "wsums[2] = {k:b[2] + f(wsums[1][k]) @ W[2] for k in range(len(t))}\n",
    "# and define the 3rd layer weight sums using the second weight sums we just calculated\n",
    "wsums[3] = {k:b[3] + f(wsums[2][k]) @ W[3] for k in range(len(t))}\n",
    "#this way, we get all the weight sums we need and only have to do each matrix multiplication once! work samrter not harder?\n",
    "\n",
    "# original formula here for reference\n",
    "# f(b[3] + ( f(b[2] + ( f(b[1] + ( train_arr_01[k] @ W[1] )) @ W[2])) @ W[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true labels for all K training cases\n",
    "t = np.array(train_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of phat for the 0th training case: [0.11268324 0.10074844 0.08986191 0.08986191 0.08986191 0.10758156\n",
      " 0.09234008 0.09388809 0.11892068 0.10425218]\n"
     ]
    }
   ],
   "source": [
    "# now we need to arrive at phat for each training case k from the layer 3 weightsums in wsums[3]\n",
    "expvec_all_k = [allexp(f(wsums[3][k])) for k in range(len(t))]\n",
    "phat_all_k = [expvec_all_k[k]/sum(expvec_all_k[k]) for k in range(len(t))]\n",
    "print(\"example of phat for the 0th training case:\",phat_all_k[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 0, 3: 0}"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a copy of the bias parameters with a g on the front to signify gradients\n",
    "g_b = b.copy()\n",
    "g_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of the cost function wrt the 3rd bias = 0.012928875672021787\n",
      "gradient of the cost function wrt the 2nd bias = 0.08521286016574461\n",
      "gradient of the cost function wrt the 1st bias = 0.03950793543938568\n"
     ]
    }
   ],
   "source": [
    "# biases\n",
    "g_b[3] = -sum([(wsums[3][k][t[k]] > 0) - sum([phat_all_k[k][z] for z in range(L[3]) if wsums[3][k][z] > 0]) for k in range(len(t))]) / K\n",
    "print(\"gradient of the cost function wrt the 3rd bias =\",g_b[3])\n",
    "\n",
    "\n",
    "g_b[2] = -sum([(sum([W[3][i][t[k]] for i in range(L[2]) if wsums[2][k][i] > 0]) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "- sum([phat_all_k[k][z]*sum([W[3][i][z] for i in range(L[2]) if wsums[2][k][i] > 0]) for z in range(L[3]) if wsums[3][k][z] > 0]) for k in range(len(t))]) / K\n",
    "print(\"gradient of the cost function wrt the 2nd bias =\",g_b[2])\n",
    "\n",
    "\n",
    "\n",
    "g_b[1] = -sum(\n",
    "    [\n",
    "        (\n",
    "            np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),t[k]] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][t[k]] > 0 else 0) \\\n",
    "\n",
    "- sum(\n",
    "    [\n",
    "        phat_all_k[k][z]*(np.sum(\n",
    "            np.multiply(W[3][np.nonzero(wsums[2][k][:] > 0),z] , np.sum(W[2][np.nonzero(wsums[1][k][:] > 0)][:,np.nonzero(wsums[2][k][:] > 0)],initial=0,axis=0))\n",
    "                ) if wsums[3][k][z] > 0 else 0) for z in range(L[3]) if wsums[3][k][z] > 0]\n",
    "                ) for k in range(len(t))]) / K\n",
    "\n",
    "print(\"gradient of the cost function wrt the 1st bias =\",g_b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of W that we will store the corresponding gradients in\n",
    "g_W = W.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_W[3] = np.transpose(-np.sum(np.array([np.multiply(np.array([np.repeat([( (b==t[k]) - phat_all_k[k][b] )],repeats=L[2]) for b in range(L[3])]),np.transpose(np.array(wsums[3][k] > 0,ndmin=2)) @ np.array(f(wsums[2][k]),ndmin=2))\n",
    "                            for k in range(len(t))]),axis=0) / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 2 weights has shape: (128, 64)\n",
      "element a=L[1]-1,b=L[2]-1 has value: 0.00012143750540080099\n"
     ]
    }
   ],
   "source": [
    "# so, here is the final code I'll use for layer 2 weights\n",
    "i3W3_all_k_all_z_fast = np.array(\n",
    "    [\n",
    "        [np.array(W[3][:,z]) if (wsums[3][k][z] > 0) else np.zeros(L[2]) for z in range(L[3])\n",
    "         ] for k in range(len(t))\n",
    "         ]\n",
    "         )\n",
    "\n",
    "g_W[2] = -sum([np.multiply(\n",
    "    #start np.multiply with the transpose of new_dotted\n",
    "    np.transpose(np.transpose(np.array(wsums[2][k] > 0,ndmin=2)) @ np.array(f(wsums[1][k]),ndmin=2)),\n",
    "            #bracketsterm_repeated is defined as the right-part of this np.multiply\n",
    "            np.repeat([i3W3_all_k_all_z_fast[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3W3_all_k_all_z_fast[k][z] for z in range(L[3])]),axis=0)],repeats=L[1],axis=0))\n",
    "            for k in range(len(t))]) / K\n",
    "\n",
    "print(\"matrix of gradients relating to layer 2 weights has shape:\",g_W[2].shape)\n",
    "print(\"element a=L[1]-1,b=L[2]-1 has value:\",g_W[2][L[1]-1][L[2]-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix of gradients relating to layer 1 weights has shape: (784, 128)\n",
      "element a=L[0]-1,b=L[1]-1 has value: -0.0\n"
     ]
    }
   ],
   "source": [
    "# so in totality here is g_W[1]\n",
    "i3thensum_all_k_all_z_3 = [\n",
    "        [\n",
    "            np.sum(np.multiply(W[2][:,np.nonzero(wsums[2][k] > 0)[0]],\n",
    "                          #this is \"repeated\" we defined earlier\n",
    "                          np.repeat([W[3][np.nonzero(wsums[2][k] > 0)[0],z]],repeats=L[1],axis=0)),axis=1,initial=0)\n",
    "                            if (wsums[3][k][z] > 0) else np.zeros(L[1]) for z in range(L[3])\n",
    "        ] for k in range(len(t))\n",
    "    ]\n",
    "\n",
    "g_W[1] = -sum([np.multiply(\n",
    "    #start np.multiply with the same idea as new_dotted in the g_W[2] case, but with layers 2 and 1 swapped for 1 and 0 (resp.)\n",
    "    np.transpose(np.transpose(np.array(wsums[1][k] > 0,ndmin=2)) @ np.array(train_arr_01[k],ndmin=2)),\n",
    "            #then this second part of np.multiply is similar to the g_W[2] case, using i3thensum_all_k_all_z_3 in place of i3W3_all_k_all_z (and repeat L[0] times)\n",
    "            np.repeat([i3thensum_all_k_all_z_3[k][t[k]] - np.sum(np.array([phat_all_k[k][z] * i3thensum_all_k_all_z_3[k][z] for z in range(L[3])]),axis=0)],repeats=L[0],axis=0))\n",
    "            for k in range(len(t))]) / K\n",
    "\n",
    "print(\"matrix of gradients relating to layer 1 weights has shape:\",g_W[1].shape)\n",
    "print(\"element a=L[0]-1,b=L[1]-1 has value:\",g_W[1][L[0]-1][L[1]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform the step, with learning rate parameter nu\n",
    "nu = 0.01\n",
    "\n",
    "for l in [1,2,3]:\n",
    "    #gradient step for bias and weioghts corresponding to layer l\n",
    "    b[l] -= nu*g_b[l]\n",
    "    W[l] -= nu*g_W[l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now test the network after 1 step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Performance After 1 Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine relevant funcs to use updat3ed network, just in case\n",
    "def predict_k(k):\n",
    "    vec = train_arr_01[k]\n",
    "    p3 = f(b[3] + ( f(b[2] + ( f(b[1] + ( vec @ W[1] )) @ W[2])) @ W[3]))\n",
    "    expvec = allexp(p3)\n",
    "    phat = expvec/sum(expvec)\n",
    "    return [idx for idx,val in enumerate(phat) if val >= max(phat)][0]\n",
    "\n",
    "# is it quicker than 13s vectorized? Yes, a bit\n",
    "vec_predict = np.vectorize(predict_k)\n",
    "pred_labels1 = vec_predict(np.array(range(len(t))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0776\n",
      "total accuracy = 7.76 %\n"
     ]
    }
   ],
   "source": [
    "# TOTAL ACCURACY (percent of cases that the model predicted correctly)\n",
    "true_val_checklist = [pred_labels1[k]==true_val for k,true_val in enumerate(t)]\n",
    "total_accuracy = sum(true_val_checklist)/len(true_val_checklist)\n",
    "print(total_accuracy)\n",
    "print(\"total accuracy =\", round(100*total_accuracy,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7742697957116326,\n",
       " 1: 0.009492732126965293,\n",
       " 2: 0.00016784155756965425,\n",
       " 3: 0.0,\n",
       " 4: 0.0,\n",
       " 5: 0.01549529607083564,\n",
       " 6: 0.0003379520108144643,\n",
       " 7: 0.016280925778132484,\n",
       " 8: 0.16287814048880533,\n",
       " 9: 0.02050764834425954}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# digit accuracy is the percent of training cases containing digit n that the model predicted correctly\n",
    "digit_accuracy = {}\n",
    "for digit in range(10):\n",
    "    train_cases_containing_digit = [k for k,true_val in enumerate(t) if true_val==digit]\n",
    "    # now calculate how many of these cases we predicted correctly\n",
    "    checklist = [pred_labels1[k]==digit for k,true_val in enumerate(train_cases_containing_digit)]\n",
    "    digit_accuracy[digit] = sum(checklist)/len(checklist)\n",
    "\n",
    "digit_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09835137312977764,\n",
       " 1: 0.13213703099510604,\n",
       " 2: 0.16666666666666666,\n",
       " 3: None,\n",
       " 4: 0.3333333333333333,\n",
       " 5: 0.08948004836759371,\n",
       " 6: 0.08620689655172414,\n",
       " 7: 0.1159274193548387,\n",
       " 8: 0.0944998518811099,\n",
       " 9: 0.1029512697323267}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# digit recall should tell us the percent of cases the model predicted as that digit that were actually that digit\n",
    "\n",
    "digit_recall = {}\n",
    "for digit in range(10):\n",
    "    cases_we_predicted_digit = [k for k,true_val in enumerate(pred_labels1) if true_val==digit]\n",
    "    # now calculate how many of these cases actually contained this digit correctly\n",
    "    checklist = [t[k]==digit for k,true_val in enumerate(cases_we_predicted_digit)]\n",
    "    digit_recall[digit] = sum(checklist)/len(checklist) if len(checklist) != 0 else None\n",
    "\n",
    "digit_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Nnet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have the code we need to start training the network, lets create a class to neatly store our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX/COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
